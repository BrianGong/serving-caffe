{
	"auto_complete":
	{
		"selected_items":
		[
			[
				"model",
				"model_name"
			],
			[
				"enable",
				"enable_batching"
			],
			[
				"Build",
				"BuildAndRun"
			],
			[
				"Sour",
				"SourceAdapterConfig"
			],
			[
				"DetectSer",
				"DetectServiceImpl"
			],
			[
				"DetectRe",
				"DetectRequest"
			],
			[
				"service_",
				"service_impl_"
			],
			[
				"Detect",
				"DetectFn"
			],
			[
				"Detec",
				"DetectService"
			],
			[
				"TRes",
				"TRequest"
			],
			[
				"int2",
				"int32_t"
			],
			[
				"export",
				"export_path"
			],
			[
				"pixel",
				"img_pixel_means_"
			],
			[
				"im_",
				"im_blob"
			],
			[
				"WITH_",
				"WITH_SSD"
			],
			[
				"WITH",
				"WITH_RCNN"
			],
			[
				"input",
				"input_resolution"
			],
			[
				"inpu",
				"input_resolution"
			],
			[
				"Res",
				"Resolution_WH"
			],
			[
				"re",
				"resolution"
			],
			[
				"parse",
				"parse_cmdline"
			],
			[
				"detect",
				"detection_thresh"
			],
			[
				"res",
				"results"
			],
			[
				"output",
				"output_mat"
			],
			[
				"score",
				"scores"
			],
			[
				"schedul",
				"scheduler_options"
			],
			[
				"initial",
				"initial_shape"
			],
			[
				"named",
				"named_initial_shapes"
			],
			[
				"tmp_",
				"tmp_boxes"
			],
			[
				"sorted",
				"sorted_selected_indices"
			],
			[
				"add",
				"add_value"
			],
			[
				"class",
				"class_idx"
			],
			[
				"sort",
				"sorted_indices"
			],
			[
				"sele",
				"selected_indices"
			],
			[
				"boxes",
				"boxes_data"
			],
			[
				"max_",
				"max_score"
			],
			[
				"box",
				"boxes"
			],
			[
				"pred_ctr",
				"pred_ctr_x"
			],
			[
				"kIma",
				"kImageSizeW"
			],
			[
				"import",
				"importlib"
			],
			[
				"bundle",
				"bundle_cfg"
			],
			[
				"py",
				"py-faster-rcnn"
			],
			[
				"repo",
				"repo_path"
			],
			[
				"SOURCE",
				"SOURCE_SHA1"
			],
			[
				"if_",
				"if_pycaffe"
			],
			[
				"En",
				"EnsurePyCaffe"
			],
			[
				"num",
				"num_elements"
			],
			[
				"source",
				"source_adapter_config"
			],
			[
				"name",
				"namespace"
			],
			[
				"force",
				"force_device_id"
			],
			[
				"for",
				"force_gpu_id"
			],
			[
				"device",
				"device_id"
			],
			[
				"default",
				"default_ptr_"
			],
			[
				"Caffe",
				"CaffeSessionOptions"
			],
			[
				"file",
				"file_size"
			],
			[
				"ram",
				"ram_requirement"
			],
			[
				"Ten",
				"TensorBinding"
			],
			[
				"Sign",
				"Signature"
			],
			[
				"emplace",
				"emplace_back"
			],
			[
				"lay",
				"layer_id"
			],
			[
				"availa",
				"available_blobs"
			],
			[
				"labsl",
				"class_labels_"
			],
			[
				"labels",
				"labels_path"
			],
			[
				"batch",
				"batch_size"
			],
			[
				"infer",
				"inference_stats"
			],
			[
				"percent",
				"percentiles"
			],
			[
				"result",
				"result_timing"
			],
			[
				"proto",
				"protobuf"
			],
			[
				"Invali",
				"InvalidArgument"
			],
			[
				"input_blob",
				"input_blob_map_"
			],
			[
				"Tensor",
				"TensorShape"
			],
			[
				"target",
				"target_node_names"
			],
			[
				"inp",
				"inputs"
			],
			[
				"input_blo",
				"input_blob_map_"
			],
			[
				"wieghts",
				"weights_path"
			],
			[
				"weights",
				"weights_path"
			],
			[
				"extr",
				"extract-archive"
			]
		]
	},
	"buffers":
	[
		{
			"file": "WORKSPACE",
			"settings":
			{
				"buffer_size": 423,
				"line_ending": "Unix"
			}
		},
		{
			"file": "caffe.BUILD",
			"settings":
			{
				"buffer_size": 6541,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/C/Users/IBM_ADMIN/Downloads/mnist_pretrained_caffe/deploy.prototxt",
			"settings":
			{
				"buffer_size": 1746,
				"line_ending": "Unix"
			}
		},
		{
			"file": "third_party/caffe/BUILD",
			"settings":
			{
				"buffer_size": 793,
				"line_ending": "Unix"
			}
		},
		{
			"file": "third_party/caffe/__init__.py",
			"settings":
			{
				"buffer_size": 1314,
				"line_ending": "Unix"
			}
		},
		{
			"file": "third_party/caffe/config.bzl",
			"settings":
			{
				"buffer_size": 382,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/workspace.bzl",
			"settings":
			{
				"buffer_size": 1586,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/tensorflow/session_bundle_source_adapter.h",
			"settings":
			{
				"buffer_size": 2627,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/tensorflow/simple_servers.cc",
			"settings":
			{
				"buffer_size": 4048,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "/* Copyright 2016 Google Inc. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/contrib/session_bundle/signature.h\"\n\n#include <memory>\n\n#include \"google/protobuf/any.pb.h\"\n#include \"tensorflow/contrib/session_bundle/manifest.pb.h\"\n#include \"tensorflow/core/framework/graph.pb.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/framework/tensor_testutil.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/lib/core/status_test_util.h\"\n#include \"tensorflow/core/lib/core/stringpiece.h\"\n#include \"tensorflow/core/platform/test.h\"\n#include \"tensorflow/core/public/session.h\"\n\nnamespace tensorflow {\nnamespace serving {\nnamespace {\n\nstatic bool HasSubstr(const string& base, const string& substr) {\n  bool ok = StringPiece(base).contains(substr);\n  EXPECT_TRUE(ok) << base << \", expected substring \" << substr;\n  return ok;\n}\n\nTEST(GetClassificationSignature, Basic) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  ClassificationSignature* input_signature =\n      signatures.mutable_default_signature()\n          ->mutable_classification_signature();\n  input_signature->mutable_input()->set_tensor_name(\"flow\");\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  ClassificationSignature signature;\n  const Status status = GetClassificationSignature(meta_graph_def, &signature);\n  TF_ASSERT_OK(status);\n  EXPECT_EQ(signature.input().tensor_name(), \"flow\");\n}\n\nTEST(GetClassificationSignature, MissingSignature) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  signatures.mutable_default_signature();\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  ClassificationSignature signature;\n  const Status status = GetClassificationSignature(meta_graph_def, &signature);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message())\n                  .contains(\"Expected a classification signature\"))\n      << status.error_message();\n}\n\nTEST(GetClassificationSignature, WrongSignatureType) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  signatures.mutable_default_signature()->mutable_regression_signature();\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  ClassificationSignature signature;\n  const Status status = GetClassificationSignature(meta_graph_def, &signature);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message())\n                  .contains(\"Expected a classification signature\"))\n      << status.error_message();\n}\n\nTEST(GetNamedClassificationSignature, Basic) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  ClassificationSignature* input_signature =\n      (*signatures.mutable_named_signatures())[\"foo\"]\n          .mutable_classification_signature();\n  input_signature->mutable_input()->set_tensor_name(\"flow\");\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  ClassificationSignature signature;\n  const Status status =\n      GetNamedClassificationSignature(\"foo\", meta_graph_def, &signature);\n  TF_ASSERT_OK(status);\n  EXPECT_EQ(signature.input().tensor_name(), \"flow\");\n}\n\nTEST(GetNamedClassificationSignature, MissingSignature) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  ClassificationSignature signature;\n  const Status status =\n      GetNamedClassificationSignature(\"foo\", meta_graph_def, &signature);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message())\n                  .contains(\"Missing signature named \\\"foo\\\"\"))\n      << status.error_message();\n}\n\nTEST(GetNamedClassificationSignature, WrongSignatureType) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  (*signatures.mutable_named_signatures())[\"foo\"]\n      .mutable_regression_signature();\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  ClassificationSignature signature;\n  const Status status =\n      GetNamedClassificationSignature(\"foo\", meta_graph_def, &signature);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(\n      StringPiece(status.error_message())\n          .contains(\"Expected a classification signature for name \\\"foo\\\"\"))\n      << status.error_message();\n}\n\nTEST(GetRegressionSignature, Basic) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  RegressionSignature* input_signature =\n      signatures.mutable_default_signature()->mutable_regression_signature();\n  input_signature->mutable_input()->set_tensor_name(\"flow\");\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  RegressionSignature signature;\n  const Status status = GetRegressionSignature(meta_graph_def, &signature);\n  TF_ASSERT_OK(status);\n  EXPECT_EQ(signature.input().tensor_name(), \"flow\");\n}\n\nTEST(GetRegressionSignature, MissingSignature) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  signatures.mutable_default_signature();\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  RegressionSignature signature;\n  const Status status = GetRegressionSignature(meta_graph_def, &signature);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message())\n                  .contains(\"Expected a regression signature\"))\n      << status.error_message();\n}\n\nTEST(GetRegressionSignature, WrongSignatureType) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  signatures.mutable_default_signature()->mutable_classification_signature();\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  RegressionSignature signature;\n  const Status status = GetRegressionSignature(meta_graph_def, &signature);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message())\n                  .contains(\"Expected a regression signature\"))\n      << status.error_message();\n}\n\nTEST(GetNamedSignature, Basic) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  ClassificationSignature* input_signature =\n      (*signatures.mutable_named_signatures())[\"foo\"]\n          .mutable_classification_signature();\n  input_signature->mutable_input()->set_tensor_name(\"flow\");\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  Signature signature;\n  const Status status = GetNamedSignature(\"foo\", meta_graph_def, &signature);\n  TF_ASSERT_OK(status);\n  EXPECT_EQ(signature.classification_signature().input().tensor_name(), \"flow\");\n}\n\nTEST(GetNamedSignature, MissingSignature) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  Signature signature;\n  const Status status = GetNamedSignature(\"foo\", meta_graph_def, &signature);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message())\n                  .contains(\"Missing signature named \\\"foo\\\"\"))\n      << status.error_message();\n}\n\n// MockSession used to test input and output interactions with a\n// tensorflow::Session.\nstruct MockSession : public tensorflow::Session {\n  ~MockSession() override = default;\n\n  Status Create(const GraphDef& graph) override {\n    return errors::Unimplemented(\"Not implemented for mock.\");\n  }\n\n  Status Extend(const GraphDef& graph) override {\n    return errors::Unimplemented(\"Not implemented for mock.\");\n  }\n\n  // Sets the input and output arguments.\n  Status Run(const std::vector<std::pair<string, Tensor>>& inputs_arg,\n             const std::vector<string>& output_tensor_names_arg,\n             const std::vector<string>& target_node_names_arg,\n             std::vector<Tensor>* outputs_arg) override {\n    inputs = inputs_arg;\n    output_tensor_names = output_tensor_names_arg;\n    target_node_names = target_node_names_arg;\n    *outputs_arg = outputs;\n    return status;\n  }\n\n  Status Close() override {\n    return errors::Unimplemented(\"Not implemented for mock.\");\n  }\n\n  // Arguments stored on a Run call.\n  std::vector<std::pair<string, Tensor>> inputs;\n  std::vector<string> output_tensor_names;\n  std::vector<string> target_node_names;\n\n  // Output argument set by Run; should be set before calling.\n  std::vector<Tensor> outputs;\n\n  // Return value for Run; should be set before calling.\n  Status status;\n};\n\nconstexpr char kInputName[] = \"in:0\";\nconstexpr char kClassesName[] = \"classes:0\";\nconstexpr char kScoresName[] = \"scores:0\";\n\nclass RunClassificationTest : public ::testing::Test {\n public:\n  void SetUp() override {\n    signature_.mutable_input()->set_tensor_name(kInputName);\n    signature_.mutable_classes()->set_tensor_name(kClassesName);\n    signature_.mutable_scores()->set_tensor_name(kScoresName);\n  }\n\n protected:\n  ClassificationSignature signature_;\n  Tensor input_tensor_;\n  Tensor classes_tensor_;\n  Tensor scores_tensor_;\n  MockSession session_;\n};\n\nTEST_F(RunClassificationTest, Basic) {\n  input_tensor_ = test::AsTensor<int>({99});\n  session_.outputs = {test::AsTensor<int>({3}), test::AsTensor<int>({2})};\n  const Status status = RunClassification(signature_, input_tensor_, &session_,\n                                          &classes_tensor_, &scores_tensor_);\n\n  // Validate outputs.\n  TF_ASSERT_OK(status);\n  test::ExpectTensorEqual<int>(test::AsTensor<int>({3}), classes_tensor_);\n  test::ExpectTensorEqual<int>(test::AsTensor<int>({2}), scores_tensor_);\n\n  // Validate inputs.\n  ASSERT_EQ(1, session_.inputs.size());\n  EXPECT_EQ(kInputName, session_.inputs[0].first);\n  test::ExpectTensorEqual<int>(test::AsTensor<int>({99}),\n                               session_.inputs[0].second);\n\n  ASSERT_EQ(2, session_.output_tensor_names.size());\n  EXPECT_EQ(kClassesName, session_.output_tensor_names[0]);\n  EXPECT_EQ(kScoresName, session_.output_tensor_names[1]);\n}\n\nTEST_F(RunClassificationTest, ClassesOnly) {\n  input_tensor_ = test::AsTensor<int>({99});\n  session_.outputs = {test::AsTensor<int>({3})};\n  const Status status = RunClassification(signature_, input_tensor_, &session_,\n                                          &classes_tensor_, nullptr);\n\n  // Validate outputs.\n  TF_ASSERT_OK(status);\n  test::ExpectTensorEqual<int>(test::AsTensor<int>({3}), classes_tensor_);\n\n  // Validate inputs.\n  ASSERT_EQ(1, session_.inputs.size());\n  EXPECT_EQ(kInputName, session_.inputs[0].first);\n  test::ExpectTensorEqual<int>(test::AsTensor<int>({99}),\n                               session_.inputs[0].second);\n\n  ASSERT_EQ(1, session_.output_tensor_names.size());\n  EXPECT_EQ(kClassesName, session_.output_tensor_names[0]);\n}\n\nTEST_F(RunClassificationTest, ScoresOnly) {\n  input_tensor_ = test::AsTensor<int>({99});\n  session_.outputs = {test::AsTensor<int>({2})};\n  const Status status = RunClassification(signature_, input_tensor_, &session_,\n                                          nullptr, &scores_tensor_);\n\n  // Validate outputs.\n  TF_ASSERT_OK(status);\n  test::ExpectTensorEqual<int>(test::AsTensor<int>({2}), scores_tensor_);\n\n  // Validate inputs.\n  ASSERT_EQ(1, session_.inputs.size());\n  EXPECT_EQ(kInputName, session_.inputs[0].first);\n  test::ExpectTensorEqual<int>(test::AsTensor<int>({99}),\n                               session_.inputs[0].second);\n\n  ASSERT_EQ(1, session_.output_tensor_names.size());\n  EXPECT_EQ(kScoresName, session_.output_tensor_names[0]);\n}\n\nTEST(RunClassification, RunNotOk) {\n  ClassificationSignature signature;\n  signature.mutable_input()->set_tensor_name(\"in:0\");\n  signature.mutable_classes()->set_tensor_name(\"classes:0\");\n  Tensor input_tensor = test::AsTensor<int>({99});\n  MockSession session;\n  session.status = errors::DataLoss(\"Data is gone\");\n  Tensor classes_tensor;\n  const Status status = RunClassification(signature, input_tensor, &session,\n                                          &classes_tensor, nullptr);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message()).contains(\"Data is gone\"))\n      << status.error_message();\n}\n\nTEST(RunClassification, TooManyOutputs) {\n  ClassificationSignature signature;\n  signature.mutable_input()->set_tensor_name(\"in:0\");\n  signature.mutable_classes()->set_tensor_name(\"classes:0\");\n  Tensor input_tensor = test::AsTensor<int>({99});\n  MockSession session;\n  session.outputs = {test::AsTensor<int>({3}), test::AsTensor<int>({4})};\n\n  Tensor classes_tensor;\n  const Status status = RunClassification(signature, input_tensor, &session,\n                                          &classes_tensor, nullptr);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message()).contains(\"Expected 1 output\"))\n      << status.error_message();\n}\n\nTEST(RunClassification, WrongBatchOutputs) {\n  ClassificationSignature signature;\n  signature.mutable_input()->set_tensor_name(\"in:0\");\n  signature.mutable_classes()->set_tensor_name(\"classes:0\");\n  Tensor input_tensor = test::AsTensor<int>({99, 100});\n  MockSession session;\n  session.outputs = {test::AsTensor<int>({3})};\n\n  Tensor classes_tensor;\n  const Status status = RunClassification(signature, input_tensor, &session,\n                                          &classes_tensor, nullptr);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message())\n                  .contains(\"Input batch size did not match output batch size\"))\n      << status.error_message();\n}\n\nconstexpr char kRegressionsName[] = \"regressions:0\";\n\nclass RunRegressionTest : public ::testing::Test {\n public:\n  void SetUp() override {\n    signature_.mutable_input()->set_tensor_name(kInputName);\n    signature_.mutable_output()->set_tensor_name(kRegressionsName);\n  }\n\n protected:\n  RegressionSignature signature_;\n  Tensor input_tensor_;\n  Tensor output_tensor_;\n  MockSession session_;\n};\n\nTEST_F(RunRegressionTest, Basic) {\n  input_tensor_ = test::AsTensor<int>({99, 100});\n  session_.outputs = {test::AsTensor<float>({1, 2})};\n  const Status status =\n      RunRegression(signature_, input_tensor_, &session_, &output_tensor_);\n\n  // Validate outputs.\n  TF_ASSERT_OK(status);\n  test::ExpectTensorEqual<float>(test::AsTensor<float>({1, 2}), output_tensor_);\n\n  // Validate inputs.\n  ASSERT_EQ(1, session_.inputs.size());\n  EXPECT_EQ(kInputName, session_.inputs[0].first);\n  test::ExpectTensorEqual<int>(test::AsTensor<int>({99, 100}),\n                               session_.inputs[0].second);\n\n  ASSERT_EQ(1, session_.output_tensor_names.size());\n  EXPECT_EQ(kRegressionsName, session_.output_tensor_names[0]);\n}\n\nTEST_F(RunRegressionTest, RunNotOk) {\n  input_tensor_ = test::AsTensor<int>({99});\n  session_.status = errors::DataLoss(\"Data is gone\");\n  const Status status =\n      RunRegression(signature_, input_tensor_, &session_, &output_tensor_);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message()).contains(\"Data is gone\"))\n      << status.error_message();\n}\n\nTEST_F(RunRegressionTest, MismatchedSizeForBatchInputAndOutput) {\n  input_tensor_ = test::AsTensor<int>({99, 100});\n  session_.outputs = {test::AsTensor<float>({3})};\n\n  const Status status =\n      RunRegression(signature_, input_tensor_, &session_, &output_tensor_);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message())\n                  .contains(\"Input batch size did not match output batch size\"))\n      << status.error_message();\n}\n\nTEST(SetAndGetSignatures, RoundTrip) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  signatures.mutable_default_signature()\n      ->mutable_classification_signature()\n      ->mutable_input()\n      ->set_tensor_name(\"in:0\");\n  TF_ASSERT_OK(SetSignatures(signatures, &meta_graph_def));\n  Signatures read_signatures;\n  TF_ASSERT_OK(GetSignatures(meta_graph_def, &read_signatures));\n\n  EXPECT_EQ(\"in:0\", read_signatures.default_signature()\n                        .classification_signature()\n                        .input()\n                        .tensor_name());\n}\n\nTEST(GetSignatures, MissingSignature) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures read_signatures;\n  const auto status = GetSignatures(meta_graph_def, &read_signatures);\n  EXPECT_FALSE(status.ok());\n  EXPECT_TRUE(\n      StringPiece(status.error_message()).contains(\"Expected exactly one\"))\n      << status.error_message();\n}\n\nTEST(GetSignatures, WrongProtoInAny) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  auto& collection_def = *(meta_graph_def.mutable_collection_def());\n  auto* any =\n      collection_def[kSignaturesKey].mutable_any_list()->mutable_value()->Add();\n  // Put an unexpected type into the Signatures Any.\n  any->PackFrom(TensorBinding());\n  Signatures read_signatures;\n  const auto status = GetSignatures(meta_graph_def, &read_signatures);\n  EXPECT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message())\n                  .contains(\"Expected signature Any type_url for: \"\n                            \"tensorflow.serving.Signatures\"))\n      << status.error_message();\n}\n\nTEST(GetSignatures, JunkInAny) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  auto& collection_def = *(meta_graph_def.mutable_collection_def());\n  auto* any =\n      collection_def[kSignaturesKey].mutable_any_list()->mutable_value()->Add();\n  // Create a valid Any then corrupt it.\n  any->PackFrom(Signatures());\n  any->set_value(\"junk junk\");\n  Signatures read_signatures;\n  const auto status = GetSignatures(meta_graph_def, &read_signatures);\n  EXPECT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message()).contains(\"Failed to unpack\"))\n      << status.error_message();\n}\n\n// GenericSignature test fixture that contains a signature initialized with two\n// bound Tensors.\nclass GenericSignatureTest : public ::testing::Test {\n protected:\n  GenericSignatureTest() {\n    TensorBinding binding;\n    binding.set_tensor_name(\"graph_A\");\n    signature_.mutable_map()->insert({\"logical_A\", binding});\n\n    binding.set_tensor_name(\"graph_B\");\n    signature_.mutable_map()->insert({\"logical_B\", binding});\n  }\n\n  // GenericSignature that contains two bound Tensors.\n  GenericSignature signature_;\n};\n\n// GenericSignature tests.\n\nTEST_F(GenericSignatureTest, GetGenericSignatureBasic) {\n  Signature expected_signature;\n  expected_signature.mutable_generic_signature()->MergeFrom(signature_);\n\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  signatures.mutable_named_signatures()->insert(\n      {\"generic_bindings\", expected_signature});\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  GenericSignature actual_signature;\n  TF_ASSERT_OK(GetGenericSignature(\"generic_bindings\", meta_graph_def,\n                                   &actual_signature));\n  ASSERT_EQ(\"graph_A\", actual_signature.map().at(\"logical_A\").tensor_name());\n  ASSERT_EQ(\"graph_B\", actual_signature.map().at(\"logical_B\").tensor_name());\n}\n\nTEST(GetGenericSignature, MissingSignature) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  GenericSignature signature;\n  const Status status =\n      GetGenericSignature(\"generic_bindings\", meta_graph_def, &signature);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(HasSubstr(status.error_message(),\n                        \"Missing generic signature named \\\"generic_bindings\\\"\"))\n      << status.error_message();\n}\n\nTEST(GetGenericSignature, WrongSignatureType) {\n  tensorflow::MetaGraphDef meta_graph_def;\n  Signatures signatures;\n  (*signatures.mutable_named_signatures())[\"generic_bindings\"]\n      .mutable_regression_signature();\n  (*meta_graph_def.mutable_collection_def())[kSignaturesKey]\n      .mutable_any_list()\n      ->add_value()\n      ->PackFrom(signatures);\n\n  GenericSignature signature;\n  const Status status =\n      GetGenericSignature(\"generic_bindings\", meta_graph_def, &signature);\n  ASSERT_FALSE(status.ok());\n  EXPECT_TRUE(StringPiece(status.error_message())\n                  .contains(\"Expected a generic signature:\"))\n      << status.error_message();\n}\n\n// BindGeneric Tests.\n\nTEST_F(GenericSignatureTest, BindGenericInputsBasic) {\n  const std::vector<std::pair<string, Tensor>> inputs = {\n      {\"logical_A\", test::AsTensor<float>({-1.0})},\n      {\"logical_B\", test::AsTensor<float>({-2.0})}};\n\n  std::vector<std::pair<string, Tensor>> bound_inputs;\n  TF_ASSERT_OK(BindGenericInputs(signature_, inputs, &bound_inputs));\n\n  EXPECT_EQ(\"graph_A\", bound_inputs[0].first);\n  EXPECT_EQ(\"graph_B\", bound_inputs[1].first);\n  test::ExpectTensorEqual<float>(test::AsTensor<float>({-1.0}),\n                                 bound_inputs[0].second);\n  test::ExpectTensorEqual<float>(test::AsTensor<float>({-2.0}),\n                                 bound_inputs[1].second);\n}\n\nTEST_F(GenericSignatureTest, BindGenericInputsMissingBinding) {\n  const std::vector<std::pair<string, Tensor>> inputs = {\n      {\"logical_A\", test::AsTensor<float>({-42.0})},\n      {\"logical_MISSING\", test::AsTensor<float>({-43.0})}};\n\n  std::vector<std::pair<string, Tensor>> bound_inputs;\n  const Status status = BindGenericInputs(signature_, inputs, &bound_inputs);\n  ASSERT_FALSE(status.ok());\n}\n\nTEST_F(GenericSignatureTest, BindGenericNamesBasic) {\n  const std::vector<string> input_names = {\"logical_B\", \"logical_A\"};\n  std::vector<string> bound_names;\n  TF_ASSERT_OK(BindGenericNames(signature_, input_names, &bound_names));\n\n  EXPECT_EQ(\"graph_B\", bound_names[0]);\n  EXPECT_EQ(\"graph_A\", bound_names[1]);\n}\n\nTEST_F(GenericSignatureTest, BindGenericNamesMissingBinding) {\n  const std::vector<string> input_names = {\"logical_B\", \"logical_MISSING\"};\n  std::vector<string> bound_names;\n  const Status status = BindGenericNames(signature_, input_names, &bound_names);\n  ASSERT_FALSE(status.ok());\n}\n\n}  // namespace\n}  // namespace serving\n}  // namespace tensorflow\n",
			"file": "tensorflow/tensorflow/contrib/session_bundle/signature_test.cc",
			"file_size": 23166,
			"file_write_time": 131219627756964250,
			"settings":
			{
				"buffer_size": 23164,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_session_bundle.h",
			"settings":
			{
				"buffer_size": 1264,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/BUILD",
			"settings":
			{
				"buffer_size": 7980,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_signature.h",
			"settings":
			{
				"buffer_size": 1452,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_signature.cc",
			"settings":
			{
				"buffer_size": 6192,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n\"\"\"Test a Fast R-CNN network on an imdb (image database).\"\"\"\n\nfrom fast_rcnn.config import cfg, get_output_dir\nfrom fast_rcnn.bbox_transform import clip_boxes, bbox_transform_inv\nimport argparse\nfrom utils.timer import Timer\nimport numpy as np\nimport cv2\nimport caffe\nfrom fast_rcnn.nms_wrapper import nms\nimport six.moves.cPickle as pickle\nfrom utils.blob import im_list_to_blob\nimport os\nfrom six.moves import range\n\n\ndef _get_image_blob(im):\n    \"\"\"Converts an image into a network input.\n\n    Arguments:\n        im (ndarray): a color image in BGR order\n\n    Returns:\n        blob (ndarray): a data blob holding an image pyramid\n        im_scale_factors (list): list of image scales (relative to im) used\n            in the image pyramid\n    \"\"\"\n    im_orig = im.astype(np.float32, copy=True)\n    im_orig -= cfg.PIXEL_MEANS\n\n    im_shape = im_orig.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n\n    processed_ims = []\n    im_scale_factors = []\n\n    for target_size in cfg.TEST.SCALES:\n        im_scale = float(target_size) / float(im_size_min)\n        # Prevent the biggest axis from being more than MAX_SIZE\n        if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n            im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\n\n        im = cv2.resize(\n            im_orig,\n            None, None,\n            fx=im_scale, fy=im_scale,\n            interpolation=cv2.INTER_LINEAR\n        )\n\n        im_scale_factors.append(im_scale)\n        processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, np.array(im_scale_factors)\n\n\ndef _get_rois_blob(im_rois, im_scale_factors):\n    \"\"\"\n    Converts RoIs into network inputs.\n\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        im_scale_factors (list): scale factors as returned by _get_image_blob\n\n    Returns:\n        blob (ndarray): R x 5 matrix of RoIs in the image pyramid\n    \"\"\"\n    rois, levels = _project_im_rois(im_rois, im_scale_factors)\n    rois_blob = np.hstack((levels, rois))\n\n    return rois_blob.astype(np.float32, copy=False)\n\n\ndef _project_im_rois(im_rois, scales):\n    \"\"\"Project image RoIs into the image pyramid built by _get_image_blob.\n\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        scales (list): scale factors as returned by _get_image_blob\n\n    Returns:\n        rois (ndarray): R x 4 matrix of projected RoI coordinates\n        levels (list): image pyramid levels used by each projected RoI\n    \"\"\"\n    im_rois = im_rois.astype(np.float, copy=False)\n\n    if len(scales) > 1:\n        widths = im_rois[:, 2] - im_rois[:, 0] + 1\n        heights = im_rois[:, 3] - im_rois[:, 1] + 1\n\n        areas = widths * heights\n        scaled_areas = areas[:, np.newaxis] * (scales[np.newaxis, :] ** 2)\n        diff_areas = np.abs(scaled_areas - 224 * 224)\n        levels = diff_areas.argmin(axis=1)[:, np.newaxis]\n    else:\n        levels = np.zeros((im_rois.shape[0], 1), dtype=np.int)\n\n    rois = im_rois * scales[levels]\n\n    return rois, levels\n\n\ndef _get_blobs(im, rois):\n    \"\"\"Convert an image and RoIs within that image into network inputs.\"\"\"\n    blobs = {'data': None, 'rois': None}\n    blobs['data'], im_scale_factors = _get_image_blob(im)\n\n    if not cfg.TEST.HAS_RPN:\n        blobs['rois'] = _get_rois_blob(rois, im_scale_factors)\n\n    return blobs, im_scale_factors\n\n\ndef im_detect(net, im, boxes=None):\n    \"\"\"Detect object classes in an image given object proposals.\n\n    Arguments:\n        net (caffe.Net): Fast R-CNN network to use\n        im (ndarray): color image to test (in BGR order)\n        boxes (ndarray): R x 4 array of object proposals or None (for RPN)\n\n    Returns:\n        scores (ndarray): R x K array of object class scores (K includes\n            background as object category 0)\n        boxes (ndarray): R x (4*K) array of predicted bounding boxes\n    \"\"\"\n    blobs, im_scales = _get_blobs(im, boxes)\n\n    # When mapping from image ROIs to feature map ROIs, there's some aliasing\n    # (some distinct image ROIs get mapped to the same feature ROI).\n    # Here, we identify duplicate feature ROIs, so we only compute features\n    # on the unique subset.\n    if cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n        v = np.array([1, 1e3, 1e6, 1e9, 1e12])\n        hashes = np.round(blobs['rois'] * cfg.DEDUP_BOXES).dot(v)\n\n        _, index, inv_index = np.unique(\n            hashes, return_index=True, return_inverse=True\n        )\n\n        blobs['rois'] = blobs['rois'][index, :]\n        boxes = boxes[index, :]\n\n    if cfg.TEST.HAS_RPN:\n        im_blob = blobs['data']\n\n        blobs['im_info'] = np.array(\n            [[im_blob.shape[2], im_blob.shape[3], im_scales[0]]],\n            dtype=np.float32\n        )\n\n    # reshape network inputs\n    net.blobs['data'].reshape(*(blobs['data'].shape))\n    if cfg.TEST.HAS_RPN:\n        net.blobs['im_info'].reshape(*(blobs['im_info'].shape))\n    else:\n        net.blobs['rois'].reshape(*(blobs['rois'].shape))\n\n    # do forward\n    forward_kwargs = {'data': blobs['data'].astype(np.float32, copy=False)}\n\n    if cfg.TEST.HAS_RPN:\n        forward_kwargs['im_info'] = blobs['im_info'].astype(\n            np.float32, copy=False\n        )\n    else:\n        forward_kwargs['rois'] = blobs['rois'].astype(np.float32, copy=False)\n\n    blobs_out = net.forward(**forward_kwargs)\n\n    if cfg.TEST.HAS_RPN:\n        assert len(im_scales) == 1, \"Only single-image batch implemented\"\n        rois = net.blobs['rois'].data.copy()\n        # unscale back to raw image space\n        boxes = rois[:, 1:5] / im_scales[0]\n\n    if cfg.TEST.SVM:\n        # use the raw scores before softmax under the assumption they\n        # were trained as linear SVMs\n        scores = net.blobs['cls_score'].data\n    else:\n        # use softmax estimated probabilities\n        scores = blobs_out['cls_prob']\n\n    if cfg.TEST.BBOX_REG:\n        # Apply bounding-box regression deltas\n        box_deltas = blobs_out['bbox_pred']\n        pred_boxes = bbox_transform_inv(boxes, box_deltas)\n        pred_boxes = clip_boxes(pred_boxes, im.shape)\n    else:\n        # Simply repeat the boxes, once for each class\n        pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n\n    if cfg.DEDUP_BOXES > 0 and not cfg.TEST.HAS_RPN:\n        # Map scores and predictions back to the original set of boxes\n        scores = scores[inv_index, :]\n        pred_boxes = pred_boxes[inv_index, :]\n\n    return scores, pred_boxes\n\n\ndef vis_detections(im, class_name, dets, thresh=0.3):\n    \"\"\"Visual debugging of detections.\"\"\"\n    import matplotlib.pyplot as plt\n\n    im = im[:, :, (2, 1, 0)]\n    for i in range(np.minimum(10, dets.shape[0])):\n        bbox = dets[i, :4]\n        score = dets[i, -1]\n\n        if score > thresh:\n            plt.cla()\n            plt.imshow(im)\n            plt.gca().add_patch(\n                plt.Rectangle((bbox[0], bbox[1]),\n                              bbox[2] - bbox[0],\n                              bbox[3] - bbox[1], fill=False,\n                              edgecolor='g', linewidth=3)\n                )\n            plt.title('{}  {:.3f}'.format(class_name, score))\n            plt.show()\n\n\ndef apply_nms(all_boxes, thresh):\n    \"\"\"\n    Apply non-maximum suppression to all predicted boxes output by the\n    test_net method.\n    \"\"\"\n    num_classes = len(all_boxes)\n    num_images = len(all_boxes[0])\n    nms_boxes = [[[] for _ in range(num_images)]\n                 for _ in range(num_classes)]\n\n    for cls_ind in range(num_classes):\n        for im_ind in range(num_images):\n            dets = all_boxes[cls_ind][im_ind]\n\n            if dets == []:\n                continue\n            # CPU NMS is much faster than GPU NMS when the number of boxes\n            # is relative small (e.g., < 10k)\n            # TODO(rbg): autotune NMS dispatch\n            keep = nms(dets, thresh, force_cpu=True)\n\n            if len(keep) == 0:\n                continue\n\n            nms_boxes[cls_ind][im_ind] = dets[keep, :].copy()\n\n    return nms_boxes\n\n\ndef test_net(net, imdb, max_per_image=100, thresh=0.05, vis=False):\n    \"\"\"Test a Fast R-CNN network on an image database.\"\"\"\n    num_images = len(imdb.image_index)\n    # all detections are collected into:\n    #    all_boxes[cls][image] = N x 5 array of detections in\n    #    (x1, y1, x2, y2, score)\n    all_boxes = [[[] for _ in range(num_images)]\n                 for _ in range(imdb.num_classes)]\n\n    output_dir = get_output_dir(imdb, net)\n\n    # timers\n    _t = {'im_detect' : Timer(), 'misc' : Timer()}\n\n    if not cfg.TEST.HAS_RPN:\n        roidb = imdb.roidb\n\n    for i in range(num_images):\n        # filter out any ground truth boxes\n        if cfg.TEST.HAS_RPN:\n            box_proposals = None\n        else:\n            # The roidb may contain ground-truth rois (for example, if the roidb\n            # comes from the training or val split). We only want to evaluate\n            # detection on the *non*-ground-truth rois. We select those the rois\n            # that have the gt_classes field set to 0, which means there's no\n            # ground truth.\n            box_proposals = roidb[i]['boxes'][roidb[i]['gt_classes'] == 0]\n\n        im = cv2.imread(imdb.image_path_at(i))\n        _t['im_detect'].tic()\n        scores, boxes = im_detect(net, im, box_proposals)\n        _t['im_detect'].toc()\n\n        _t['misc'].tic()\n        # skip j = 0, because it's the background class\n        for j in range(1, imdb.num_classes):\n            inds = np.where(scores[:, j] > thresh)[0]\n            cls_scores = scores[inds, j]\n            cls_boxes = boxes[inds, j*4:(j+1)*4]\n            cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])) \\\n                .astype(np.float32, copy=False)\n            keep = nms(cls_dets, cfg.TEST.NMS)\n            cls_dets = cls_dets[keep, :]\n            if vis:\n                vis_detections(im, imdb.classes[j], cls_dets)\n            all_boxes[j][i] = cls_dets\n\n        # Limit to max_per_image detections *over all classes*\n        if max_per_image > 0:\n            image_scores = np.hstack([all_boxes[j][i][:, -1]\n                                      for j in range(1, imdb.num_classes)])\n            if len(image_scores) > max_per_image:\n                image_thresh = np.sort(image_scores)[-max_per_image]\n                for j in range(1, imdb.num_classes):\n                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n        _t['misc'].toc()\n\n        print(\n            'im_detect: {:d}/{:d} {:.3f}s {:.3f}s'.format(\n                i + 1,\n                num_images,\n                _t['im_detect'].average_time,\n                _t['misc'].average_time\n            )\n        )\n\n    det_file = os.path.join(output_dir, 'detections.pkl')\n\n    with open(det_file, 'wb') as f:\n        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n\n    print('Evaluating detections')\n    imdb.evaluate_detections(all_boxes, output_dir)\n",
			"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/fast_rcnn/test.py",
			"file_size": 11299,
			"file_write_time": 131146832770000000,
			"settings":
			{
				"buffer_size": 11299,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/example/BUILD",
			"settings":
			{
				"buffer_size": 3629,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/setup.py",
			"settings":
			{
				"buffer_size": 5715,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom fast_rcnn.config import cfg\n#from nms.gpu_nms import gpu_nms\nfrom nms.cpu_nms import cpu_nms\n\n\ndef nms(dets, thresh, force_cpu=False):\n    \"\"\"Dispatch to either CPU or GPU NMS implementations.\"\"\"\n\n    if dets.shape[0] == 0:\n        return []\n\n    if cfg.USE_GPU_NMS and not force_cpu:\n        return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n    else:\n        return cpu_nms(dets, thresh)\n",
			"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/fast_rcnn/nms_wrapper.py",
			"file_size": 643,
			"file_write_time": 131146832770000000,
			"settings":
			{
				"buffer_size": 644,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "import os\nimport os.path as osp\nimport numpy as np\n# `pip install easydict` if you don't have it\nfrom easydict import EasyDict as edict\n\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n\"\"\"\nFast R-CNN config system.\n\nThis file specifies default config options for Fast R-CNN. You should not\nchange values in this file. Instead, you should write a config file (in yaml)\nand use cfg_from_file(yaml_file) to load it and override the default options.\n\nMost tools in $ROOT/tools take a --cfg option to specify an override file.\n    - See tools/{train,test}_net.py for example code that uses cfg_from_file()\n    - See experiments/cfgs/*.yml for example YAML config override files\n\"\"\"\n\n\n__C = edict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n#\n# Training options\n#\n\n__C.TRAIN = edict()\n\n# Scales to use during training (can list multiple scales)\n# Each scale is the pixel size of an image's shortest side\n__C.TRAIN.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TRAIN.MAX_SIZE = 1000\n\n# Images to use per minibatch\n__C.TRAIN.IMS_PER_BATCH = 2\n\n# Minibatch size (number of regions of interest [ROIs])\n__C.TRAIN.BATCH_SIZE = 128\n\n# Fraction of minibatch that is labeled foreground (i.e. class > 0)\n__C.TRAIN.FG_FRACTION = 0.25\n\n# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n__C.TRAIN.FG_THRESH = 0.5\n\n# Overlap threshold for a ROI to be considered background (class = 0 if\n# overlap in [LO, HI))\n__C.TRAIN.BG_THRESH_HI = 0.5\n__C.TRAIN.BG_THRESH_LO = 0.1\n\n# Use horizontally-flipped images during training?\n__C.TRAIN.USE_FLIPPED = True\n\n# Train bounding-box regressors\n__C.TRAIN.BBOX_REG = True\n\n# Overlap required between a ROI and ground-truth box in order for that ROI to\n# be used as a bounding-box regression training example\n__C.TRAIN.BBOX_THRESH = 0.5\n\n# Iterations between snapshots\n__C.TRAIN.SNAPSHOT_ITERS = 10000\n\n# solver.prototxt specifies the snapshot path prefix, this adds an optional\n# infix to yield the path: <prefix>[_<infix>]_iters_XYZ.caffemodel\n__C.TRAIN.SNAPSHOT_INFIX = ''\n\n# Use a prefetch thread in roi_data_layer.layer\n# So far I haven't found this useful; likely more engineering work is required\n__C.TRAIN.USE_PREFETCH = False\n\n# Normalize the targets (subtract empirical mean, divide by empirical stddev)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS = True\n# Deprecated (inside weights)\n__C.TRAIN.BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Normalize the targets using \"precomputed\" (or made up) means and stdevs\n# (BBOX_NORMALIZE_TARGETS must also be True)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED = False\n__C.TRAIN.BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n__C.TRAIN.BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n\n# Train using these proposals\n__C.TRAIN.PROPOSAL_METHOD = 'selective_search'\n\n# Make minibatches from images that have similar aspect ratios (i.e. both\n# tall and thin or both short and wide) in order to avoid wasting computation\n# on zero-padding.\n__C.TRAIN.ASPECT_GROUPING = True\n\n# Use RPN to detect objects\n__C.TRAIN.HAS_RPN = False\n# IOU >= thresh: positive example\n__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7\n# IOU < thresh: negative example\n__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3\n# If an anchor statisfied by positive and negative conditions set to negative\n__C.TRAIN.RPN_CLOBBER_POSITIVES = False\n# Max number of foreground examples\n__C.TRAIN.RPN_FG_FRACTION = 0.5\n# Total number of examples\n__C.TRAIN.RPN_BATCHSIZE = 256\n# NMS threshold used on RPN proposals\n__C.TRAIN.RPN_NMS_THRESH = 0.7\n# Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000\n# Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TRAIN.RPN_POST_NMS_TOP_N = 2000\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TRAIN.RPN_MIN_SIZE = 16\n# Deprecated (outside weights)\n__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Give the positive RPN examples weight of p * 1 / {num positives}\n# and give negatives a weight of (1 - p)\n# Set to -1.0 to use uniform example weighting\n__C.TRAIN.RPN_POSITIVE_WEIGHT = -1.0\n\n\n#\n# Testing options\n#\n\n__C.TEST = edict()\n\n# Scales to use during testing (can list multiple scales)\n# Each scale is the pixel size of an image's shortest side\n__C.TEST.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TEST.MAX_SIZE = 1000\n\n# Overlap threshold used for non-maximum suppression (suppress boxes with\n# IoU >= this threshold)\n__C.TEST.NMS = 0.3\n\n# Experimental: treat the (K+1) units in the cls_score layer as linear\n# predictors (trained, eg, with one-vs-rest SVMs).\n__C.TEST.SVM = False\n\n# Test using bounding-box regressors\n__C.TEST.BBOX_REG = True\n\n# Propose boxes\n__C.TEST.HAS_RPN = False\n\n# Test using these proposals\n__C.TEST.PROPOSAL_METHOD = 'selective_search'\n\n# NMS threshold used on RPN proposals\n__C.TEST.RPN_NMS_THRESH = 0.7\n# Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TEST.RPN_PRE_NMS_TOP_N = 6000\n# Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TEST.RPN_POST_NMS_TOP_N = 300\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TEST.RPN_MIN_SIZE = 16\n\n\n#\n# MISC\n#\n\n# The mapping from image coordinates to feature map coordinates might cause\n# some boxes that are distinct in image space to become identical in feature\n# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor\n# for identifying duplicate boxes.\n# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16\n__C.DEDUP_BOXES = 1./16.\n\n# Pixel mean values (BGR order) as a (1, 1, 3) array\n# We use the same pixel mean for all networks even though it's not exactly what\n# they were trained with\n__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n\n# For reproducibility\n__C.RNG_SEED = 3\n\n# A small number that's used many times\n__C.EPS = 1e-14\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), '..', '..'))\n\n# Data directory\n__C.DATA_DIR = osp.abspath(osp.join(__C.ROOT_DIR, 'data'))\n\n# Model directory\n__C.MODELS_DIR = osp.abspath(osp.join(__C.ROOT_DIR, 'models', 'pascal_voc'))\n\n# Name (or path to) the matlab executable\n__C.MATLAB = 'matlab'\n\n# Place outputs under an experiments directory\n__C.EXP_DIR = 'default'\n\n# Use GPU implementation of non-maximum suppression\n__C.USE_GPU_NMS = False\n\n# Default GPU device id\n__C.GPU_ID = 0\n\n\ndef get_output_dir(imdb, net=None):\n    \"\"\"Return the directory where experimental artifacts are placed.\n    If the directory does not exist, it is created.\n\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    \"\"\"\n    outdir = osp.abspath(osp.join(__C.ROOT_DIR, 'output', __C.EXP_DIR, imdb.name))\n    if net is not None:\n        outdir = osp.join(outdir, net.name)\n\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n\n    return outdir\n\n\ndef _merge_a_into_b(a, b):\n    \"\"\"Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    \"\"\"\n    if type(a) is not edict:\n        return\n\n    for k, v in a.items():\n        # a must specify keys that are in b\n        if k not in b:\n            raise KeyError('{} is not a valid config key'.format(k))\n\n        # the types must match, too\n        old_type = type(b[k])\n        if old_type is not type(v):\n            if isinstance(b[k], np.ndarray):\n                v = np.array(v, dtype=b[k].dtype)\n            else:\n                raise ValueError(\n                    'Type mismatch ({} vs. {}) for config key: {}'.format(\n                        type(b[k]),\n                        type(v),\n                        k\n                    )\n                )\n\n        # recursively merge dicts\n        if type(v) is edict:\n            try:\n                _merge_a_into_b(a[k], b[k])\n            except:\n                print('Error under config key: {}'.format(k))\n                raise\n        else:\n            b[k] = v\n\n\ndef cfg_from_file(filename):\n    \"\"\"Load a config file and merge it into the default options.\"\"\"\n    import yaml\n    with open(filename, 'r') as f:\n        yaml_cfg = edict(yaml.load(f))\n\n    _merge_a_into_b(yaml_cfg, __C)\n\n\ndef cfg_from_list(cfg_list):\n    \"\"\"Set config keys via list (e.g., from command line).\"\"\"\n    from ast import literal_eval\n\n    assert len(cfg_list) % 2 == 0\n\n    for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n        key_list = k.split('.')\n        d = __C\n\n        for subkey in key_list[:-1]:\n            assert subkey in d\n\n            d = d[subkey]\n\n        subkey = key_list[-1]\n\n        assert subkey in d\n\n        try:\n            value = literal_eval(v)\n        except:\n            # handle the case when v is a string literal\n            value = v\n\n        assert isinstance(value, type(d[subkey])), 'type {} does not match original type {}'.format(\n                type(value),\n                type(d[subkey])\n            )\n\n        d[subkey] = value\n",
			"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/fast_rcnn/config.py",
			"file_size": 9262,
			"file_write_time": 131146832770000000,
			"settings":
			{
				"buffer_size": 9263,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/example/obj_detector_fetch.py",
			"settings":
			{
				"buffer_size": 6070,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n\"\"\"Set up paths for Fast R-CNN.\"\"\"\n\nimport os.path as osp\nimport sys\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\nthis_dir = osp.dirname(__file__)\n\n# Add caffe to PYTHONPATH\ncaffe_path = osp.join(this_dir, '..', 'caffe', 'python')\nadd_path(caffe_path)\n\n# Add lib to PYTHONPATH\nlib_path = osp.join(this_dir, '..', 'lib')\nadd_path(lib_path)\n",
			"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/tools/_init_paths.py",
			"file_size": 627,
			"file_write_time": 131146832770000000,
			"settings":
			{
				"buffer_size": 627,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/model_servers/server_core.cc",
			"settings":
			{
				"buffer_size": 13967,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/model_servers/main.cc",
			"settings":
			{
				"buffer_size": 11839,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/model_servers/server_core.h",
			"settings":
			{
				"buffer_size": 11002,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/tensorflow/session_bundle_factory.h",
			"settings":
			{
				"buffer_size": 3917,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/example/obj_detector.proto",
			"settings":
			{
				"buffer_size": 1015,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/example/obj_detector.cc",
			"settings":
			{
				"buffer_size": 17650,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "error: cannot convert \n		void (tensorflow::serving::DetectService::WithAsyncMethod_GetConfiguration<		tensorflow::serving::DetectService::WithAsyncMethod_Detect<tensorflow::serving::DetectService::Service> >::*)\n				(grpc::ServerContext*,\n				 tensorflow::serving::ConfigurationRequest*,\n				 grpc::ServerAsyncResponseWriter<tensorflow::serving::DetectConfiguration>*,\n				 grpc::CompletionQueue*, grpc::ServerCompletionQueue*, void*)\n\nto 'MyReq<tensorflow::serving::DetectService::WithAsyncMethod_GetConfiguration<\n		tensorflow::serving::DetectService::WithAsyncMethod_Detect<tensorflow::serving::DetectService::Service> >, tensorflow::serving::DetectRequest, tensorflow::serving::DetectResponse>::type {\n\n		aka void (tensorflow::serving::DetectService::WithAsyncMethod_GetConfiguration<\n		  		  tensorflow::serving::DetectService::WithAsyncMethod_Detect<tensorflow::serving::DetectService::Service> >::*)\n				  		(grpc::ServerContext*, \n				  		 tensorflow::serving::DetectRequest*,\n				  		 grpc::ServerAsyncResponseWriter<tensorflow::serving::DetectResponse>*,\n				  		 grpc::CompletionQueue*,\n				  		 grpc::ServerCompletionQueue*,\n				  		 void*)\n\n\n &tensorflow::serving::DetectService::WithAsyncMethod_Detect<tensorflow::serving::DetectService::Service>::RequestDetect' to \n 		'MyReq<tensorflow::serving::DetectService::WithAsyncMethod_GetConfiguration<tensorflow::serving::DetectService::WithAsyncMethod_Detect<tensorflow::serving::DetectService::Service> >, tensorflow::serving::DetectRequest, tensorflow::serving::DetectResponse>::type {\n\n 				aka void (tensorflow::serving::DetectService::WithAsyncMethod_GetConfiguration<\n 						tensorflow::serving::DetectService::WithAsyncMethod_Detect<tensorflow::serving::DetectService::Service> >::*)(\n 						grpc::ServerContext*, \n 						tensorflow::serving::DetectRequest*, \n 						grpc::ServerAsyncResponseWriter<\n 								tensorflow::serving::DetectResponse>*, \n 								grpc::CompletionQueue*, \n 								grpc::ServerCompletionQueue*, \n 								void*)}'\n",
			"settings":
			{
				"buffer_size": 2008,
				"line_ending": "Windows",
				"name": "error: cannot convert"
			}
		},
		{
			"contents": "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/util/command_line_flags.h\"\n#include \"tensorflow/core/lib/core/stringpiece.h\"\n#include \"tensorflow/core/platform/logging.h\"\n\nnamespace tensorflow {\nnamespace {\n\nbool ParseStringFlag(tensorflow::StringPiece arg, tensorflow::StringPiece flag,\n                     string* dst, bool* value_parsing_ok) {\n  *value_parsing_ok = true;\n  if (arg.Consume(\"--\") && arg.Consume(flag) && arg.Consume(\"=\")) {\n    *dst = arg.ToString();\n    return true;\n  }\n\n  return false;\n}\n\nbool ParseInt32Flag(tensorflow::StringPiece arg, tensorflow::StringPiece flag,\n                    tensorflow::int32* dst, bool* value_parsing_ok) {\n  *value_parsing_ok = true;\n  if (arg.Consume(\"--\") && arg.Consume(flag) && arg.Consume(\"=\")) {\n    char extra;\n    if (sscanf(arg.data(), \"%d%c\", dst, &extra) != 1) {\n      LOG(ERROR) << \"Couldn't interpret value \" << arg << \" for flag \" << flag\n                 << \".\";\n      *value_parsing_ok = false;\n    }\n    return true;\n  }\n\n  return false;\n}\n\nbool ParseBoolFlag(tensorflow::StringPiece arg, tensorflow::StringPiece flag,\n                   bool* dst, bool* value_parsing_ok) {\n  *value_parsing_ok = true;\n  if (arg.Consume(\"--\") && arg.Consume(flag)) {\n    if (arg.empty()) {\n      *dst = true;\n      return true;\n    }\n\n    if (arg == \"=true\") {\n      *dst = true;\n      return true;\n    } else if (arg == \"=false\") {\n      *dst = false;\n      return true;\n    } else {\n      LOG(ERROR) << \"Couldn't interpret value \" << arg << \" for flag \" << flag\n                 << \".\";\n      *value_parsing_ok = false;\n      return true;\n    }\n  }\n\n  return false;\n}\n\n}  // namespace\n\nFlag::Flag(const char* name, tensorflow::int32* dst)\n    : name_(name), type_(TYPE_INT), int_value_(dst) {}\n\nFlag::Flag(const char* name, bool* dst)\n    : name_(name), type_(TYPE_BOOL), bool_value_(dst) {}\n\nFlag::Flag(const char* name, string* dst)\n    : name_(name), type_(TYPE_STRING), string_value_(dst) {}\n\nbool Flag::Parse(string arg, bool* value_parsing_ok) const {\n  bool result = false;\n  if (type_ == TYPE_INT) {\n    result = ParseInt32Flag(arg, name_, int_value_, value_parsing_ok);\n  } else if (type_ == TYPE_BOOL) {\n    result = ParseBoolFlag(arg, name_, bool_value_, value_parsing_ok);\n  } else if (type_ == TYPE_STRING) {\n    result = ParseStringFlag(arg, name_, string_value_, value_parsing_ok);\n  }\n  return result;\n}\n\nbool ParseFlags(int* argc, char** argv, const std::vector<Flag>& flag_list) {\n  bool result = true;\n  std::vector<char*> unknown_flags;\n  for (int i = 1; i < *argc; ++i) {\n    if (string(argv[i]) == \"--\") {\n      while (i < *argc) {\n        unknown_flags.push_back(argv[i]);\n        ++i;\n      }\n      break;\n    }\n\n    bool was_found = false;\n    for (const Flag& flag : flag_list) {\n      bool value_parsing_ok;\n      was_found = flag.Parse(argv[i], &value_parsing_ok);\n      if (!value_parsing_ok) {\n        result = false;\n      }\n      if (was_found) {\n        break;\n      }\n    }\n    if (!was_found) {\n      unknown_flags.push_back(argv[i]);\n    }\n  }\n  // Passthrough any extra flags.\n  int dst = 1;  // Skip argv[0]\n  for (char* f : unknown_flags) {\n    argv[dst++] = f;\n  }\n  argv[dst++] = nullptr;\n  *argc = unknown_flags.size() + 1;\n  return result;\n}\n\n}  // namespace tensorflow\n",
			"file": "tensorflow/tensorflow/core/util/command_line_flags.cc",
			"file_size": 6497,
			"file_write_time": 131219627769975902,
			"settings":
			{
				"buffer_size": 3919,
				"line_ending": "Unix"
			}
		},
		{
			"settings":
			{
				"buffer_size": 0,
				"line_ending": "Windows"
			}
		},
		{
			"file": "tensorflow_serving/servables/tensorflow/session_bundle_config.proto",
			"settings":
			{
				"buffer_size": 3642,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "#!/usr/bin/env python3\n\n# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n\"\"\"\nDemo script showing detections in sample images.\n\nSee README.md for installation instructions before running.\n\"\"\"\n\nimport _init_paths\nfrom fast_rcnn.config import cfg\nfrom fast_rcnn.test import im_detect\nfrom fast_rcnn.nms_wrapper import nms\nfrom utils.timer import Timer\nfrom six.moves import range\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.io as sio\nimport caffe\nimport os\nimport sys\nimport cv2\nimport argparse\n\nCLASSES = (\n    '__background__',\n    'aeroplane', 'bicycle', 'bird', 'boat',\n    'bottle', 'bus', 'car', 'cat', 'chair',\n    'cow', 'diningtable', 'dog', 'horse',\n    'motorbike', 'person', 'pottedplant',\n    'sheep', 'sofa', 'train', 'tvmonitor'\n)\n\nNETS = {\n    'vgg16': (\n        'VGG16',\n        'VGG16_faster_rcnn_final.caffemodel'\n    ),\n    'zf': (\n        'ZF',\n        'ZF_faster_rcnn_final.caffemodel'\n    )\n}\n\n\ndef vis_detections(im, class_name, dets, thresh=0.5):\n    \"\"\"\n    Draw detected bounding boxes.\n    \"\"\"\n    inds = np.where(dets[:, -1] >= thresh)[0]\n\n    if len(inds) == 0:\n        return\n\n    im = im[:, :, (2, 1, 0)]\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.imshow(im, aspect='equal')\n    for i in inds:\n        bbox = dets[i, :4]\n        score = dets[i, -1]\n\n        ax.add_patch(\n            plt.Rectangle((bbox[0], bbox[1]),\n                          bbox[2] - bbox[0],\n                          bbox[3] - bbox[1], fill=False,\n                          edgecolor='red', linewidth=3.5)\n            )\n        ax.text(bbox[0], bbox[1] - 2,\n                '{:s} {:.3f}'.format(class_name, score),\n                bbox=dict(facecolor='blue', alpha=0.5),\n                fontsize=14, color='white')\n\n    ax.set_title('{} detections with p({} | box) >= {:.1f}'.format(\n        class_name, class_name, thresh),\n        fontsize=14\n    )\n    plt.axis('off')\n    plt.tight_layout()\n    plt.draw()\n\n\ndef demo(net, image_name):\n    \"\"\"\n    Detect object classes in an image using pre-computed object proposals.\n    \"\"\"\n\n    # Load the demo image\n    im_file = os.path.join(cfg.DATA_DIR, 'demo', image_name)\n    im = cv2.imread(im_file)\n\n    # Detect all object classes and regress object bounds\n    timer = Timer()\n    timer.tic()\n    scores, boxes = im_detect(net, im)\n    timer.toc()\n\n    print('Detection took {:.3f}s for {:d} object proposals'.format(\n            timer.total_time, boxes.shape[0]\n        )\n    )\n\n    # Visualize detections for each class\n    CONF_THRESH = 0.8\n    NMS_THRESH = 0.3\n    for cls_ind, cls in enumerate(CLASSES[1:]):\n        cls_ind += 1  # because we skipped background\n        cls_boxes = boxes[:, 4*cls_ind:4*(cls_ind + 1)]\n        cls_scores = scores[:, cls_ind]\n\n        if cls_ind == 15:\n            print(\"class=15, scores=\")\n            print(cls_scores)\n\n        dets = np.hstack(\n                (cls_boxes, cls_scores[:, np.newaxis])\n            ).astype(np.float32)\n\n        keep = nms(dets, NMS_THRESH)\n        dets = dets[keep, :]\n        vis_detections(im, cls, dets, thresh=CONF_THRESH)\n\n\ndef parse_args():\n    \"\"\"Parse input arguments.\"\"\"\n    parser = argparse.ArgumentParser(description='Faster R-CNN demo')\n    parser.add_argument(\n        '--gpu', dest='gpu_id', help='GPU device id to use [0]',\n        default=0, type=int\n    )\n    parser.add_argument(\n        '--cpu', dest='cpu_mode',\n        help='Use CPU mode (overrides --gpu)',\n        action='store_true'\n    )\n    parser.add_argument(\n        '--net', dest='demo_net', help='Network to use [vgg16]',\n        choices=NETS.keys(), default='vgg16'\n    )\n\n    args = parser.parse_args()\n\n    return args\n\nif __name__ == '__main__':\n    cfg.TEST.HAS_RPN = True  # Use RPN for proposals\n\n    args = parse_args()\n\n    prototxt = os.path.join(cfg.MODELS_DIR, NETS[args.demo_net][0],\n                            'faster_rcnn_alt_opt', 'faster_rcnn_test.pt')\n    caffemodel = os.path.join(cfg.DATA_DIR, 'faster_rcnn_models',\n                              NETS[args.demo_net][1])\n\n    if not os.path.isfile(caffemodel):\n        raise IOError(('{:s} not found.\\nDid you run ./data/script/'\n                       'fetch_faster_rcnn_models.sh?').format(caffemodel))\n\n    if args.cpu_mode:\n        caffe.set_mode_cpu()\n    else:\n        caffe.set_mode_gpu()\n        caffe.set_device(args.gpu_id)\n        cfg.GPU_ID = args.gpu_id\n    net = caffe.Net(prototxt, caffemodel, caffe.TEST)\n\n    print('\\n\\nLoaded network {:s}'.format(caffemodel))\n\n    # Warmup on a dummy image\n    im = 128 * np.ones((300, 500, 3), dtype=np.uint8)\n    for i in range(2):\n        _, _ = im_detect(net, im)\n\n    im_names = [\n        '000456.jpg', '000542.jpg', '001150.jpg',\n        '001763.jpg', '004545.jpg'\n    ]\n\n    for im_name in im_names:\n        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n        print('Demo for data/demo/{}'.format(im_name))\n        demo(net, im_name)\n\n    plt.show()\n\n",
			"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/tools/demo.py",
			"file_size": 5018,
			"file_write_time": 131146832770000000,
			"settings":
			{
				"buffer_size": 5115,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/example/obj_detector_pb2.py",
			"settings":
			{
				"buffer_size": 13859,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/example/obj_detector_utils.h",
			"settings":
			{
				"buffer_size": 1611,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/example/obj_detector_utils.cc",
			"settings":
			{
				"buffer_size": 11082,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/example/obj_detector_client.py",
			"settings":
			{
				"buffer_size": 6911,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/example/client_util.py",
			"settings":
			{
				"buffer_size": 1087,
				"line_ending": "Unix",
				"name": "client_util"
			}
		},
		{
			"file": "tensorflow/tensorflow/tensorflow.bzl",
			"settings":
			{
				"buffer_size": 30603,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/tensorflow/session_bundle_source_adapter.cc",
			"settings":
			{
				"buffer_size": 2938,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tf_models/syntaxnet/tensorflow/tensorflow/contrib/session_bundle/session_bundle.h",
			"settings":
			{
				"buffer_size": 2320,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/contrib/session_bundle/session_bundle.h",
			"settings":
			{
				"buffer_size": 3336,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/tensorflow/simple_servers.h",
			"settings":
			{
				"buffer_size": 2367,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/contrib/session_bundle/signature.cc",
			"settings":
			{
				"buffer_size": 10854,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_serving_session.cc",
			"settings":
			{
				"buffer_size": 13629,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/tensorflow/predict_impl.cc",
			"settings":
			{
				"buffer_size": 5043,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/tensorflow/predict_impl.h",
			"settings":
			{
				"buffer_size": 1399,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/example/mnist_client.py",
			"settings":
			{
				"buffer_size": 5981,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "# Description: TensorFlow servables.\n\npackage(\n    default_visibility = [\n        \"//tensorflow_serving:internal\",\n    ],\n    features = [\n        \"-layering_check\",\n        \"-parse_headers\",\n    ],\n)\n\nlicenses([\"notice\"])  # Apache 2.0\n\nexports_files([\"LICENSE\"])\n\nfilegroup(\n    name = \"all_files\",\n    srcs = glob(\n        [\"**/*\"],\n        exclude = [\n            \"**/METADATA\",\n            \"**/OWNERS\",\n        ],\n    ),\n)\n\nload(\"//tensorflow_serving:serving.bzl\", \"serving_proto_library\")\n\nserving_proto_library(\n    name = \"session_bundle_config_proto\",\n    srcs = [\"session_bundle_config.proto\"],\n    cc_api_version = 2,\n    visibility = [\n        \"//visibility:public\",\n    ],\n    deps = [\n        \"@org_tensorflow//tensorflow/core:protos_all_cc\",\n        \"@protobuf//:cc_wkt_protos\",\n    ],\n)\n\ncc_library(\n    name = \"session_bundle_factory\",\n    srcs = [\"session_bundle_factory.cc\"],\n    hdrs = [\"session_bundle_factory.h\"],\n    visibility = [\n        \"//visibility:public\",\n    ],\n    deps = [\n        \":session_bundle_config_proto\",\n        \"//tensorflow_serving/batching:batching_session\",\n        \"//tensorflow_serving/batching:shared_batch_scheduler\",\n        \"//tensorflow_serving/resources:resource_values\",\n        \"//tensorflow_serving/resources:resources_proto\",\n        \"//tensorflow_serving/servables/tensorflow:serving_session\",\n        \"@org_tensorflow//tensorflow/contrib/session_bundle\",\n        \"@org_tensorflow//tensorflow/core:core_cpu\",\n        \"@org_tensorflow//tensorflow/core:lib\",\n        \"@org_tensorflow//tensorflow/core:tensorflow\",\n        \"@protobuf//:cc_wkt_protos\",\n    ],\n)\n\ncc_test(\n    name = \"session_bundle_factory_test\",\n    size = \"medium\",\n    srcs = [\"session_bundle_factory_test.cc\"],\n    data = [\"@org_tensorflow//tensorflow/contrib/session_bundle/example:half_plus_two\"],\n    deps = [\n        \":session_bundle_config_proto\",\n        \":session_bundle_factory\",\n        \"//tensorflow_serving/core/test_util:test_main\",\n        \"//tensorflow_serving/resources:resource_values\",\n        \"//tensorflow_serving/test_util\",\n        \"@org_tensorflow//tensorflow/contrib/session_bundle\",\n        \"@org_tensorflow//tensorflow/core:core_cpu\",\n        \"@org_tensorflow//tensorflow/core:lib\",\n        \"@org_tensorflow//tensorflow/core:tensorflow\",\n        \"@org_tensorflow//tensorflow/core:test\",\n        \"@org_tensorflow//tensorflow/core:testlib\",\n        \"@protobuf//:cc_wkt_protos\",\n    ],\n)\n\ncc_library(\n    name = \"session_bundle_source_adapter\",\n    srcs = [\"session_bundle_source_adapter.cc\"],\n    hdrs = [\"session_bundle_source_adapter.h\"],\n    visibility = [\n        \"//visibility:public\",\n    ],\n    deps = [\n        \":session_bundle_factory\",\n        \":session_bundle_source_adapter_proto\",\n        \"//tensorflow_serving/core:loader\",\n        \"//tensorflow_serving/core:simple_loader\",\n        \"//tensorflow_serving/core:source_adapter\",\n        \"//tensorflow_serving/core:storage_path\",\n        \"//tensorflow_serving/servables/tensorflow:serving_session\",\n        \"//tensorflow_serving/util:optional\",\n        \"@org_tensorflow//tensorflow/contrib/session_bundle\",\n        \"@org_tensorflow//tensorflow/core:core_cpu\",\n        \"@org_tensorflow//tensorflow/core:lib\",\n        \"@org_tensorflow//tensorflow/core:tensorflow\",\n    ],\n)\n\ncc_test(\n    name = \"session_bundle_source_adapter_test\",\n    size = \"medium\",\n    srcs = [\"session_bundle_source_adapter_test.cc\"],\n    data = [\"@org_tensorflow//tensorflow/contrib/session_bundle/example:half_plus_two\"],\n    # Link in all registered kernels.\n    linkstatic = 1,\n    deps = [\n        \":session_bundle_config_proto\",\n        \":session_bundle_source_adapter\",\n        \":session_bundle_source_adapter_proto\",\n        \"//tensorflow_serving/core:loader\",\n        \"//tensorflow_serving/core:servable_data\",\n        \"//tensorflow_serving/core:servable_id\",\n        \"//tensorflow_serving/core:source_adapter\",\n        \"//tensorflow_serving/core/test_util:source_adapter_test_util\",\n        \"//tensorflow_serving/core/test_util:test_main\",\n        \"//tensorflow_serving/test_util\",\n        \"//tensorflow_serving/util:any_ptr\",\n        \"@org_tensorflow//tensorflow/contrib/session_bundle\",\n        \"@org_tensorflow//tensorflow/core:core_cpu\",\n        \"@org_tensorflow//tensorflow/core:framework\",\n        \"@org_tensorflow//tensorflow/core:lib\",\n        \"@org_tensorflow//tensorflow/core:tensorflow\",\n        \"@org_tensorflow//tensorflow/core:test\",\n        \"@org_tensorflow//tensorflow/core:testlib\",\n    ],\n)\n\nserving_proto_library(\n    name = \"session_bundle_source_adapter_proto\",\n    srcs = [\"session_bundle_source_adapter.proto\"],\n    cc_api_version = 2,\n    visibility = [\n        \"//visibility:public\",\n    ],\n    deps = [\n        \":session_bundle_config_proto\",\n    ],\n)\n\ncc_library(\n    name = \"simple_servers\",\n    srcs = [\"simple_servers.cc\"],\n    hdrs = [\"simple_servers.h\"],\n    visibility = [\n        \"//visibility:public\",\n    ],\n    deps = [\n        \":session_bundle_source_adapter\",\n        \":session_bundle_source_adapter_proto\",\n        \"//tensorflow_serving/core:aspired_versions_manager_builder\",\n        \"//tensorflow_serving/core:eager_load_policy\",\n        \"//tensorflow_serving/core:loader\",\n        \"//tensorflow_serving/core:manager\",\n        \"//tensorflow_serving/core:source\",\n        \"//tensorflow_serving/core:source_adapter\",\n        \"//tensorflow_serving/core:storage_path\",\n        \"//tensorflow_serving/core:target\",\n        \"//tensorflow_serving/sources/storage_path:file_system_storage_path_source\",\n        \"//tensorflow_serving/sources/storage_path:file_system_storage_path_source_proto\",\n        \"@org_tensorflow//tensorflow/contrib/session_bundle\",\n        \"@org_tensorflow//tensorflow/core:lib\",\n        \"@org_tensorflow//tensorflow/core:tensorflow\",\n    ],\n)\n\ncc_test(\n    name = \"simple_servers_test\",\n    srcs = [\"simple_servers_test.cc\"],\n    data = [\"@org_tensorflow//tensorflow/contrib/session_bundle/example:half_plus_two\"],\n    # Link in all registered kernels.\n    linkstatic = 1,\n    deps = [\n        \":simple_servers\",\n        \"//tensorflow_serving/core:servable_handle\",\n        \"//tensorflow_serving/core/test_util:test_main\",\n        \"//tensorflow_serving/test_util\",\n        \"//tensorflow_serving/util:unique_ptr_with_deps\",\n        \"@org_tensorflow//tensorflow/contrib/session_bundle\",\n        \"@org_tensorflow//tensorflow/core:core_cpu\",\n        \"@org_tensorflow//tensorflow/core:framework\",\n        \"@org_tensorflow//tensorflow/core:lib\",\n        \"@org_tensorflow//tensorflow/core:tensorflow\",\n        \"@org_tensorflow//tensorflow/core:test\",\n        \"@org_tensorflow//tensorflow/core:testlib\",\n    ],\n)\n\ncc_library(\n    name = \"serving_session\",\n    srcs = [\"serving_session.cc\"],\n    hdrs = [\"serving_session.h\"],\n    deps = [\n        \"@org_tensorflow//tensorflow/core:core_cpu\",\n        \"@org_tensorflow//tensorflow/core:lib\",\n        \"@org_tensorflow//tensorflow/core:protos_all_cc\",\n        \"@org_tensorflow//tensorflow/core:tensorflow\",\n    ],\n)\n\ncc_library(\n    name = \"predict_impl\",\n    srcs = [\"predict_impl.cc\"],\n    hdrs = [\"predict_impl.h\"],\n    deps = [\n        \"//tensorflow_serving/apis:predict_proto\",\n        \"//tensorflow_serving/core:servable_handle\",\n        \"//tensorflow_serving/model_servers:server_core\",\n        \"@org_tensorflow//tensorflow/contrib/session_bundle\",\n        \"@org_tensorflow//tensorflow/core:lib\",\n        \"@org_tensorflow//tensorflow/core:protos_all_cc\",\n    ],\n)\n",
			"file": "tensorflow_serving/servables/tensorflow/BUILD",
			"file_size": 7520,
			"file_write_time": 131193571244206821,
			"settings":
			{
				"buffer_size": 7457,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "Searching 8909 files for \"@org_tensorflow//third_party/gpus/crosstool:crosstool\"\n\nC:\\dev\\vm\\data-share\\serving\\bazel-bin:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-genfiles:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-out:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-serving:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-testlogs:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\py\\numpy\\numpy_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_lib:\n    ERROR: Unable to open file\n\n0 matches\n\nSearching 8909 files for \"third_party/gpus/crosstool:crosstool\"\n\nC:\\dev\\vm\\data-share\\serving\\bazel-bin:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-genfiles:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-out:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-serving:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-testlogs:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\py\\numpy\\numpy_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_lib:\n    ERROR: Unable to open file\n\n0 matches\n\nSearching 8909 files for \":crosstool\"\n\nC:\\dev\\vm\\data-share\\serving\\bazel-bin:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-genfiles:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-out:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-serving:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-testlogs:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\BUILD:\n   48      name = \"ios\",\n   49      values = {\n   50:         \"crosstool_top\": \"//tools/osx/crosstool:crosstool\",\n   51      },\n   52      visibility = [\"//visibility:public\"],\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda_configure.bzl:\n  346    native.filegroup(\n  347        name = \"crosstool\",\n  348:       srcs = [\":CROSSTOOL\"],\n  349        output_licenses = [\"unencumbered\"],\n  350    )\n  ...\n  474    cc = find_cc(repository_ctx)\n  475    gcc_host_compiler_includes = _gcc_host_compiler_includes(repository_ctx, cc)\n  476:   _tpl(repository_ctx, \"crosstool:CROSSTOOL\",\n  477         {\n  478             \"%{cuda_include_path}\": cuda_toolkit_path + '/include',\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\py\\numpy\\numpy_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_lib:\n    ERROR: Unable to open file\n\n3 matches across 2 files\n\n\nSearching 8909 files for \"/crosstool\"\n\nC:\\dev\\vm\\data-share\\serving\\bazel-bin:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-genfiles:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-out:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-serving:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-testlogs:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\BUILD:\n   16      name = \"android\",\n   17      values = {\n   18:         \"crosstool_top\": \"//external:android/crosstool\",\n   19      },\n   20      visibility = [\"//visibility:public\"],\n   ..\n   24      name = \"android_arm\",\n   25      values = {\n   26:         \"crosstool_top\": \"//external:android/crosstool\",\n   27          \"android_cpu\": \"armeabi-v7a\",\n   28      },\n   ..\n   33      name = \"android_arm64\",\n   34      values = {\n   35:         \"crosstool_top\": \"//external:android/crosstool\",\n   36          \"android_cpu\": \"arm64-v8a\",\n   37      },\n   ..\n   48      name = \"ios\",\n   49      values = {\n   50:         \"crosstool_top\": \"//tools/osx/crosstool:crosstool\",\n   51      },\n   52      visibility = [\"//visibility:public\"],\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\BUILD:\n  652  # a command such as the following must be used:\n  653  # bazel build -c opt tensorflow/core:android_tensorflow_lib \\\n  654: # --crosstool_top=//external:android/crosstool \\\n  655  # --cpu=armeabi-v7a \\\n  656  # --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\examples\\android\\build.gradle:\n   57      commandLine bazel_location, 'build', '-c', 'opt', \\\n   58        'tensorflow/examples/android:tensorflow_native_libs', \\\n   59:        '--crosstool_top=//external:android/crosstool', \\\n   60         '--cpu=' + cpuType, \\\n   61         '--host_crosstool_top=@bazel_tools//tools/cpp:toolchain'\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\g3doc\\get_started\\os_setup.md:\n  979  GitHub issue: https://github.com/tensorflow/tensorflow/issues/1066\n  980  \n  981: Solution: Add the following compiler flags to third_party/gpus/crosstool/CROSSTOOL\n  982  \n  983  cxx_flag: \"-D_MWAITXINTRIN_H_INCLUDED\"\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\tools\\benchmark\\BUILD:\n   61  # A typical Android build command will look like the following:\n   62  # bazel build -c opt tensorflow/core:android_tensorflow_lib \\\n   63: # --crosstool_top=//external:android/crosstool \\\n   64  # --cpu=armeabi-v7a \\\n   65  # --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\tools\\benchmark\\README.md:\n   13  ```bash\n   14  $bazel build -c opt \\\n   15:   --crosstool_top=//external:android/crosstool \\\n   16    --cpu=armeabi-v7a \\\n   17    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\crosstool\\clang\\bin\\crosstool_wrapper_driver_is_not_gcc.tpl:\n   31  NOTES:\n   32    Changes to the contents of this file must be propagated from\n   33:   //third_party/gpus/crosstool/crosstool_wrapper_is_not_gcc to\n   34:   //third_party/gpus/crosstool/v*/*/clang/bin/crosstool_wrapper_is_not_gcc\n   35  \"\"\"\n   36  \n   ..\n   56  \n   57  def Log(s):\n   58:   print('gpus/crosstool: {0}'.format(s))\n   59  \n   60  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\crosstool\\CROSSTOOL.tpl:\n   48    tool_path { name: \"dwp\" path: \"/usr/bin/dwp\" }\n   49    # As part of the TensorFlow release, we place some cuda-related compilation\n   50:   # files in @local_config_cuda//crosstool/clang/bin, and this relative\n   51    # path, combined with the rest of our Bazel configuration causes our\n   52    # compilation to use those files.\n   53:   tool_path { name: \"gcc\" path: \"clang/bin/crosstool_wrapper_driver_is_not_gcc\" }\n   54    # Use \"-std=c++11\" for nvcc. For consistency, force both the host compiler\n   55    # and the device compiler to use \"-std=c++11\".\n   ..\n  169    tool_path { name: \"cpp\" path: \"/usr/bin/cpp\" }\n  170    tool_path { name: \"dwp\" path: \"/usr/bin/dwp\" }\n  171:   tool_path { name: \"gcc\" path: \"clang/bin/crosstool_wrapper_driver_is_not_gcc\" }\n  172    cxx_flag: \"-std=c++11\"\n  173    ar_flag: \"-static\"\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda_configure.bzl:\n  353  \n  354  _DUMMY_CROSSTOOL_BUILD_FILE = \"\"\"\n  355: load(\"//crosstool:error_gpu_disabled.bzl\", \"error_gpu_disabled\")\n  356  \n  357  error_gpu_disabled()\n  ...\n  480         })\n  481    _tpl(repository_ctx,\n  482:        \"crosstool:clang/bin/crosstool_wrapper_driver_is_not_gcc\",\n  483         {\n  484             \"%{cpu_compiler}\": str(cc),\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\py\\numpy\\numpy_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tools\\bazel.rc:\n    1  # Autogenerated by configure: DO NOT EDIT\n    2: build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\n    3  build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n    4  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tools\\bazel.rc.template:\n    1: build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\n    2  build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n    3  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_lib:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\configure:\n  230  # Configure the gcc host compiler to use\n  231  export WARNING=$DO_NOT_SUBMIT_WARNING\n  232: perl -pi -e \"s,CPU_COMPILER = \\('.*'\\),# \\$ENV{WARNING}\\nCPU_COMPILER = ('$GCC_HOST_COMPILER_PATH'),s\" third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n  233: perl -pi -e \"s,GCC_HOST_COMPILER_PATH = \\('.*'\\),# \\$ENV{WARNING}\\nGCC_HOST_COMPILER_PATH = ('$GCC_HOST_COMPILER_PATH'),s\" third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n  234  \n  235  # Configure the platform name.\n  ...\n  288    }\n  289    export CUDA_GEN_CODES_OPTS=$(CudaGenCodeOpts ${TF_CUDA_COMPUTE_CAPABILITIES//,/ })\n  290:   perl -pi -0 -e 's,\\n( *)([^\\n]*supported_cuda_compute_capabilities\\s*=\\s*\\[).*?(\\]),\\n\\1# $ENV{WARNING}\\n\\1\\2$ENV{CUDA_GEN_CODES_OPTS}\\3,s' third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc\n  291    function CudaVersionOpts() {\n  292      OUTPUT=\"\"\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\BUILD:\n   16      name = \"android\",\n   17      values = {\n   18:         \"crosstool_top\": \"//external:android/crosstool\",\n   19      },\n   20      visibility = [\"//visibility:public\"],\n   ..\n   24      name = \"android_arm\",\n   25      values = {\n   26:         \"crosstool_top\": \"//external:android/crosstool\",\n   27          \"android_cpu\": \"armeabi-v7a\",\n   28      },\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\BUILD:\n  605  # a command such as the following must be used:\n  606  # bazel build -c opt tensorflow/core:android_tensorflow_lib \\\n  607: # --crosstool_top=//external:android/crosstool \\\n  608  # --cpu=armeabi-v7a \\\n  609  # --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\tools\\benchmark\\BUILD:\n   60  # A typical Android build command will look like the following:\n   61  # bazel build -c opt tensorflow/core:android_tensorflow_lib \\\n   62: # --crosstool_top=//external:android/crosstool \\\n   63  # --cpu=armeabi-v7a \\\n   64  # --host_crosstool_top=@bazel_tools//tools/cpp:toolchain\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\tools\\benchmark\\README.md:\n   13  ```bash\n   14  $bazel build -c opt \\\n   15:   --crosstool_top=//external:android/crosstool \\\n   16    --cpu=armeabi-v7a \\\n   17    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \\\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\third_party\\gpus\\crosstool\\clang\\bin\\crosstool_wrapper_driver_is_not_gcc:\n   31  NOTES:\n   32    Changes to the contents of this file must be propagated from\n   33:   //third_party/gpus/crosstool/crosstool_wrapper_is_not_gcc to\n   34:   //third_party/gpus/crosstool/v*/*/clang/bin/crosstool_wrapper_is_not_gcc\n   35  \"\"\"\n   36  \n   ..\n   55  \n   56  def Log(s):\n   57:   print 'gpus/crosstool: {0}'.format(s)\n   58  \n   59  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\third_party\\gpus\\crosstool\\CROSSTOOL:\n   44    tool_path { name: \"dwp\" path: \"/usr/bin/dwp\" }\n   45    # As part of the TensorFlow release, we place some cuda-related compilation\n   46:   # files in third_party/gpus/crosstool/clang/bin, and this relative\n   47    # path, combined with the rest of our Bazel configuration causes our\n   48    # compilation to use those files.\n   49:   tool_path { name: \"gcc\" path: \"clang/bin/crosstool_wrapper_driver_is_not_gcc\" }\n   50    # Use \"-std=c++11\" for nvcc. For consistency, force both the host compiler\n   51    # and the device compiler to use \"-std=c++11\".\n   ..\n  168    tool_path { name: \"cpp\" path: \"/usr/bin/cpp\" }\n  169    tool_path { name: \"dwp\" path: \"/usr/bin/dwp\" }\n  170:   tool_path { name: \"gcc\" path: \"clang/bin/crosstool_wrapper_driver_is_not_gcc\" }\n  171    cxx_flag: \"-std=c++11\"\n  172    ar_flag: \"-static\"\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tools\\bazel.rc.template:\n    1: build:cuda --crosstool_top=//third_party/gpus/crosstool\n    2  build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n    3  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tools\\bazel.rc:\n    1: build:cuda --crosstool_top=//third_party/gpus/crosstool\n    2  \n    3  build --define=use_fast_cpp_protos=true\n\nC:\\dev\\vm\\data-share\\serving\\tools\\bazel.rc:\n    1: build:cuda --crosstool_top=@org_tensorflow//third_party/gpus/crosstool\n    2  build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n    3  \n\n44 matches across 21 files\n\n\nSearching 8909 files for \"local_config_cuda\"\n\nC:\\dev\\vm\\data-share\\serving\\bazel-bin:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-genfiles:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-out:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-serving:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-testlogs:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\caffe.BUILD:\n    1  load(\"@org_tensorflow//tensorflow:tensorflow.bzl\", \"if_cuda\")\n    2  load(\"@caffe_tools//:config.bzl\", \"if_pycaffe\")\n    3: load(\"@local_config_cuda//cuda:platform.bzl\",\n    4       \"cuda_sdk_version\",\n    5       \"cudnn_library_path\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\platform\\default\\build_config\\BUILD:\n   10  load(\"//tensorflow:tensorflow.bzl\", \"tf_copts\")\n   11  load(\"//tensorflow:tensorflow.bzl\", \"tf_cuda_library\")\n   12: load(\"@local_config_cuda//cuda:platform.bzl\", \"cuda_library_path\")\n   13  \n   14  cc_library(\n   ..\n   33          \"//tensorflow/stream_executor\",\n   34      ] + select({\n   35:         \"@local_config_cuda//cuda:darwin\": [\"IOKit\"],\n   36          \"//conditions:default\": [],\n   37      }),\n   ..\n   92      name = \"cuda\",\n   93      data = [\n   94:         \"@local_config_cuda//cuda:{}\".format(cuda_library_path(\"cudart\")),\n   95      ],\n   96      linkopts = select({\n   97:         \"@local_config_cuda//cuda:darwin\": [\n   98:             \"-Wl,-rpath,../local_config_cuda/cuda/lib\",\n   99:             \"-Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib\",\n  100          ],\n  101          \"//conditions:default\": [\n  102:             \"-Wl,-rpath,../local_config_cuda/cuda/lib64\",\n  103:             \"-Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64\",\n  104          ],\n  105      }),\n  106      deps = [\n  107:         \"@local_config_cuda//cuda:cudart\",\n  108      ],\n  109  )\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\platform\\default\\gpu\\BUILD:\n   16      cuda_deps = [\n   17          \"//tensorflow/core:stream_executor\",\n   18:         \"@local_config_cuda//cuda:cuda_headers\",\n   19:         \"@local_config_cuda//cuda:cupti_headers\",\n   20      ],\n   21:     data = [\"@local_config_cuda//cuda:cupti_dsos\"],\n   22      visibility = [\"//visibility:public\"],\n   23  )\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\BUILD:\n   28      data = [\n   29          \"//tensorflow/core:cuda\",\n   30:         \"@local_config_cuda//cuda:cublas\",\n   31:         \"@local_config_cuda//cuda:cudnn\",\n   32:         \"@local_config_cuda//cuda:cufft\",\n   33:         \"@local_config_cuda//cuda:curand\",\n   34      ],\n   35      linkopts = [\n   ..\n   39      deps = [\n   40          \"//tensorflow/core:lib\",\n   41:         \"@local_config_cuda//cuda:cuda_headers\",\n   42      ],\n   43      alwayslink = 1,\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:\n  142    auto rpaths = new std::vector<string>;\n  143  #if defined(__APPLE__)\n  144:   rpaths->push_back(\"driver/driver_sh.runfiles/local_config_cuda/cuda/lib\");\n  145  #else\n  146:   rpaths->push_back(\"driver/driver_sh.runfiles/local_config_cuda/cuda/lib64\");\n  147  #endif\n  148    return rpaths;\n  ...\n  194  /* static */ string DsoLoader::GetCudaLibraryDirPath() {\n  195  #if defined(__APPLE__)\n  196:   return \"external/local_config_cuda/cuda/lib\";\n  197  #else\n  198:   return \"external/local_config_cuda/cuda/lib64\";\n  199  #endif\n  200  }\n  ...\n  202  /* static */ string DsoLoader::GetCudaDriverLibraryPath() {\n  203  #if defined(__APPLE__)\n  204:   return \"external/local_config_cuda/cuda/driver/lib\";\n  205  #else\n  206:   return \"external/local_config_cuda/cuda/driver/lib64\";\n  207  #endif\n  208  }\n  ...\n  210  /* static */ string DsoLoader::GetCudaCuptiLibraryPath() {\n  211  #if defined(__APPLE__)\n  212:   return \"external/local_config_cuda/cuda/extras/CUPTI/lib\";\n  213  #else\n  214:   return \"external/local_config_cuda/cuda/extras/CUPTI/lib64\";\n  215  #endif\n  216  }\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\tensorflow.bzl:\n   39  )\n   40  load(\n   41:     \"@local_config_cuda//cuda:build_defs.bzl\",\n   42      \"if_cuda\",\n   43  )\n   ..\n  392      return select({\n  393          \"//conditions:default\": [],\n  394:         \"@local_config_cuda//cuda:using_nvcc\": (\n  395              common_cuda_opts +\n  396              [\n  ...\n  399              ]\n  400          ),\n  401:         \"@local_config_cuda//cuda:using_clang\": (\n  402              common_cuda_opts +\n  403              [\n  404                  \"-fcuda-flush-denormals-to-zero\",\n  405:                 \"--cuda-path=external/local_config_cuda/cuda\",\n  406                  \"--cuda-gpu-arch=sm_35\",\n  407              ]\n  ...\n  410          # Pass -O3 when building CUDA code with clang; some important\n  411          # optimizations are not enabled at O2.\n  412:         \"@local_config_cuda//cuda:using_clang_opt\": [\"-O3\"],\n  413          \"//conditions:default\": [],\n  414      })\n  ...\n  686    cuda_deps = [\n  687        \"//tensorflow/core:stream_executor_headers_lib\",\n  688:       \"@local_config_cuda//cuda:cudart_static\",\n  689    ]\n  690    deps = deps + tf_custom_op_library_additional_deps()\n  ...\n  736                py_module_name=name)\n  737    extra_linkopts = select({\n  738:       \"@local_config_cuda//cuda:darwin\": [\n  739            \"-Wl,-exported_symbols_list\",\n  740            \"//tensorflow:tf_exported_symbols.lds\"\n  ...\n  745        ]})\n  746    extra_deps += select({\n  747:       \"@local_config_cuda//cuda:darwin\": [\n  748          \"//tensorflow:tf_exported_symbols.lds\"\n  749        ],\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\tools\\pip_package\\build_pip_package.sh:\n   21    local src_dir=$1\n   22    local dest_dir=$2\n   23:   for f in `find \"$src_dir\" -maxdepth 1 -mindepth 1 ! -name '*local_config_cuda*'`; do\n   24      cp -R \"$f\" \"$dest_dir\"\n   25    done\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\workspace.bzl:\n    6  # path_prefix and tf_repo_name are no longer used.\n    7  def tf_workspace(path_prefix = \"\", tf_repo_name = \"\"):\n    8:   cuda_configure(name = \"local_config_cuda\")\n    9    if path_prefix:\n   10      print(\"path_prefix was specified to tf_workspace but is no longer used and will be removed in the future.\")\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\crosstool\\CROSSTOOL.tpl:\n   48    tool_path { name: \"dwp\" path: \"/usr/bin/dwp\" }\n   49    # As part of the TensorFlow release, we place some cuda-related compilation\n   50:   # files in @local_config_cuda//crosstool/clang/bin, and this relative\n   51    # path, combined with the rest of our Bazel configuration causes our\n   52    # compilation to use those files.\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda\\BUILD.tpl:\n    1  licenses([\"restricted\"])  # MPL2, portions GPL v3, LGPL v3, BSD-like\n    2  \n    3: load(\"@local_config_cuda//cuda:platform.bzl\", \"cuda_library_path\")\n    4: load(\"@local_config_cuda//cuda:platform.bzl\", \"cuda_static_library_path\")\n    5: load(\"@local_config_cuda//cuda:platform.bzl\", \"cudnn_library_path\")\n    6: load(\"@local_config_cuda//cuda:platform.bzl\", \"cupti_library_path\")\n    7: load(\"@local_config_cuda//cuda:platform.bzl\", \"readlink_command\")\n    8  \n    9  package(default_visibility = [\"//visibility:public\"])\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda\\build_defs.bzl.tpl:\n    9      \"\"\"\n   10      return select({\n   11:         \"@local_config_cuda//cuda:using_nvcc\": if_true,\n   12          \"//conditions:default\": if_false\n   13      })\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda_configure.bzl:\n  516  \n  517  ```python\n  518: cuda_configure(name = \"local_config_cuda\")\n  519  ```\n  520  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\py\\numpy\\numpy_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tools\\bazel.rc:\n    1  # Autogenerated by configure: DO NOT EDIT\n    2: build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\n    3  build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n    4  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tools\\bazel.rc.template:\n    1: build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\n    2  build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n    3  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_lib:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tools\\bazel.rc:\n    1: build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\n    2  build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n    3  \n\n47 matches across 15 files\n\n\nSearching 8909 files for \"local_config_cuda\"\n\nC:\\dev\\vm\\data-share\\serving\\bazel-bin:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-genfiles:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-out:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-serving:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-testlogs:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\caffe.BUILD:\n    1  load(\"@org_tensorflow//tensorflow:tensorflow.bzl\", \"if_cuda\")\n    2  load(\"@caffe_tools//:config.bzl\", \"if_pycaffe\")\n    3: load(\"@local_config_cuda//cuda:platform.bzl\",\n    4       \"cuda_sdk_version\",\n    5       \"cudnn_library_path\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\platform\\default\\build_config\\BUILD:\n   10  load(\"//tensorflow:tensorflow.bzl\", \"tf_copts\")\n   11  load(\"//tensorflow:tensorflow.bzl\", \"tf_cuda_library\")\n   12: load(\"@local_config_cuda//cuda:platform.bzl\", \"cuda_library_path\")\n   13  \n   14  cc_library(\n   ..\n   33          \"//tensorflow/stream_executor\",\n   34      ] + select({\n   35:         \"@local_config_cuda//cuda:darwin\": [\"IOKit\"],\n   36          \"//conditions:default\": [],\n   37      }),\n   ..\n   92      name = \"cuda\",\n   93      data = [\n   94:         \"@local_config_cuda//cuda:{}\".format(cuda_library_path(\"cudart\")),\n   95      ],\n   96      linkopts = select({\n   97:         \"@local_config_cuda//cuda:darwin\": [\n   98:             \"-Wl,-rpath,../local_config_cuda/cuda/lib\",\n   99:             \"-Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib\",\n  100          ],\n  101          \"//conditions:default\": [\n  102:             \"-Wl,-rpath,../local_config_cuda/cuda/lib64\",\n  103:             \"-Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64\",\n  104          ],\n  105      }),\n  106      deps = [\n  107:         \"@local_config_cuda//cuda:cudart\",\n  108      ],\n  109  )\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\platform\\default\\gpu\\BUILD:\n   16      cuda_deps = [\n   17          \"//tensorflow/core:stream_executor\",\n   18:         \"@local_config_cuda//cuda:cuda_headers\",\n   19:         \"@local_config_cuda//cuda:cupti_headers\",\n   20      ],\n   21:     data = [\"@local_config_cuda//cuda:cupti_dsos\"],\n   22      visibility = [\"//visibility:public\"],\n   23  )\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\BUILD:\n   28      data = [\n   29          \"//tensorflow/core:cuda\",\n   30:         \"@local_config_cuda//cuda:cublas\",\n   31:         \"@local_config_cuda//cuda:cudnn\",\n   32:         \"@local_config_cuda//cuda:cufft\",\n   33:         \"@local_config_cuda//cuda:curand\",\n   34      ],\n   35      linkopts = [\n   ..\n   39      deps = [\n   40          \"//tensorflow/core:lib\",\n   41:         \"@local_config_cuda//cuda:cuda_headers\",\n   42      ],\n   43      alwayslink = 1,\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:\n  142    auto rpaths = new std::vector<string>;\n  143  #if defined(__APPLE__)\n  144:   rpaths->push_back(\"driver/driver_sh.runfiles/local_config_cuda/cuda/lib\");\n  145  #else\n  146:   rpaths->push_back(\"driver/driver_sh.runfiles/local_config_cuda/cuda/lib64\");\n  147  #endif\n  148    return rpaths;\n  ...\n  194  /* static */ string DsoLoader::GetCudaLibraryDirPath() {\n  195  #if defined(__APPLE__)\n  196:   return \"external/local_config_cuda/cuda/lib\";\n  197  #else\n  198:   return \"external/local_config_cuda/cuda/lib64\";\n  199  #endif\n  200  }\n  ...\n  202  /* static */ string DsoLoader::GetCudaDriverLibraryPath() {\n  203  #if defined(__APPLE__)\n  204:   return \"external/local_config_cuda/cuda/driver/lib\";\n  205  #else\n  206:   return \"external/local_config_cuda/cuda/driver/lib64\";\n  207  #endif\n  208  }\n  ...\n  210  /* static */ string DsoLoader::GetCudaCuptiLibraryPath() {\n  211  #if defined(__APPLE__)\n  212:   return \"external/local_config_cuda/cuda/extras/CUPTI/lib\";\n  213  #else\n  214:   return \"external/local_config_cuda/cuda/extras/CUPTI/lib64\";\n  215  #endif\n  216  }\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\tensorflow.bzl:\n   39  )\n   40  load(\n   41:     \"@local_config_cuda//cuda:build_defs.bzl\",\n   42      \"if_cuda\",\n   43  )\n   ..\n  392      return select({\n  393          \"//conditions:default\": [],\n  394:         \"@local_config_cuda//cuda:using_nvcc\": (\n  395              common_cuda_opts +\n  396              [\n  ...\n  399              ]\n  400          ),\n  401:         \"@local_config_cuda//cuda:using_clang\": (\n  402              common_cuda_opts +\n  403              [\n  404                  \"-fcuda-flush-denormals-to-zero\",\n  405:                 \"--cuda-path=external/local_config_cuda/cuda\",\n  406                  \"--cuda-gpu-arch=sm_35\",\n  407              ]\n  ...\n  410          # Pass -O3 when building CUDA code with clang; some important\n  411          # optimizations are not enabled at O2.\n  412:         \"@local_config_cuda//cuda:using_clang_opt\": [\"-O3\"],\n  413          \"//conditions:default\": [],\n  414      })\n  ...\n  686    cuda_deps = [\n  687        \"//tensorflow/core:stream_executor_headers_lib\",\n  688:       \"@local_config_cuda//cuda:cudart_static\",\n  689    ]\n  690    deps = deps + tf_custom_op_library_additional_deps()\n  ...\n  736                py_module_name=name)\n  737    extra_linkopts = select({\n  738:       \"@local_config_cuda//cuda:darwin\": [\n  739            \"-Wl,-exported_symbols_list\",\n  740            \"//tensorflow:tf_exported_symbols.lds\"\n  ...\n  745        ]})\n  746    extra_deps += select({\n  747:       \"@local_config_cuda//cuda:darwin\": [\n  748          \"//tensorflow:tf_exported_symbols.lds\"\n  749        ],\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\tools\\pip_package\\build_pip_package.sh:\n   21    local src_dir=$1\n   22    local dest_dir=$2\n   23:   for f in `find \"$src_dir\" -maxdepth 1 -mindepth 1 ! -name '*local_config_cuda*'`; do\n   24      cp -R \"$f\" \"$dest_dir\"\n   25    done\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\workspace.bzl:\n    6  # path_prefix and tf_repo_name are no longer used.\n    7  def tf_workspace(path_prefix = \"\", tf_repo_name = \"\"):\n    8:   cuda_configure(name = \"local_config_cuda\")\n    9    if path_prefix:\n   10      print(\"path_prefix was specified to tf_workspace but is no longer used and will be removed in the future.\")\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\crosstool\\CROSSTOOL.tpl:\n   48    tool_path { name: \"dwp\" path: \"/usr/bin/dwp\" }\n   49    # As part of the TensorFlow release, we place some cuda-related compilation\n   50:   # files in @local_config_cuda//crosstool/clang/bin, and this relative\n   51    # path, combined with the rest of our Bazel configuration causes our\n   52    # compilation to use those files.\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda\\BUILD.tpl:\n    1  licenses([\"restricted\"])  # MPL2, portions GPL v3, LGPL v3, BSD-like\n    2  \n    3: load(\"@local_config_cuda//cuda:platform.bzl\", \"cuda_library_path\")\n    4: load(\"@local_config_cuda//cuda:platform.bzl\", \"cuda_static_library_path\")\n    5: load(\"@local_config_cuda//cuda:platform.bzl\", \"cudnn_library_path\")\n    6: load(\"@local_config_cuda//cuda:platform.bzl\", \"cupti_library_path\")\n    7: load(\"@local_config_cuda//cuda:platform.bzl\", \"readlink_command\")\n    8  \n    9  package(default_visibility = [\"//visibility:public\"])\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda\\build_defs.bzl.tpl:\n    9      \"\"\"\n   10      return select({\n   11:         \"@local_config_cuda//cuda:using_nvcc\": if_true,\n   12          \"//conditions:default\": if_false\n   13      })\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda_configure.bzl:\n  516  \n  517  ```python\n  518: cuda_configure(name = \"local_config_cuda\")\n  519  ```\n  520  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\py\\numpy\\numpy_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tools\\bazel.rc:\n    1  # Autogenerated by configure: DO NOT EDIT\n    2: build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\n    3  build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n    4  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tools\\bazel.rc.template:\n    1: build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\n    2  build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n    3  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_lib:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tools\\bazel.rc:\n    1: build:cuda --crosstool_top=@local_config_cuda//crosstool:toolchain\n    2  build:cuda --define=using_cuda=true --define=using_cuda_nvcc=true\n    3  \n\n47 matches across 15 files\n\n\nSearching 8909 files for \"cuda_configure.bzl\"\n\nC:\\dev\\vm\\data-share\\serving\\bazel-bin:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-genfiles:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-out:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-serving:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-testlogs:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\workspace.bzl:\n    1  # TensorFlow external dependencies that can be loaded in WORKSPACE files.\n    2  \n    3: load(\"//third_party/gpus:cuda_configure.bzl\", \"cuda_configure\")\n    4  \n    5  # If TensorFlow is linked as a submodule.\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\py\\numpy\\numpy_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_lib:\n    ERROR: Unable to open file\n\n1 match in 1 file\n\n\nSearching 8909 files for \"cudnn\"\n\nC:\\dev\\vm\\data-share\\serving\\bazel-bin:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-genfiles:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-out:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-serving:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-testlogs:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\caffe.BUILD:\n    3  load(\"@local_config_cuda//cuda:platform.bzl\",\n    4       \"cuda_sdk_version\",\n    5:      \"cudnn_library_path\",\n    6      )\n    7  \n    .\n   34      message = \"Building Caffe (this may take a while)\",\n   35      srcs = if_cuda([\n   36:         \"@org_tensorflow//third_party/gpus/cuda:include/cudnn.h\",\n   37:         \"@org_tensorflow//third_party/gpus/cuda:\" + cudnn_library_path()\n   38      ]) + [\n   39          \":protobuf-root\",\n   ..\n   68          # sensible.\n   69          if_cuda('''\n   70:             cudnn_includes=$(location @org_tensorflow//third_party/gpus/cuda:include/cudnn.h);\n   71:             cudnn_lib=$(location @org_tensorflow//third_party/gpus/cuda:%s);\n   72              extra_cmake_opts=\"-DCPU_ONLY:bool=OFF\n   73:                               -DUSE_CUDNN:bool=ON\n   74:                               -DCUDNN_INCLUDE:path=$$srcdir/$$(dirname $$cudnn_includes)\n   75:                               -DCUDNN_LIBRARY:path=$$srcdir/$$cudnn_lib\"; ''' % cudnn_library_path(),\n   76  \n   77              'extra_cmake_opts=\"-DCPU_ONLY:bool=ON\";') +\n   ..\n  199      hdrs = glob([\"include/**\"]) + [\"include/caffe/proto/caffe.pb.h\"],\n  200      defines = if_cuda(\n  201:         [\"USE_CUDNN\"],\n  202          [\"CPU_ONLY\"]\n  203      ) + if_pycaffe(\n  ...\n  205      ),\n  206      deps = if_cuda([\n  207:         \"@org_tensorflow//third_party/gpus/cuda:cudnn\",\n  208          \"@org_tensorflow//third_party/gpus/cuda:cublas\",\n  209          \":curand\"\n\nC:\\dev\\vm\\data-share\\serving\\README.md:\n   55  ### GPU Support (optional, linux only)\n   56  \n   57: The Caffe build adopts the CUDA configuration from Tensorflow, and as such will use the version (and location) of cudnn, and the standard cuda libraries you specified when you configured Tensorflow. You can validate this configuration by building Caffe with CUDA:\n   58  \n   59      > bazel build -c opt --config=cuda @caffe//:lib\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\configure:\n  193  done\n  194  \n  195: # Find out where the cuDNN library is installed\n  196  while true; do\n  197:   # Configure the Cudnn version to use.\n  198:   if [ -z \"$TF_CUDNN_VERSION\" ]; then\n  199:     read -p \"Please specify the Cudnn version you want to use. [Leave empty to use system default]: \" TF_CUDNN_VERSION\n  200    fi\n  201  \n  202    fromuser=\"\"\n  203:   if [ -z \"$CUDNN_INSTALL_PATH\" ]; then\n  204:     default_cudnn_path=${CUDA_TOOLKIT_PATH}\n  205:     read -p \"Please specify the location where cuDNN $TF_CUDNN_VERSION library is installed. Refer to README.md for more details. [Default is $default_cudnn_path]: \" CUDNN_INSTALL_PATH\n  206      fromuser=\"1\"\n  207:     if [ -z \"$CUDNN_INSTALL_PATH\" ]; then\n  208:       CUDNN_INSTALL_PATH=$default_cudnn_path\n  209      fi\n  210      # Result returned from \"read\" will be used unexpanded. That make \"~\" unuseable.\n  211      # Going through one more level of expansion to handle that.\n  212:     CUDNN_INSTALL_PATH=`${PYTHON_BIN_PATH} -c \"import os; print(os.path.realpath(os.path.expanduser('${CUDNN_INSTALL_PATH}')))\"`\n  213    fi\n  214  \n  215:   if [[ -z \"$TF_CUDNN_VERSION\" ]]; then\n  216:     TF_CUDNN_EXT=\"\"\n  217      # Resolve to the SONAME of the symlink.  Use readlink without -f\n  218:     # to resolve exactly once to the SONAME.  E.g, libcudnn.so ->\n  219:     # libcudnn.so.4\n  220:     REALVAL=`readlink ${CUDNN_INSTALL_PATH}/lib64/libcudnn.so`\n  221  \n  222      # Extract the version of the SONAME, if it was indeed symlinked to\n  ...\n  224      if [[ \"$REALVAL\" =~ .so[.]+([0-9]*) ]];\n  225      then\n  226:       TF_CUDNN_EXT=\".\"${BASH_REMATCH[1]}\n  227:       TF_CUDNN_VERSION=${BASH_REMATCH[1]}\n  228:       echo \"libcudnn.so resolves to libcudnn${TF_CUDNN_EXT}\"\n  229      fi\n  230    else\n  231:     TF_CUDNN_EXT=\".$TF_CUDNN_VERSION\"\n  232    fi\n  233  \n  234    if [ \"$OSNAME\" == \"Linux\" ]; then\n  235:     CUDA_DNN_LIB_PATH=\"lib64/libcudnn.so${TF_CUDNN_EXT}\"\n  236:     CUDA_DNN_LIB_ALT_PATH=\"libcudnn.so${TF_CUDNN_EXT}\"\n  237    elif [ \"$OSNAME\" == \"Darwin\" ]; then\n  238:     CUDA_DNN_LIB_PATH=\"lib/libcudnn${TF_CUDNN_EXT}.dylib\"\n  239:     CUDA_DNN_LIB_ALT_PATH=\"libcudnn${TF_CUDNN_EXT}.dylib\"\n  240    fi\n  241  \n  242:   if [ -e \"$CUDNN_INSTALL_PATH/${CUDA_DNN_LIB_ALT_PATH}\" -o -e \"$CUDNN_INSTALL_PATH/${CUDA_DNN_LIB_PATH}\" ]; then\n  243:     export TF_CUDNN_VERSION\n  244:     export CUDNN_INSTALL_PATH\n  245      break\n  246    fi\n  247  \n  248    if [ \"$OSNAME\" == \"Linux\" ]; then\n  249:     CUDNN_PATH_FROM_LDCONFIG=\"$(ldconfig -p | sed -n 's/.*libcudnn.so .* => \\(.*\\)/\\1/p')\"\n  250:     if [ -e \"${CUDNN_PATH_FROM_LDCONFIG}${TF_CUDNN_EXT}\" ]; then\n  251:       export TF_CUDNN_VERSION\n  252:       export CUDNN_INSTALL_PATH=\"$(dirname ${CUDNN_PATH_FROM_LDCONFIG})\"\n  253        break\n  254      fi\n  255    fi\n  256:   echo \"Invalid path to cuDNN ${CUDNN_VERSION} toolkit. Neither of the following two files can be found:\"\n  257:   echo \"${CUDNN_INSTALL_PATH}/${CUDA_DNN_LIB_PATH}\"\n  258:   echo \"${CUDNN_INSTALL_PATH}/${CUDA_DNN_LIB_ALT_PATH}\"\n  259    if [ \"$OSNAME\" == \"Linux\" ]; then\n  260:     echo \"${CUDNN_PATH_FROM_LDCONFIG}${TF_CUDNN_EXT}\"\n  261    fi\n  262  \n  ...\n  265    fi\n  266    # Retry\n  267:   TF_CUDNN_VERSION=\"\"\n  268:   CUDNN_INSTALL_PATH=\"\"\n  269  done\n  270  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\ISSUE_TEMPLATE.md:\n   14  Operating System:\n   15  \n   16: Installed version of CUDA and cuDNN: \n   17  (please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n   18  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\RELEASE.md:\n   77  ## Big Fixes and Other Changes\n   78  \n   79: * Turned on CuDNN Autotune.\n   80  * Added support for using third-party Python optimization algorithms (contrib.opt).\n   81  * Google Cloud Storage filesystem support.\n   ..\n  126  * Utility for inspecting checkpoints\n  127  * Basic tracing and timeline support\n  128: * Allow building against cuDNN 5 (not incl. RNN/LSTM support) \n  129  * Added instructions and binaries for ProtoBuf library with fast serialization and without 64MB limit\n  130  * Added special functions\n  ...\n  156  * Added gfile.Open and gfile.Copy, used by input_data.py.\n  157  * Fixed Saver bug when MakeDirs tried to create empty directory.\n  158: * GPU Pip wheels are built with cuda 7.5 and cudnn-v4, making them\n  159:   required for the binary releases. Lower versions of cuda/cudnn can\n  160    be supported by installing from sources and setting the options\n  161    during ./configure\n  ...\n  171  ## Major Features and Improvements\n  172  \n  173: * Allow using any installed Cuda >= 7.0 and cuDNN >= R2, and add support\n  174:   for cuDNN R4\n  175  * Added a `contrib/` directory for unsupported or experimental features, \n  176    including higher level `layers` module\n  ...\n  281  * Some improvements to GPU performance and memory usage:\n  282    [convnet benchmarks](https://github.com/soumith/convnet-benchmarks/issues/66)\n  283:   roughly equivalent with native cudnn v2 performance.  Improvements mostly due\n  284    to moving to 32-bit indices, faster shuffling kernels.  More improvements to\n  285    come in later releases.\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\BUILD:\n  100          \"//tensorflow/contrib/copy_graph:all_files\",\n  101          \"//tensorflow/contrib/crf:all_files\",\n  102:         \"//tensorflow/contrib/cudnn_rnn:all_files\",\n  103          \"//tensorflow/contrib/distributions:all_files\",\n  104          \"//tensorflow/contrib/factorization:all_files\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\__init__.py:\n   23  from tensorflow.contrib import copy_graph\n   24  from tensorflow.contrib import crf\n   25: from tensorflow.contrib import cudnn_rnn\n   26  from tensorflow.contrib import distributions\n   27  from tensorflow.contrib import factorization\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\BUILD:\n   17          \"//tensorflow/contrib/copy_graph:copy_graph_py\",\n   18          \"//tensorflow/contrib/crf:crf_py\",\n   19:         \"//tensorflow/contrib/cudnn_rnn:cudnn_rnn_py\",\n   20          \"//tensorflow/contrib/distributions:distributions_py\",\n   21          \"//tensorflow/contrib/factorization:factorization_py\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\cmake\\tf_stream_executor.cmake:\n   23  #        \"//tensorflow/core:cuda\",\n   24  #        \"//third_party/gpus/cuda:cublas\",\n   25: #        \"//third_party/gpus/cuda:cudnn\",\n   26  #    ],\n   27  #    linkopts = [\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\cudnn_rnn\\__init__.py:\n   13  # limitations under the License.\n   14  # ==============================================================================\n   15: \"\"\"Ops for fused Cudnn RNN models.\"\"\"\n   16  \n   17  from __future__ import absolute_import\n   ..\n   19  from __future__ import print_function\n   20  \n   21: from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnGRU\n   22: from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnLSTM\n   23: from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnRNNRelu\n   24: from tensorflow.contrib.cudnn_rnn.python.ops.cudnn_rnn_ops import CudnnRNNTanh\n   25  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\cudnn_rnn\\BUILD:\n    1  # Description:\n    2: #   A Cudnn RNN wrapper.\n    3  #   APIs are meant to change over time.\n    4  package(\n    .\n   16  \n   17  tf_custom_op_library(\n   18:     name = \"python/ops/_cudnn_rnn_ops.so\",\n   19      srcs = [\n   20:         \"kernels/cudnn_rnn_ops.cc\",\n   21:         \"ops/cudnn_rnn_ops.cc\",\n   22      ],\n   23      deps = [\n   ..\n   27  \n   28  tf_gen_op_libs(\n   29:     op_lib_names = [\"cudnn_rnn_ops\"],\n   30      deps = [\n   31          \"//tensorflow/core:lib\",\n   ..\n   34  \n   35  tf_gen_op_wrapper_py(\n   36:     name = \"cudnn_rnn_ops\",\n   37:     deps = [\":cudnn_rnn_ops_op_lib\"],\n   38  )\n   39  \n   40  py_library(\n   41:     name = \"cudnn_rnn_py\",\n   42      srcs = [\n   43          \"__init__.py\",\n   44:         \"python/ops/cudnn_rnn_ops.py\",\n   45      ],\n   46      data = [\n   47:         \":python/ops/_cudnn_rnn_ops.so\",\n   48      ],\n   49      srcs_version = \"PY2AND3\",\n   50      visibility = [\"//visibility:public\"],\n   51      deps = [\n   52:         \":cudnn_rnn_ops\",\n   53      ],\n   54  )\n   55  \n   56  cuda_py_test(\n   57:     name = \"cudnn_rnn_ops_test\",\n   58      size = \"small\",\n   59:     srcs = [\"python/kernel_tests/cudnn_rnn_ops_test.py\"],\n   60      additional_deps = [\n   61:         \":cudnn_rnn_py\",\n   62          \"//tensorflow:tensorflow_py\",\n   63          \"//tensorflow/python:framework_test_lib\",\n   ..\n   66      tags = [\n   67          \"manual\",\n   68:         \"requires_cudnn5\",\n   69      ],\n   70  )\n   71  \n   72  cuda_py_test(\n   73:     name = \"cudnn_rnn_ops_benchmark\",\n   74      size = \"large\",\n   75:     srcs = [\"python/kernel_tests/cudnn_rnn_ops_benchmark.py\"],\n   76      additional_deps = [\n   77:         \":cudnn_rnn_py\",\n   78          \"//tensorflow:tensorflow_py\",\n   79          \"//tensorflow/contrib/rnn:rnn_py\",\n   ..\n   83      tags = [\n   84          \"manual\",\n   85:         \"requires_cudnn5\",\n   86      ],\n   87  )\n   88  \n   89  cc_test(\n   90:     name = \"cudnn_rnn_ops_test_cc\",\n   91      size = \"small\",\n   92      srcs = [\n   93:         \"ops/cudnn_rnn_ops_test.cc\",\n   94      ],\n   95      deps = [\n   96:         \":cudnn_rnn_ops_op_lib\",\n   97          \"//tensorflow/core\",\n   98          \"//tensorflow/core:framework\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\cudnn_rnn\\kernels\\cudnn_rnn_ops.cc:\n   48  /*\n   49   * This module implements ops that fuse a multi-layer multi-step RNN/LSTM model\n   50:  * using the underlying Cudnn library.\n   51   *\n   52:  * Cudnn RNN library exposes an opaque parameter buffer with unknown layout and\n   53   * format. And it is very likely that if saved, they cannot be used across\n   54   * different GPUs. So users need to first query the size of the opaque\n   ..\n   60   * be produced for the backward pass. So there is a performance penalty.\n   61   *\n   62:  * In addition to the actual data and reserve_space, Cudnn also needs more\n   63   * memory as temporary workspace. The memory management to and from\n   64   * stream-executor is done through ScratchAllocator. In general,\n   ..\n   77  \n   78  template <typename Device, typename T, typename Index>\n   79: class CudnnRNNParamsSizeOp;\n   80  \n   81  template <typename Device, typename T>\n   82: class CudnnRNNForwardOp;\n   83  \n   84  template <typename Device, typename T>\n   85: class CudnnRNNBackwardOp;\n   86  \n   87  enum class TFRNNInputMode {\n   ..\n  205  }\n  206  \n  207: // A helper to allocate temporary scratch memory for Cudnn RNN models. It takes\n  208  // the ownership of the underlying memory. The expectation is that the memory\n  209: // should be alive for the span of the Cudnn RNN itself.\n  210: class CudnnRNNWorkspaceAllocator : public ScratchAllocator {\n  211   public:\n  212:   virtual ~CudnnRNNWorkspaceAllocator() {}\n  213:   CudnnRNNWorkspaceAllocator(OpKernelContext* context) : context_(context) {}\n  214    int64 GetMemoryLimitInBytes(perftools::gputools::Stream* stream) override {\n  215      return std::numeric_limits<int64>::max();\n  ...\n  239  };\n  240  \n  241: // A helper to allocate reserve-space memory for Cudnn RNN models. The tensors\n  242  // are allocated as a kernel output, and will be fed into the backward pass.\n  243  // The memory is expected to live long enough after the backward pass is\n  244  // finished.\n  245  template <typename T>\n  246: class CudnnRNNReserveSpaceAllocator : public ScratchAllocator {\n  247   public:\n  248:   virtual ~CudnnRNNReserveSpaceAllocator() {}\n  249:   CudnnRNNReserveSpaceAllocator(OpKernelContext* context, int output_index)\n  250        : context_(context), output_index_(output_index) {}\n  251    int64 GetMemoryLimitInBytes(perftools::gputools::Stream* stream) override {\n  ...\n  279  };\n  280  \n  281: struct CudnnModelTypes {\n  282    RnnMode rnn_mode;\n  283    TFRNNInputMode rnn_input_mode;\n  284    RnnDirectionMode rnn_direction_mode;\n  285    bool HasInputC() const {\n  286:     // For Cudnn 5.0, only LSTM has input-c. All other models use only input-h.\n  287      return rnn_mode == RnnMode::kRnnLstm;\n  288    }\n  ...\n  290  \n  291  // A helper class that collects the shapes to describe a RNN model.\n  292: struct CudnnModelShapes {\n  293    int num_layers;\n  294    int input_size;\n  ...\n  305  // OpKernelContext.\n  306  Status ExtractForwardInput(OpKernelContext* context,\n  307:                            const CudnnModelTypes& model_types,\n  308                             const Tensor** input, const Tensor** input_h,\n  309                             const Tensor** input_c, const Tensor** params,\n  310:                            CudnnModelShapes* model_shapes) {\n  311    TF_RETURN_IF_ERROR(context->input(\"input\", input));\n  312    TF_RETURN_IF_ERROR(context->input(\"input_h\", input_h));\n  ...\n  362  // A common base class for RNN kernels. It extracts common attributes and\n  363  // shape validations.\n  364: class CudnnRNNKernelCommon : public OpKernel {\n  365   protected:\n  366:   CudnnRNNKernelCommon(OpKernelConstruction* context) : OpKernel(context) {\n  367      string str;\n  368      OP_REQUIRES_OK(context, context->GetAttr(\"rnn_mode\", &str));\n  ...\n  382      return model_types_.rnn_direction_mode;\n  383    }\n  384:   CudnnModelTypes model_types() const { return model_types_; }\n  385  \n  386    template <typename T>\n  387:   Status ExtractCudnnRNNParamsInfo(OpKernelContext* context,\n  388                                     std::unique_ptr<RnnDescriptor>* rnn_desc) {\n  389      const Tensor* num_layers_t = nullptr;\n  ...\n  422  \n  423   private:\n  424:   CudnnModelTypes model_types_;\n  425  };\n  426  \n  ...\n  429  // should not be used for saving and restoring.\n  430  template <typename T, typename Index>\n  431: class CudnnRNNParamsSizeOp<GPUDevice, T, Index> : public CudnnRNNKernelCommon {\n  432   public:\n  433    typedef GPUDevice Device;\n  434:   explicit CudnnRNNParamsSizeOp(OpKernelConstruction* context)\n  435:       : CudnnRNNKernelCommon(context) {}\n  436  \n  437    void Compute(OpKernelContext* context) override {\n  438      std::unique_ptr<RnnDescriptor> rnn_desc;\n  439:     OP_REQUIRES_OK(context, ExtractCudnnRNNParamsInfo<T>(context, &rnn_desc));\n  440      int64 params_size_in_bytes = rnn_desc->ParamsSizeInBytes();\n  441      CHECK(params_size_in_bytes % sizeof(T) == 0)\n  ...\n  449  };\n  450  \n  451: REGISTER_KERNEL_BUILDER(Name(\"CudnnRNNParamsSize\")\n  452                              .Device(DEVICE_GPU)\n  453                              .HostMemory(\"num_layers\")\n  ...\n  457                              .TypeConstraint<float>(\"T\")\n  458                              .TypeConstraint<int32>(\"S\"),\n  459:                         CudnnRNNParamsSizeOp<GPUDevice, float, int32>);\n  460  \n  461  // Run the forward operation of the RNN model.\n  462  template <typename T>\n  463: class CudnnRNNForwardOp<GPUDevice, T> : public CudnnRNNKernelCommon {\n  464   public:\n  465    typedef GPUDevice Device;\n  466:   explicit CudnnRNNForwardOp(OpKernelConstruction* context)\n  467:       : CudnnRNNKernelCommon(context) {\n  468      OP_REQUIRES_OK(context, context->GetAttr(\"is_training\", &is_training_));\n  469    }\n  ...\n  474      const Tensor* input_c = nullptr;\n  475      const Tensor* params = nullptr;\n  476:     CudnnModelShapes model_shapes;\n  477      OP_REQUIRES_OK(context,\n  478                     ExtractForwardInput(context, model_types(), &input, &input_h,\n  ...\n  549      // output of this kernel. And it will be fed into the backward pass when\n  550      // needed.\n  551:     CudnnRNNReserveSpaceAllocator<T> reserve_space_allocator(context, 3);\n  552      if (!is_training_) {\n  553        Tensor* dummy_reserve_space = nullptr;\n  ...\n  557      // Creates a memory callback for the workspace. The memory lives to the end\n  558      // of this kernel calls.\n  559:     CudnnRNNWorkspaceAllocator workspace_allocator(context);\n  560      bool launch_status =\n  561          stream\n  ...\n  576  \n  577  REGISTER_KERNEL_BUILDER(\n  578:     Name(\"CudnnRNN\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n  579:     CudnnRNNForwardOp<GPUDevice, float>);\n  580  \n  581  // Run the backward operation of the RNN model.\n  582  template <typename T>\n  583: class CudnnRNNBackwardOp<GPUDevice, T> : public CudnnRNNKernelCommon {\n  584   public:\n  585    typedef GPUDevice Device;\n  586  \n  587:   explicit CudnnRNNBackwardOp(OpKernelConstruction* context)\n  588:       : CudnnRNNKernelCommon(context) {}\n  589  \n  590    void Compute(OpKernelContext* context) override {\n  ...\n  593      const Tensor* input_c = nullptr;\n  594      const Tensor* params = nullptr;\n  595:     CudnnModelShapes model_shapes;\n  596      OP_REQUIRES_OK(context,\n  597                     ExtractForwardInput(context, model_types(), &input, &input_h,\n  ...\n  658      OP_REQUIRES_OK(context,\n  659                     context->input(\"reserve_space\", &reserve_space_const));\n  660:     // Cudnn needs the reserve space to be writeable. This is fine because they\n  661      // are opaque.\n  662      Tensor* reserve_space = const_cast<Tensor*>(reserve_space_const);\n  ...\n  743      // Creates a memory callback for the workspace. The memory lives to the end\n  744      // of this kernel calls.\n  745:     CudnnRNNWorkspaceAllocator workspace_allocator(context);\n  746      bool launch_status =\n  747          stream\n  ...\n  762  \n  763  REGISTER_KERNEL_BUILDER(\n  764:     Name(\"CudnnRNNBackprop\").Device(DEVICE_GPU).TypeConstraint<float>(\"T\"),\n  765:     CudnnRNNBackwardOp<GPUDevice, float>);\n  766  \n  767: // TODO(zhengxq): Add the conversion of Cudnn RNN Params from and to\n  768  // its canonical form.\n  769  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\cudnn_rnn\\ops\\cudnn_rnn_ops.cc:\n   22  namespace {\n   23  \n   24: constexpr auto kCudnnRNNCommonAttrs = R\"doc(\n   25  rnn_mode: Indicates the type of the RNN model.\n   26  input_mode: Indicate whether there is a linear projection between the input and\n   ..\n   32  )doc\";\n   33  \n   34: constexpr auto kCudnnRNNParamsBuffer = R\"doc(\n   35  Note that the params buffer may not be compatible across different GPUs. So any\n   36  save and restoration should be converted to and from the canonical weights and\n   ..\n   48      \"direction: {'unidirectional', 'bidirectional'} = 'unidirectional'\";\n   49  \n   50: constexpr auto kCudnnRNNCanonicalParams = R\"doc(\n   51  canonical_weights: the canonical form of weights that can be used for saving\n   52      and restoration. They are more likely to be compatible across different\n   ..\n   63  using shape_inference::ShapeHandle;\n   64  \n   65: REGISTER_OP(\"CudnnRNNParamsSize\")\n   66      .Input(\"num_layers: int32\")\n   67      .Input(\"num_units: int32\")\n   ..\n   75      .SetShapeFn(shape_inference::ScalarShape)\n   76      .Doc(strings::StrCat(R\"doc(\n   77: Return the params size that can be used by the Cudnn RNN model. Subsequent\n   78  weight allocation and initialization should use this size.\n   79  )doc\",\n   80:                          kCudnnRNNCommonAttrs,\n   81                           R\"doc(\n   82  num_layers: Specifies the number of layers in the RNN model.\n   ..\n   85  params_size: The size of the params buffer that should be allocated and\n   86      initialized for this RNN model. Note that this params buffer may not be\n   87:     compatible across GPUs. Please use CudnnRNNParamsWeights and\n   88:     CudnnRNNParamsBiases to save and restore them in a way that is compatible\n   89      across different runs.\n   90  )doc\",\n   91:                          kCudnnRNNParamsBuffer));\n   92  \n   93: static string CudnnRNNForwardTensors() {\n   94    return R\"doc(\n   95  input: a 3-D tensor with the shape of [seq_length, batch_size, input_size].\n   ..\n   99      [num_layer * dir, batch, num_units]. For other models, it is ignored.\n  100  params: a 1-D tensor that contains the weights and biases in an opaque layout.\n  101:     The size must be created through CudnnRNNParamsSize, and initialized\n  102      separately. Note that they might not be compatible across different\n  103      generations. So it is a good idea to save and restore\n  ...\n  109  }\n  110  \n  111: REGISTER_OP(\"CudnnRNN\")\n  112      .Input(\"input: T\")\n  113      .Input(\"input_h: T\")\n  ...\n  153  buffer.\n  154  )doc\",\n  155:                          kCudnnRNNCommonAttrs, CudnnRNNForwardTensors(), R\"doc(\n  156  is_training: Indicates whether this operation is used for inferenece or\n  157      training.\n  ...\n  160  )doc\"));\n  161  \n  162: REGISTER_OP(\"CudnnRNNBackprop\")\n  163      .Input(\"input: T\")\n  164      .Input(\"input_h: T\")\n  ...\n  194  Compute the backprop of both data and weights in a RNN.\n  195  )doc\",\n  196:                          kCudnnRNNCommonAttrs, CudnnRNNForwardTensors(), R\"doc(\n  197  output_backprop: A 3-D tensor with the same shape as output in the forward pass.\n  198  output_h_backprop: A 3-D tensor with the same shape as output_h in the forward\n  ...\n  213  // NOTE(zhengxq): this is not currently implemented yet. And may subject to\n  214  // change.\n  215: REGISTER_OP(\"CudnnRNNParamsToCanonical\")\n  216      .Input(\"num_layers: int32\")\n  217      .Input(\"num_units: int32\")\n  ...\n  229  restored in a way compatible with future runs.\n  230  )doc\",\n  231:                          kCudnnRNNCommonAttrs, kCudnnRNNParamsBuffer,\n  232:                          kCudnnRNNCanonicalParams));\n  233  \n  234  // NOTE(zhengxq): this is not currently implemented yet. And may subject to\n  235  // change.\n  236: REGISTER_OP(\"CudnnRNNParamsFromCanonical\")\n  237      .Input(\"num_layers: int32\")\n  238      .Input(\"num_units: int32\")\n  ...\n  250  upcoming training or inferences.\n  251  )doc\",\n  252:                          kCudnnRNNCommonAttrs, kCudnnRNNParamsBuffer,\n  253:                          kCudnnRNNCanonicalParams));\n  254  \n  255  }  // namespace tensorflow\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\cudnn_rnn\\ops\\cudnn_rnn_ops_test.cc:\n   25  namespace tensorflow {\n   26  \n   27: TEST(CudnnRNNOpsTest, ParamsSize_ShapeFn) {\n   28:   ShapeInferenceTestOp op(\"CudnnRNNParamsSize\");\n   29    INFER_OK(op, \"[1];[1];[1]\", \"[]\");\n   30  }\n   31  \n   32: TEST(CudnnRNNOpsTest, ForwardLstm_ShapeFn) {\n   33:   ShapeInferenceTestOp op(\"CudnnRNN\");\n   34:   TF_ASSERT_OK(NodeDefBuilder(\"test\", \"CudnnRNN\")\n   35                     .Input({\"input\", 0, DT_FLOAT})\n   36                     .Input({\"input_h\", 0, DT_FLOAT})\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\cudnn_rnn\\python\\kernel_tests\\cudnn_rnn_ops_benchmark.py:\n   13  # limitations under the License.\n   14  # ==============================================================================\n   15: \"\"\"Benchmarks for Cudnn RNN models.\"\"\"\n   16  \n   17  from __future__ import absolute_import\n   ..\n   26  \n   27  \n   28: class CudnnRNNBenchmark(tf.test.Benchmark):\n   29:   \"\"\"Benchmarks Cudnn LSTM and other related models.\n   30    \"\"\"\n   31  \n   ..\n   75            name=desc, iters=benchmark_steps, wall_time=total_time)\n   76  \n   77:   def benchmarkCudnnLSTMTraining(self):\n   78      test_configs = self._GetTestConfig()\n   79      for config_name, config in test_configs.items():\n   ..\n   85  \n   86        with tf.Graph().as_default(), tf.device(\"/gpu:0\"):\n   87:         model = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers, num_units, num_units)\n   88          params_size_t = model.params_size()\n   89          input_data = tf.Variable(tf.ones([seq_length, batch_size, num_units]))\n   ..\n  100                                   [params, input_data, input_h, input_c])\n  101          training_op = tf.group(*all_grads)\n  102:         self._BenchmarkOp(training_op, \"cudnn_lstm %s %s\" %\n  103                            (config_name, self._GetConfigDesc(config)))\n  104  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\cudnn_rnn\\python\\kernel_tests\\cudnn_rnn_ops_test.py:\n   13  # limitations under the License.\n   14  # ==============================================================================\n   15: \"\"\"Tests for Cudnn RNN models.\"\"\"\n   16  \n   17  from __future__ import absolute_import\n   ..\n   24  \n   25  \n   26: class CudnnRNNTest(TensorFlowTestCase):\n   27  \n   28    def _CreateModel(self, rnn_mode, num_layers, num_units, input_size):\n   29      if rnn_mode == \"lstm\":\n   30:       model = tf.contrib.cudnn_rnn.CudnnLSTM(num_layers, num_units, input_size)\n   31      elif rnn_mode == \"gru\":\n   32:       model = tf.contrib.cudnn_rnn.CudnnGRU(num_layers, num_units, input_size)\n   33      elif rnn_mode == \"rnn_tanh\":\n   34:       model = tf.contrib.cudnn_rnn.CudnnRNNTanh(num_layers, num_units,\n   35                                                  input_size)\n   36      elif rnn_mode == \"rnn_relu\":\n   37:       model = tf.contrib.cudnn_rnn.CudnnRNNRelu(num_layers, num_units,\n   38                                                  input_size)\n   39      else:\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\cudnn_rnn\\python\\ops\\cudnn_rnn_ops.py:\n   13  # limitations under the License.\n   14  # ==============================================================================\n   15: \"\"\"Cudnn RNN operators.\"\"\"\n   16  from __future__ import absolute_import\n   17  from __future__ import division\n   18  from __future__ import print_function\n   19  \n   20: from tensorflow.contrib.cudnn_rnn.ops import gen_cudnn_rnn_ops\n   21  from tensorflow.python.framework import common_shapes\n   22  from tensorflow.python.framework import dtypes\n   ..\n   26  from tensorflow.python.platform import resource_loader\n   27  \n   28: _cudnn_rnn_ops_so = load_library.load_op_library(\n   29:     resource_loader.get_path_to_datafile(\"_cudnn_rnn_ops.so\"))\n   30: assert _cudnn_rnn_ops_so, \"Could not load _cudnn_rnn_ops.so.\"\n   31  \n   32: _cudnn_rnn_common_doc_string = \"\"\"\n   33:   Cudnn RNN has an opaque parameter buffer that can be used for inference and\n   34    training. But it is possible that the layout of the parameter buffers\n   35    changes between generations. So it is highly recommended to use the canonical\n   ..\n   37  \n   38    This is a typical use case:\n   39:     * The user creates a CudnnRNN model.\n   40      * The user query that parameter buffer size.\n   41      * The user creates a variable of that size that serves as the parameter\n   ..\n   50  \n   51  \n   52: class _CudnnRNN(object):\n   53:   \"\"\"Create an RNN model using the underlying Cudnn implementation.\n   54    \"\"\"\n   55:   __doc__ += _cudnn_rnn_common_doc_string\n   56  \n   57    def __init__(self,\n   ..\n   65                 seed=0,\n   66                 seed2=0):\n   67:     \"\"\"Create a CudnnRNN model from model spec.\n   68  \n   69      Args:\n   ..\n  102        The calculated parameter buffer size.\n  103      \"\"\"\n  104:     return gen_cudnn_rnn_ops.cudnn_rnn_params_size(\n  105          num_layers=self._num_layers,\n  106          num_units=self._num_units,\n  ...\n  130        # For model that doesn't take input_c, replace with a dummy tensor.\n  131        input_c = array_ops.constant([], dtype=dtypes.float32)\n  132:     output, output_h, output_c, _ = gen_cudnn_rnn_ops.cudnn_rnn(\n  133          input=input_data,\n  134          input_h=input_h,\n  ...\n  147  \n  148  \n  149: class CudnnLSTM(_CudnnRNN):\n  150:   \"\"\"Cudnn implementation of the LSTM model.\n  151    \"\"\"\n  152:   __doc__ += _cudnn_rnn_common_doc_string\n  153  \n  154    def __init__(self,\n  ...\n  161                 seed=0,\n  162                 seed2=0):\n  163:     \"\"\"Create a Cudnn LSTM model from model spec.\n  164  \n  165      Args:\n  ...\n  180        seed2: the second part of a seed that is used to initialize dropout.\n  181      \"\"\"\n  182:     super(CudnnLSTM, self).__init__(\n  183          \"lstm\",\n  184          num_layers,\n  ...\n  192  \n  193    def __call__(self, input_data, input_h, input_c, params, is_training=True):\n  194:     \"\"\"Run the forward step for the Cudnn LSTM model.\n  195  \n  196      Args:\n  ...\n  206        output_c: the final state for c.\n  207      \"\"\"\n  208:     output, output_h, output_c = super(CudnnLSTM, self).__call__(input_data,\n  209                                                                   input_h,\n  210                                                                   input_c,\n  ...\n  214  \n  215  \n  216: class _CudnnRNNNoInputC(_CudnnRNN):\n  217:   \"\"\"Simple CudnnRNN models without input_c.\n  218    \"\"\"\n  219:   __doc__ += _cudnn_rnn_common_doc_string\n  220  \n  221    def __init__(self,\n  ...\n  228                 seed=0,\n  229                 seed2=0):\n  230:     \"\"\"Create a Cudnn RNN model from model without hidden-state C.\n  231  \n  232      Args:\n  ...\n  247        seed2: the second part of a seed that is used to initialize dropout.\n  248      \"\"\"\n  249:     super(_CudnnRNNNoInputC, self).__init__(\n  250          self._rnn_mode,\n  251          num_layers,\n  ...\n  259  \n  260    def __call__(self, input_data, input_h, params, is_training=True):\n  261:     \"\"\"Run the forward step for the Cudnn LSTM model.\n  262  \n  263      Args:\n  ...\n  271        output_h: the final state for h.\n  272      \"\"\"\n  273:     output, output_h, _ = super(_CudnnRNNNoInputC, self).__call__(\n  274          input_data, input_h, None, params, is_training=True)\n  275      return (output, output_h)\n  276  \n  277  \n  278: class CudnnGRU(_CudnnRNNNoInputC):\n  279:   \"\"\"Cudnn implementation of the GRU model.\n  280    \"\"\"\n  281:   __doc__ += _cudnn_rnn_common_doc_string\n  282    _rnn_mode = \"gru\"\n  283  \n  284  \n  285: class CudnnRNNTanh(_CudnnRNNNoInputC):\n  286:   \"\"\"Cudnn implementation of the RNN-tanh model.\n  287    \"\"\"\n  288:   __doc__ += _cudnn_rnn_common_doc_string\n  289    _rnn_mode = \"rnn_tanh\"\n  290  \n  291  \n  292: class CudnnRNNRelu(_CudnnRNNNoInputC):\n  293:   \"\"\"Cudnn implementation of the RNN-relu model.\n  294    \"\"\"\n  295:   __doc__ += _cudnn_rnn_common_doc_string\n  296    _rnn_mode = \"rnn_relu\"\n  297  \n  298  \n  299: @ops.RegisterGradient(\"CudnnRNN\")\n  300: def _cudnn_rnn_backward(op, *grad):\n  301    if not op.get_attr(\"is_training\"):\n  302      raise ValueError(\n  303:         \"CudnnRNN must set is_training to True to be used in gradients\")\n  304:   return gen_cudnn_rnn_ops.cudnn_rnn_backprop(\n  305        input=op.inputs[0],\n  306        input_h=op.inputs[1],\n  ...\n  319  \n  320  \n  321: ops.RegisterShape(\"CudnnRNNParamsSize\")(common_shapes.call_cpp_shape_fn)\n  322: ops.RegisterShape(\"CudnnRNN\")(common_shapes.call_cpp_shape_fn)\n  323: ops.RegisterShape(\"CudnnRNNBackprop\")(common_shapes.call_cpp_shape_fn)\n  324  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\tfprof\\tools\\tfprof\\internal\\testdata\\graph.pbtxt:\n  326    }\n  327    attr {\n  328:     key: \"use_cudnn_on_gpu\"\n  329      value {\n  330        b: true\n  ...\n  626    }\n  627    attr {\n  628:     key: \"use_cudnn_on_gpu\"\n  629      value {\n  630        b: true\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\BUILD:\n  296          \"util/tensor_slice_reader_cache.h\",\n  297          \"util/tensor_slice_writer.h\",\n  298:         \"util/use_cudnn.h\",\n  299          \"util/util.h\",\n  300          \"util/work_sharder.h\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\avgpooling_op.cc:\n   40  #include \"tensorflow/core/kernels/maxpooling_op_gpu.h\"\n   41  #include \"tensorflow/core/kernels/pooling_ops_common_gpu.h\"\n   42: #include \"tensorflow/core/util/use_cudnn.h\"\n   43  #endif  // GOOGLE_CUDA\n   44  \n   ..\n  152      TensorShape output_shape = params.forward_output_shape();\n  153  \n  154:     if (internal::AvgPoolUseCudnn() || data_format_ == FORMAT_NCHW) {\n  155        DnnPoolingOp<T>::Compute(\n  156            context, perftools::gputools::dnn::PoolingMode::kAverage, ksize_,\n  ...\n  360  #if GOOGLE_CUDA\n  361  \n  362: // A CUDNN based AvgPoolingGrad implementation. It includes the padding as the\n  363  // candidates for the pooling operation.\n  364  template <class T>\n  ...\n  424                              .TypeConstraint<float>(\"T\")\n  425                              .HostMemory(\"orig_input_shape\")\n  426:                             .Label(\"cudnn\"),\n  427                          AvgPoolingGradOp<GPUDevice, float>);\n  428  REGISTER_KERNEL_BUILDER(Name(\"AvgPoolGrad\")\n  ...\n  430                              .TypeConstraint<Eigen::half>(\"T\")\n  431                              .HostMemory(\"orig_input_shape\")\n  432:                             .Label(\"cudnn\"),\n  433                          AvgPoolingGradOp<GPUDevice, Eigen::half>);\n  434  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\BUILD:\n 1534      srcs = [\n 1535          \"avgpooling_op.cc\",\n 1536:         \"cudnn_pooling_gpu.cc\",\n 1537          \"fractional_avg_pool_op.cc\",\n 1538          \"fractional_max_pool_op.cc\",\n ....\n 1544      hdrs = [\n 1545          \"avgpooling_op.h\",\n 1546:         \"cudnn_pooling_gpu.h\",\n 1547          \"fractional_pool_common.h\",\n 1548          \"maxpooling_op.h\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\conv_grad_ops.cc:\n   38  #include \"tensorflow/core/util/padding.h\"\n   39  #include \"tensorflow/core/util/tensor_format.h\"\n   40: #include \"tensorflow/core/util/use_cudnn.h\"\n   41  #include \"tensorflow/core/util/work_sharder.h\"\n   42  \n   ..\n  918          errors::InvalidArgument(\"Current implementation does not yet support \"\n  919                                  \"strides in the batch and depth dimensions.\"));\n  920:     OP_REQUIRES_OK(context, context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_));\n  921:     use_cudnn_ &= CanUseCudnn();\n  922:     cudnn_use_autotune_ = CudnnUseAutotune();\n  923      OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n  924    }\n  ...\n  965                                     dims.cols.input_size);\n  966  \n  967:     // TODO(keveman): cuDNN only supports equal padding on both sides, so only\n  968:     // calling it when that is true. Remove this check when (if?) cuDNN starts\n  969      // supporting different padding.\n  970      bool rows_odd = (padding_rows % 2 != 0);\n  ...\n  974      OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));\n  975  \n  976:     if (!use_cudnn_) {\n  977        context->SetStatus(errors::Unimplemented(\n  978            \"Conv2DBackpropInput for GPU is not currently supported \"\n  979:           \"without cudnn\"));\n  980        return;\n  981      }\n  ...\n 1016      if (rows_odd || cols_odd) {\n 1017        // If a padding dimension is odd, we have one more element on the right\n 1018:       // side or the bottom side. This is unsupported in cudnn. Therefore,\n 1019        // we pad that extra element and make it compatible.\n 1020        compatible_input_shape = ShapeFromFormat(\n ....\n 1052  \n 1053      // NOTE(keveman):\n 1054:     // cuDNN only supports the following layouts :\n 1055      // Input  : B x D x R x C\n 1056      // Filter : OD x ID x R x C\n ....\n 1061      // The first TransformDepth performs\n 1062      // (B x R x C x D) => (B x D x R x C).\n 1063:     // Since the tensor returned from cuDNN is B x D x R x C also,\n 1064      // the second TransformDepth performs\n 1065      // (B x D x R x C) => (B x R x C x D).\n ....\n 1119                         pre_transformed_in_backprop.template flat<T>().size());\n 1120  \n 1121:     static int64 ConvolveBackwardDataScratchSize = GetCudnnWorkspaceLimit(\n 1122:         \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32  // 4GB by default\n 1123          );\n 1124:     CudnnScratchAllocator scratch_allocator(ConvolveBackwardDataScratchSize,\n 1125                                              context);\n 1126      int device_id = stream->parent()->device_ordinal();\n ....\n 1140      };\n 1141      AlgorithmConfig algorithm_config;\n 1142:     if (cudnn_use_autotune_ &&\n 1143          !autotune_results_.Find(conv_parameters, &algorithm_config)) {\n 1144        std::vector<AlgorithmType> algorithms;\n ....\n 1149          // TODO(zhengxq): profile each algorithm multiple times to better\n 1150          // accuracy.\n 1151:         CudnnScratchAllocator scratch_allocator(ConvolveBackwardDataScratchSize,\n 1152                                                  context);\n 1153          ProfileResult profile_result;\n 1154:         bool cudnn_launch_status =\n 1155              stream\n 1156                  ->ThenConvolveBackwardDataWithAlgorithm(\n ....\n 1159                      AlgorithmConfig(profile_algorithm), &profile_result)\n 1160                  .ok();\n 1161:         if (cudnn_launch_status) {\n 1162            if (profile_result.is_valid()) {\n 1163              if (profile_result.elapsed_time_in_ms() <\n ....\n 1185        autotune_results_.Insert(conv_parameters, algorithm_config);\n 1186      }\n 1187:     bool cudnn_launch_status =\n 1188          stream\n 1189              ->ThenConvolveBackwardDataWithAlgorithm(\n ....\n 1193              .ok();\n 1194  \n 1195:     if (!cudnn_launch_status) {\n 1196        context->SetStatus(errors::Internal(\n 1197:           \"cuDNN Backward Data function launch failure : input shape(\",\n 1198            input_shape.DebugString(), \") filter shape(\",\n 1199            filter_shape.DebugString(), \")\"));\n ....\n 1239    std::vector<int32> strides_;\n 1240    Padding padding_;\n 1241:   bool use_cudnn_;\n 1242    TensorFormat data_format_;\n 1243    AutoTuneMap<ConvParameters, perftools::gputools::dnn::AlgorithmConfig>\n 1244        autotune_results_;\n 1245:   bool cudnn_use_autotune_;\n 1246  \n 1247    TF_DISALLOW_COPY_AND_ASSIGN(Conv2DSlowBackpropInputOp);\n ....\n 1265          errors::InvalidArgument(\"Current implementation does not yet support \"\n 1266                                  \"strides in the batch and depth dimensions.\"));\n 1267:     OP_REQUIRES_OK(context, context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_));\n 1268:     use_cudnn_ &= CanUseCudnn();\n 1269:     cudnn_use_autotune_ = CudnnUseAutotune();\n 1270      OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n 1271    }\n ....\n 1312                                     dims.cols.input_size);\n 1313  \n 1314:     // TODO(zhengxq): cuDNN only supports equal padding on both sides, so only\n 1315:     // calling it when that is true. Remove this check when (if?) cuDNN starts\n 1316      // supporting different padding.\n 1317      bool rows_odd = (padding_rows % 2 != 0);\n ....\n 1321      OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));\n 1322  \n 1323:     if (!use_cudnn_) {\n 1324        context->SetStatus(errors::Unimplemented(\n 1325            \"Conv2DBackprop for GPU is not currently supported \"\n 1326:           \"without cudnn\"));\n 1327        return;\n 1328      }\n ....\n 1370      if (rows_odd || cols_odd) {\n 1371        // If a padding dimension is odd, we have one more element on the right\n 1372:       // side or the bottom side. This is unsupported in cudnn. Therefore,\n 1373        // we pad that extra element and make it compatible.\n 1374        OP_REQUIRES_OK(\n ....\n 1416  \n 1417      // NOTE(zhengxq):\n 1418:     // cuDNN only supports the following layouts :\n 1419      // Input  : B x D x R x C\n 1420      // Filter : OD x ID x R x C\n ....\n 1425      // The first TransformDepth performs\n 1426      // (B x R x C x D) => (B x D x R x C).\n 1427:     // Since the tensor returned from cuDNN is B x D x R x C also,\n 1428      // the second TransformDepth performs\n 1429      // (B x D x R x C) => (B x R x C x D).\n ....\n 1490                         transformed_input.template flat<T>().size());\n 1491  \n 1492:     static int64 ConvolveBackwardFilterScratchSize = GetCudnnWorkspaceLimit(\n 1493:         \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32  // 4GB by default\n 1494          );\n 1495      int device_id = stream->parent()->device_ordinal();\n ....\n 1509      };\n 1510      AlgorithmConfig algorithm_config;\n 1511:     if (cudnn_use_autotune_ &&\n 1512          !autotune_results_.Find(conv_parameters, &algorithm_config)) {\n 1513        std::vector<AlgorithmType> algorithms;\n ....\n 1518          // TODO(zhengxq): profile each algorithm multiple times to better\n 1519          // accuracy.\n 1520:         CudnnScratchAllocator scratch_allocator(\n 1521              ConvolveBackwardFilterScratchSize, context);\n 1522          ProfileResult profile_result;\n 1523:         bool cudnn_launch_status =\n 1524              stream\n 1525                  ->ThenConvolveBackwardFilterWithAlgorithm(\n ....\n 1529                      &profile_result)\n 1530                  .ok();\n 1531:         if (cudnn_launch_status) {\n 1532            if (profile_result.is_valid()) {\n 1533              if (profile_result.elapsed_time_in_ms() <\n ....\n 1555        autotune_results_.Insert(conv_parameters, algorithm_config);\n 1556      }\n 1557:     CudnnScratchAllocator scratch_allocator(ConvolveBackwardFilterScratchSize,\n 1558                                              context);\n 1559:     bool cudnn_launch_status =\n 1560          stream\n 1561              ->ThenConvolveBackwardFilterWithAlgorithm(\n ....\n 1565              .ok();\n 1566  \n 1567:     if (!cudnn_launch_status) {\n 1568        context->SetStatus(errors::Internal(\n 1569:           \"cuDNN Backward Filter function launch failure : input shape(\",\n 1570            input_shape.DebugString(), \") filter shape(\",\n 1571            filter_shape.DebugString(), \")\"));\n ....\n 1583    std::vector<int32> strides_;\n 1584    Padding padding_;\n 1585:   bool use_cudnn_;\n 1586    TensorFormat data_format_;\n 1587    AutoTuneMap<ConvParameters, perftools::gputools::dnn::AlgorithmConfig>\n 1588        autotune_results_;\n 1589:   bool cudnn_use_autotune_;\n 1590  \n 1591    TF_DISALLOW_COPY_AND_ASSIGN(Conv2DSlowBackpropFilterOp);\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\conv_grad_ops_3d.cc:\n  450      TensorShape compatible_input_shape;\n  451      if (rows_odd || cols_odd || planes_odd) {\n  452:       // cuDNN only supports the same amount of padding on both sides.\n  453        compatible_input_shape = {\n  454            batch,\n  ...\n  534                         pre_transformed_in_backprop.template flat<T>().size());\n  535  \n  536:     static int64 ConvolveBackwardDataScratchSize = GetCudnnWorkspaceLimit(\n  537:         \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32);  // 4GB by default\n  538  \n  539:     CudnnScratchAllocator scratch_allocator(ConvolveBackwardDataScratchSize,\n  540                                              context);\n  541:     bool cudnn_launch_status =\n  542          stream\n  543              ->ThenConvolveBackwardDataWithScratch(\n  ...\n  546              .ok();\n  547  \n  548:     if (!cudnn_launch_status) {\n  549        context->SetStatus(errors::Internal(\n  550:           \"cuDNN Backward Data function launch failure : input shape(\",\n  551            input_shape.DebugString(), \") filter shape(\",\n  552            filter_shape.DebugString(), \")\"));\n  ...\n  761                         transformed_input.template flat<T>().size());\n  762  \n  763:     static int64 ConvolveBackwardFilterScratchSize = GetCudnnWorkspaceLimit(\n  764:         \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32);  // 4GB by default\n  765:     CudnnScratchAllocator scratch_allocator(ConvolveBackwardFilterScratchSize,\n  766                                              context);\n  767:     bool cudnn_launch_status =\n  768          stream\n  769              ->ThenConvolveBackwardFilterWithScratch(\n  ...\n  772              .ok();\n  773  \n  774:     if (!cudnn_launch_status) {\n  775        context->SetStatus(errors::Internal(\n  776:           \"cuDNN Backward Filter function launch failure : input shape(\",\n  777            input_shape.DebugString(), \") filter shape(\",\n  778            filter_shape.DebugString(), \")\"));\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\conv_ops.cc:\n   41  #include \"tensorflow/core/util/padding.h\"\n   42  #include \"tensorflow/core/util/tensor_format.h\"\n   43: #include \"tensorflow/core/util/use_cudnn.h\"\n   44  \n   45  #if GOOGLE_CUDA\n   ..\n   96  class LaunchConv2DOp<CPUDevice, T> {\n   97   public:\n   98:   void launch(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n   99                const Tensor& input, const Tensor& filter, int row_stride,\n  100                int col_stride, const Eigen::PaddingType& padding, Tensor* output,\n  ...\n  167      OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n  168                  errors::InvalidArgument(\"Invalid data format\"));\n  169:     OP_REQUIRES_OK(context, context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_));\n  170:     use_cudnn_ &= CanUseCudnn();\n  171:     cudnn_use_autotune_ = CudnnUseAutotune();\n  172      OP_REQUIRES(context, strides_.size() == 4,\n  173                  errors::InvalidArgument(\"Sliding window strides field must \"\n  ...\n  283      }\n  284  \n  285:     launcher_.launch(context, use_cudnn_, cudnn_use_autotune_, input, filter,\n  286                       stride_rows, stride_cols,\n  287                       BrainPadding2EigenPadding(padding_), output, data_format_);\n  ...\n  290   private:\n  291    std::vector<int32> strides_;\n  292:   bool use_cudnn_;\n  293    Padding padding_;\n  294    TensorFormat data_format_;\n  295    LaunchConv2DOp<Device, T> launcher_;\n  296:   bool cudnn_use_autotune_;\n  297  \n  298    TF_DISALLOW_COPY_AND_ASSIGN(Conv2DOp);\n  ...\n  315  \n  316  #if GOOGLE_CUDA\n  317: int64 GetCudnnWorkspaceLimit(const string& envvar_in_mb,\n  318                               int64 default_value_in_bytes) {\n  319    const char* workspace_limit_in_mb_str = getenv(envvar_in_mb.c_str());\n  ...\n  334  template <typename T>\n  335  void LaunchConv2DOp<GPUDevice, T>::launch(\n  336:     OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n  337      const Tensor& input_param, const Tensor& filter, int row_stride,\n  338      int col_stride, const Eigen::PaddingType& padding, Tensor* output,\n  ...\n  345    OP_REQUIRES(ctx, stream, errors::Internal(\"No GPU stream available.\"));\n  346  \n  347:   if (!use_cudnn) {\n  348      ctx->SetStatus(\n  349          errors::Unimplemented(\"Conv2D for GPU is not currently supported \"\n  350:                               \"without cudnn\"));\n  351      return;\n  352    }\n  ...\n  501                       transformed_output.template flat<T>().size());\n  502  \n  503:   static int64 ConvolveScratchSize = GetCudnnWorkspaceLimit(\n  504:       \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32  // 4GB by default\n  505        );\n  506  \n  ...\n  521    };\n  522    AlgorithmConfig algorithm_config;\n  523:   if (cudnn_use_autotune &&\n  524        !autotune_results_.Find(conv_parameters, &algorithm_config)) {\n  525      std::vector<AlgorithmType> algorithms;\n  ...\n  530        // TODO(zhengxq): profile each algorithm multiple times to better\n  531        // accuracy.\n  532:       CudnnScratchAllocator scratch_allocator(ConvolveScratchSize, ctx);\n  533        ProfileResult profile_result;\n  534:       bool cudnn_launch_status =\n  535            stream\n  536                ->ThenConvolveWithAlgorithm(\n  ...\n  539                    AlgorithmConfig(profile_algorithm), &profile_result)\n  540                .ok();\n  541:       if (cudnn_launch_status) {\n  542          if (profile_result.is_valid()) {\n  543            if (profile_result.elapsed_time_in_ms() <\n  ...\n  566    }\n  567  \n  568:   CudnnScratchAllocator scratch_allocator(ConvolveScratchSize, ctx);\n  569:   bool cudnn_launch_status =\n  570        stream\n  571            ->ThenConvolveWithAlgorithm(input_desc, input_ptr, filter_desc,\n  ...\n  575            .ok();\n  576  \n  577:   if (!cudnn_launch_status) {\n  578      ctx->SetStatus(errors::Internal(\n  579:         \"cuDNN launch failure : input shape(\", input.shape().DebugString(),\n  580          \") filter shape(\", filter.shape().DebugString(), \")\"));\n  581    }\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\conv_ops.h:\n   34  class LaunchConv2DOp {\n   35   public:\n   36:   void launch(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n   37                const Tensor& input, const Tensor& filter, int row_stride,\n   38                int col_stride, const Eigen::PaddingType& padding, Tensor* output,\n   ..\n   54  class LaunchConv2DOp<Eigen::GpuDevice, T> {\n   55   public:\n   56:   void launch(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n   57                const Tensor& input, const Tensor& filter, int row_stride,\n   58                int col_stride, const Eigen::PaddingType& padding, Tensor* output,\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\conv_ops_3d.cc:\n  201        const bool planes_odd = (pad_planes % 2 != 0);\n  202  \n  203:       // Necessary because cuDNN only supports symmetric padding.\n  204        // TODO(mjanusz): Consider making this optional? This would save some\n  205        // overhead and would work as long as an op trained this way is only\n  ...\n  236      // input: [b, x, y, z, d]\n  237      // t_input: [b, d, x, y, z]\n  238:     // NCDHW is the only format universally supported by cuDNN.\n  239      functor::NHWCToNCHW<GPUDevice, T, 5>()(\n  240          ctx->eigen_device<GPUDevice>(),\n  ...\n  302                         transformed_output.template flat<T>().size());\n  303  \n  304:     static int64 ConvolveScratchSize = GetCudnnWorkspaceLimit(\n  305:         \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32);  // 4GB by default\n  306:     CudnnScratchAllocator scratch_allocator(ConvolveScratchSize, ctx);\n  307:     bool cudnn_launch_status =\n  308          stream\n  309              ->ThenConvolveWithScratch(input_desc, input_ptr, filter_desc,\n  ...\n  312              .ok();\n  313  \n  314:     if (!cudnn_launch_status) {\n  315        ctx->SetStatus(errors::Internal(\n  316:           \"cuDNN launch failure : input shape(\", input.shape().DebugString(),\n  317            \") filter shape(\", filter.shape().DebugString(), \")\"));\n  318      }\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\conv_ops_gpu.h:\n   36  }\n   37  \n   38: // Get the Cudnn workspace limit from the environment variable, which is in MB.\n   39  // Return the workspace memory limit in bytes. If no value is set, return the\n   40  // default value.\n   41: int64 GetCudnnWorkspaceLimit(const string& envvar_in_mb,\n   42                               int64 default_value_in_bytes);\n   43  \n   44: // A class to provide scratch-space allocator for Stream-Executor Cudnn\n   45  // callback. TensorFlow is responsible for releasing the temporary buffers after\n   46  // the kernel finishes.\n   47: class CudnnScratchAllocator : public perftools::gputools::ScratchAllocator {\n   48   public:\n   49:   virtual ~CudnnScratchAllocator() {}\n   50:   CudnnScratchAllocator(int64 memory_limit, OpKernelContext* context)\n   51        : memory_limit_(memory_limit), total_byte_size_(0), context_(context) {}\n   52    virtual int64 GetMemoryLimitInBytes(\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\conv_ops_gpu_3.cu.cc:\n  316  }\n  317  \n  318: // A GPU helper function that converts TensorFlow filter format to Cudnn filter\n  319  // format.\n  320  template <typename T, int NDIMS>\n  ...\n  338  };\n  339  \n  340: // Converts Cudnn filter format back to TensorFlow filter format.\n  341  template <typename T, int NDIMS>\n  342  struct ReverseTransformFilter<GPUDevice, T, NDIMS> {\n  ...\n  431  \n  432  // A GPU helper functor that converts NHWC TensorFlow data format to\n  433: // NCHW format that is accepted by Cudnn.\n  434  template <typename T, int NDIMS>\n  435  struct NHWCToNCHW<GPUDevice, T, NDIMS> {\n  ...\n  448  };\n  449  \n  450: // A GPU helper functor that converts NCHW Cudnn data format to NHWC TensorFlow\n  451  // Format.\n  452  template <typename T, int NDIMS>\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\cudnn_pooling_gpu.cc:\n   19  #include <array>\n   20  \n   21: #include \"tensorflow/core/kernels/cudnn_pooling_gpu.h\"\n   22  #include \"tensorflow/core/kernels/conv_2d.h\"\n   23  #include \"tensorflow/core/kernels/conv_3d.h\"\n   ..\n   94                      .ok();\n   95    OP_REQUIRES(context, status,\n   96:               errors::Internal(\"cudnn PoolForward launch failed\"));\n   97  \n   98    auto toConstTensor = [](const Tensor& x) -> const Tensor { return x; };\n   ..\n  202            .ok();\n  203    OP_REQUIRES(context, status,\n  204:               errors::Internal(\"cudnn PoolBackward launch failed\"));\n  205  \n  206    auto toConstTensor = [](const Tensor& x) -> const Tensor { return x; };\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\cudnn_pooling_gpu.h:\n   14  ==============================================================================*/\n   15  \n   16: // Helper functions to run 3d pooling on GPU using CuDNN.\n   17  \n   18: #ifndef TENSORFLOW_KERNELS_CUDNN_POOLING_GPU_H_\n   19: #define TENSORFLOW_KERNELS_CUDNN_POOLING_GPU_H_\n   20  \n   21  #include <array>\n   ..\n   67  }  // namespace tensorflow\n   68  \n   69: #endif  // TENSORFLOW_KERNELS_CUDNN_POOLING_GPU_H_\n   70  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\depthwise_conv_op.cc:\n   34  #include \"tensorflow/core/platform/types.h\"\n   35  #include \"tensorflow/core/util/padding.h\"\n   36: #include \"tensorflow/core/util/use_cudnn.h\"\n   37  #include \"tensorflow/core/util/work_sharder.h\"\n   38  \n   ..\n  279  \n  280      // For special case when in_depth == 1.\n  281:     use_cudnn_ = CanUseCudnn();\n  282:     cudnn_use_autotune_ = CudnnUseAutotune();\n  283    }\n  284  \n  ...\n  365  \n  366      if (std::is_same<T, float>::value && in_depth == 1) {\n  367:       launcher_.launch(context, use_cudnn_, cudnn_use_autotune_, input, filter,\n  368                         stride, stride, BrainPadding2EigenPadding(padding_),\n  369                         output, FORMAT_NHWC);\n  ...\n  399    // For the case in_depth == 1.\n  400    LaunchConv2DOp<Device, T> launcher_;\n  401:   bool use_cudnn_;\n  402:   bool cudnn_use_autotune_;\n  403  \n  404    TF_DISALLOW_COPY_AND_ASSIGN(DepthwiseConv2dNativeOp);\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\lrn_op.cc:\n  172      OP_REQUIRES(\n  173          context, beta_ >= 0.01,\n  174:         errors::InvalidArgument(\"cuDNN requires beta >= 0.01, got: \", beta_));\n  175  \n  176      OP_REQUIRES(\n  177          context, depth_radius_ > 0 && depth_radius_ <= 7,\n  178:         errors::InvalidArgument(\"cuDNN requires depth_radius in [1, 7], got: \",\n  179                                  depth_radius_));\n  180      OP_REQUIRES(\n  181          context, bias_ >= 1e-5,\n  182:         errors::InvalidArgument(\"cuDNN requires bias >= 1e-5, got: \", bias_));\n  183  \n  184      // Cast to platform-specific int to avoid conversion warnings.\n  ...\n  388      OP_REQUIRES(\n  389          context, beta_ >= 0.01,\n  390:         errors::InvalidArgument(\"cuDNN requires beta >= 0.01, got: \", beta_));\n  391  \n  392      OP_REQUIRES(\n  393          context, depth_radius_ > 0 && depth_radius_ <= 7,\n  394:         errors::InvalidArgument(\"cuDNN requires depth_radius in [1, 7], got: \",\n  395                                  depth_radius_));\n  396      OP_REQUIRES(\n  397          context, bias_ >= 1e-5,\n  398:         errors::InvalidArgument(\"cuDNN requires bias >= 1e-5, got: \", bias_));\n  399  \n  400      const int64 batch = in_grads.dim_size(0);\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\maxpooling_op.cc:\n   36  #include \"tensorflow/core/util/padding.h\"\n   37  #include \"tensorflow/core/util/tensor_format.h\"\n   38: #include \"tensorflow/core/util/use_cudnn.h\"\n   39  \n   40  #if GOOGLE_CUDA\n   ..\n  364                      \"Pooling is not yet supported on the batch dimension.\"));\n  365  \n  366:     use_dnn_ = CanUseCudnn();\n  367    }\n  368  \n  ...\n  390      } else {\n  391        CHECK(data_format_ == FORMAT_NHWC)\n  392:           << \"Non-Cudnn MaxPoolGrad only supports NHWC format\";\n  393        MaxPoolingBackwardCustomKernel<T>(context, ksize_, stride_, padding_,\n  394                                          &tensor_in, out_backprop, output_shape);\n  ...\n  590                  errors::Unimplemented(\n  591                      \"Pooling is not yet supported on the batch dimension.\"));\n  592:     use_dnn_ = CanUseCudnn();\n  593    }\n  594  \n  ...\n  611      } else {\n  612        CHECK(data_format_ == FORMAT_NHWC)\n  613:           << \"Non-Cudnn MaxPool only supports NHWC format\";\n  614        Tensor* output = nullptr;\n  615        OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\pooling_ops_3d.cc:\n   31  \n   32  #if GOOGLE_CUDA\n   33: #include \"tensorflow/core/kernels/cudnn_pooling_gpu.h\"\n   34  #endif\n   35  namespace tensorflow {\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\pooling_ops_common.cc:\n  148    }\n  149  \n  150:   /// For now, cudnn does not support NHWC format, so we need to convert it\n  151:   /// to NCHW before calling cudnn. We need to get rid of this once it is done\n  152    Tensor transformed_input;\n  153    if (data_format == FORMAT_NHWC) {\n  ...\n  174    }\n  175  \n  176:   /// Get ready to call cudnn\n  177    perftools::gputools::dnn::PoolingDescriptor pooling_desc;\n  178    pooling_desc.set_pooling_mode(pooling_mode)\n  ...\n  212                      .ok();\n  213    OP_REQUIRES(context, status,\n  214:               errors::Internal(\"cudnn PoolBackward launch failed\"));\n  215  \n  216    if (data_format == FORMAT_NHWC) {\n  ...\n  247    }\n  248  \n  249:   /// For now, cudnn does not support NHWC format, so we need to convert it\n  250:   /// to NCHW before calling cudnn. We need to get rid of this once it is done\n  251    Tensor transformed_input;\n  252    TensorShape transformed_input_shape;\n  ...\n  294      if (tensor_in) {\n  295        // For AvgPoolGrad, the original input tensor is not necessary. However,\n  296:       // cudnn still requires them to run, although they do not affect the\n  297        // results.\n  298        functor::NHWCToNCHW<GPUDevice, T, 4>()(context->eigen_device<Device>(),\n  ...\n  302      if (tensor_out) {\n  303        // For AvgPoolGrad, the original output tensor is not necessary. However,\n  304:       // cudnn still requires them to run, although they do not affect the\n  305        // results.\n  306        functor::NHWCToNCHW<GPUDevice, T, 4>()(context->eigen_device<Device>(),\n  ...\n  313    }\n  314  \n  315:   /// Get ready to call cudnn\n  316    perftools::gputools::dnn::PoolingDescriptor pooling_desc;\n  317    pooling_desc.set_pooling_mode(pooling_mode)\n  ...\n  360            .ok();\n  361    OP_REQUIRES(context, status,\n  362:               errors::Internal(\"cudnn PoolBackward launch failed\"));\n  363  \n  364    if (data_format == FORMAT_NHWC) {\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\kernels\\pooling_ops_common_gpu.h:\n   35  namespace tensorflow {\n   36  \n   37: // A helper class that launch the cudnn pooling forward operations.\n   38  template <typename T>\n   39  class DnnPoolingOp {\n   ..\n   48  };\n   49  \n   50: // A helper class that launch the cudnn pooling backward operations.\n   51  // The original input and output tensors are optional for AvgPoolGrad, but\n   52  // mandatory for MaxPoolGrad.\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\ops\\compat\\ops_history.v0.pbtxt:\n 7424    }\n 7425    attr {\n 7426:     name: \"use_cudnn_on_gpu\"\n 7427      type: \"bool\"\n 7428      default_value {\n ....\n 7470    }\n 7471    attr {\n 7472:     name: \"use_cudnn_on_gpu\"\n 7473      type: \"bool\"\n 7474      default_value {\n ....\n 7530    }\n 7531    attr {\n 7532:     name: \"use_cudnn_on_gpu\"\n 7533      type: \"bool\"\n 7534      default_value {\n ....\n 7593    }\n 7594    attr {\n 7595:     name: \"use_cudnn_on_gpu\"\n 7596      type: \"bool\"\n 7597      default_value {\n ....\n 7643    }\n 7644    attr {\n 7645:     name: \"use_cudnn_on_gpu\"\n 7646      type: \"bool\"\n 7647      default_value {\n ....\n 7707    }\n 7708    attr {\n 7709:     name: \"use_cudnn_on_gpu\"\n 7710      type: \"bool\"\n 7711      default_value {\n ....\n 7770    }\n 7771    attr {\n 7772:     name: \"use_cudnn_on_gpu\"\n 7773      type: \"bool\"\n 7774      default_value {\n ....\n 7820    }\n 7821    attr {\n 7822:     name: \"use_cudnn_on_gpu\"\n 7823      type: \"bool\"\n 7824      default_value {\n ....\n 7884    }\n 7885    attr {\n 7886:     name: \"use_cudnn_on_gpu\"\n 7887      type: \"bool\"\n 7888      default_value {\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\ops\\nn_grad.cc:\n  125      {\"T: {float, double}\",\n  126       \"strides: list(int)\",\n  127:      \"use_cudnn_on_gpu: bool = true\",\n  128       GetPaddingAttrString(),\n  129       GetConvnetDataFormatAttrString()},\n  ...\n  136                    {\"padding\", \"$padding\"},\n  137                    {\"data_format\", \"$data_format\"},\n  138:                   {\"use_cudnn_on_gpu\", \"$use_cudnn_on_gpu\"}}},\n  139  \n  140        {{\"f_shape\"}, \"Shape\", {\"filter\"}, {{\"T\", \"$T\"}}},\n  ...\n  144                    {\"padding\", \"$padding\"},\n  145                    {\"data_format\", \"$data_format\"},\n  146:                   {\"use_cudnn_on_gpu\", \"$use_cudnn_on_gpu\"}}},\n  147      });\n  148    // clang-format on\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\ops\\nn_ops.cc:\n  339      .Attr(\"T: {half, float, double}\")\n  340      .Attr(\"strides: list(int)\")\n  341:     .Attr(\"use_cudnn_on_gpu: bool = true\")\n  342      .Attr(GetPaddingAttrString())\n  343      .Attr(GetConvnetDataFormatAttrString())\n  ...\n  385      .Attr(\"T: {half, float, double}\")\n  386      .Attr(\"strides: list(int)\")\n  387:     .Attr(\"use_cudnn_on_gpu: bool = true\")\n  388      .Attr(GetPaddingAttrString())\n  389      .Attr(GetConvnetDataFormatAttrString())\n  ...\n  417  )doc\");\n  418  \n  419: // TODO(jeff): Instead of 'use_cudnn_for_gpu', maybe we should have a\n  420  // more general string attribute ('kernel_impl'?) that can be used to\n  421  // select among several possible implementations.\n  ...\n  427      .Attr(\"T: {half, float, double}\")\n  428      .Attr(\"strides: list(int)\")\n  429:     .Attr(\"use_cudnn_on_gpu: bool = true\")\n  430      .Attr(GetPaddingAttrString())\n  431      .Attr(GetConvnetDataFormatAttrString())\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\ops\\ops.pbtxt:\n 3648    }\n 3649    attr {\n 3650:     name: \"use_cudnn_on_gpu\"\n 3651      type: \"bool\"\n 3652      default_value {\n ....\n 3721    }\n 3722    attr {\n 3723:     name: \"use_cudnn_on_gpu\"\n 3724      type: \"bool\"\n 3725      default_value {\n ....\n 3793    }\n 3794    attr {\n 3795:     name: \"use_cudnn_on_gpu\"\n 3796      type: \"bool\"\n 3797      default_value {\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\platform\\default\\platform.bzl:\n    1  CUDA_VERSION = \"\"\n    2  \n    3: CUDNN_VERSION = \"\"\n    4  \n    5  PLATFORM = \"\"\n    .\n    8    return CUDA_VERSION\n    9  \n   10: def cudnn_sdk_version():\n   11:   return CUDNN_VERSION\n   12  \n   13  def cuda_library_path(name, version = cuda_sdk_version()):\n   ..\n   29      return \"lib64/lib{}_static.a\".format(name)\n   30  \n   31: def cudnn_library_path(version = cudnn_sdk_version()):\n   32    if PLATFORM == \"Darwin\":\n   33      if not version:\n   34:       return \"lib/libcudnn.dylib\"\n   35      else:\n   36:       return \"lib/libcudnn.{}.dylib\".format(version)\n   37    else:\n   38      if not version:\n   39:       return \"lib64/libcudnn.so\"\n   40      else:\n   41:       return \"lib64/libcudnn.so.{}\".format(version)\n   42  \n   43  def cupti_library_path(version = cuda_sdk_version()):\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\util\\use_cudnn.cc:\n   14  ==============================================================================*/\n   15  \n   16: #include \"tensorflow/core/util/use_cudnn.h\"\n   17  \n   18  #include <stdlib.h>\n   ..\n   35  }\n   36  \n   37: bool CanUseCudnn() { return ReadBoolFromEnvVar(\"TF_USE_CUDNN\", true); }\n   38  \n   39: bool CudnnUseAutotune() {\n   40:   return ReadBoolFromEnvVar(\"TF_CUDNN_USE_AUTOTUNE\", true);\n   41  }\n   42  \n   43  namespace internal {\n   44  \n   45: bool AvgPoolUseCudnn() {\n   46:   return ReadBoolFromEnvVar(\"TF_AVGPOOL_USE_CUDNN\", false);\n   47  }\n   48  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\core\\util\\use_cudnn.h:\n   14  ==============================================================================*/\n   15  \n   16: // The utility to check whether we have Cudnn dependency.\n   17  \n   18: #ifndef TENSORFLOW_UTIL_USE_CUDNN_H_\n   19: #define TENSORFLOW_UTIL_USE_CUDNN_H_\n   20  \n   21  namespace tensorflow {\n   22  \n   23: bool CanUseCudnn();\n   24: bool CudnnUseAutotune();\n   25  \n   26  namespace internal {\n   27  \n   28  // This function is for transition only. And it may go away at any time.\n   29: bool AvgPoolUseCudnn();\n   30  \n   31  }  // namespace internal\n   32  }  // namespace tensorflow\n   33  \n   34: #endif  // TENSORFLOW_UTIL_USE_CUDNN_H_\n   35  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\examples\\skflow\\multiple_gpu.py:\n   29  \n   30    Note: If you want to run this example with multiple GPUs, Cuda Toolkit 7.0 and \n   31:   CUDNN 6.5 V2 from NVIDIA need to be installed beforehand. \n   32    \"\"\"\n   33    with tf.device('/gpu:1'):\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\examples\\tutorials\\deepdream\\deepdream.ipynb:\n  241         \"        <script>\\n\",\n  242         \"          function load() {\\n\",\n  243:        \"            document.getElementById(&quot;graph0.8534775751&quot;).pbtxt = 'node {\\\\n  name: &quot;input&quot;\\\\n  op: &quot;Placeholder&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;shape&quot;\\\\n    value {\\\\n      shape {\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d0/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 7\\\\n          }\\\\n          dim {\\\\n            size: 7\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 37632 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d0/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d1/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 16384 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d1/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d2/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 442368 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d2/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 49152 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 73728 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 384 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 442368 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 12288 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 64 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 51200 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 128 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 24576 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 128 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 131072 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 131072 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 884736 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 32768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 128 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 307200 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 384 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 65536 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 480\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 368640 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 480\\\\n          }\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 184320 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 384 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n          dim {\\\\n            size: 204\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 705024 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 204\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 816 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 480\\\\n          }\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 30720 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 64 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 76800 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 192 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 480\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 122880 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 508\\\\n          }\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 325120 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 640 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 508\\\\n          }\\\\n          dim {\\\\n            size: 112\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 227584 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 112\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 448 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 112\\\\n          }\\\\n          dim {\\\\n            size: 224\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 903168 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 224\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 896 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 508\\\\n          }\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 48768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 96 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 153600 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 508\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 130048 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 262144 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 262144 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1179648 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1024 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 49152 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 96 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 153600 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 131072 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 112\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 229376 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 112\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 448 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 144\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 294912 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 144\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 576 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 144\\\\n          }\\\\n          dim {\\\\n            size: 288\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1492992 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 288\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1152 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 65536 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 128 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 204800 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 131072 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 528\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 540672 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1024 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 528\\\\n          }\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 337920 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 640 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n          dim {\\\\n            size: 320\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1843200 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 320\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1280 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 528\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 67584 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 128 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 409600 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 528\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 270336 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 851968 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1024 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 532480 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 640 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n          dim {\\\\n            size: 320\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1843200 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 320\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1280 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 159744 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 192 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 614400 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 425984 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 384\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1277952 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 384\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1536 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 638976 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n          dim {\\\\n            size: 384\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 2654208 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 384\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1536 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 159744 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 192 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 614400 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 425984 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 508\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 260096 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2048\\\\n          }\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 8388608 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4096 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax0/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4128768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax0/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4032 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 528\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 270336 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2048\\\\n          }\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 8388608 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4096 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax1/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4128768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax1/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4032 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax2/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4128768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax2/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4032 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d0/pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;input&quot;\\\\n  input: &quot;conv2d0/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d0/pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;conv2d0/pre_relu/conv&quot;\\\\n  input: &quot;conv2d0/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d0&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;conv2d0/pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;maxpool0&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;conv2d0&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;localresponsenorm0&quot;\\\\n  op: &quot;LRN&quot;\\\\n  input: &quot;maxpool0&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;alpha&quot;\\\\n    value {\\\\n      f: 9.99999974738e-05\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;beta&quot;\\\\n    value {\\\\n      f: 0.5\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;bias&quot;\\\\n    value {\\\\n      f: 2.0\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;depth_radius&quot;\\\\n    value {\\\\n      i: 5\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d1/pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;localresponsenorm0&quot;\\\\n  input: &quot;conv2d1/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;VALID&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d1/pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;conv2d1/pre_relu/conv&quot;\\\\n  input: &quot;conv2d1/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;conv2d1/pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d2/pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;conv2d1&quot;\\\\n  input: &quot;conv2d2/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d2/pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;conv2d2/pre_relu/conv&quot;\\\\n  input: &quot;conv2d2/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d2&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;conv2d2/pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;localresponsenorm1&quot;\\\\n  op: &quot;LRN&quot;\\\\n  input: &quot;conv2d2&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;alpha&quot;\\\\n    value {\\\\n      f: 9.99999974738e-05\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;beta&quot;\\\\n    value {\\\\n      f: 0.5\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;bias&quot;\\\\n    value {\\\\n      f: 2.0\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;depth_radius&quot;\\\\n    value {\\\\n      i: 5\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;maxpool1&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;localresponsenorm1&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool1&quot;\\\\n  input: &quot;mixed3a/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool1&quot;\\\\n  input: &quot;mixed3a/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a/3x3_bottleneck&quot;\\\\n  input: &quot;mixed3a/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool1&quot;\\\\n  input: &quot;mixed3a/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a/5x5_bottleneck&quot;\\\\n  input: &quot;mixed3a/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;maxpool1&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a/pool&quot;\\\\n  input: &quot;mixed3a/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed3a/concat/dim&quot;\\\\n  input: &quot;mixed3a/1x1&quot;\\\\n  input: &quot;mixed3a/3x3&quot;\\\\n  input: &quot;mixed3a/5x5&quot;\\\\n  input: &quot;mixed3a/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a&quot;\\\\n  input: &quot;mixed3b/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a&quot;\\\\n  input: &quot;mixed3b/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3b/3x3_bottleneck&quot;\\\\n  input: &quot;mixed3b/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a&quot;\\\\n  input: &quot;mixed3b/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3b/5x5_bottleneck&quot;\\\\n  input: &quot;mixed3b/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed3a&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3b/pool&quot;\\\\n  input: &quot;mixed3b/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed3b/concat/dim&quot;\\\\n  input: &quot;mixed3b/1x1&quot;\\\\n  input: &quot;mixed3b/3x3&quot;\\\\n  input: &quot;mixed3b/5x5&quot;\\\\n  input: &quot;mixed3b/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;maxpool4&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed3b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool4&quot;\\\\n  input: &quot;mixed4a/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool4&quot;\\\\n  input: &quot;mixed4a/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a/3x3_bottleneck&quot;\\\\n  input: &quot;mixed4a/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool4&quot;\\\\n  input: &quot;mixed4a/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a/5x5_bottleneck&quot;\\\\n  input: &quot;mixed4a/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;maxpool4&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a/pool&quot;\\\\n  input: &quot;mixed4a/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed4a/concat/dim&quot;\\\\n  input: &quot;mixed4a/1x1&quot;\\\\n  input: &quot;mixed4a/3x3&quot;\\\\n  input: &quot;mixed4a/5x5&quot;\\\\n  input: &quot;mixed4a/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a&quot;\\\\n  input: &quot;mixed4b/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a&quot;\\\\n  input: &quot;mixed4b/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b/3x3_bottleneck&quot;\\\\n  input: &quot;mixed4b/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a&quot;\\\\n  input: &quot;mixed4b/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b/5x5_bottleneck&quot;\\\\n  input: &quot;mixed4b/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed4a&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b/pool&quot;\\\\n  input: &quot;mixed4b/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed4b/concat/dim&quot;\\\\n  input: &quot;mixed4b/1x1&quot;\\\\n  input: &quot;mixed4b/3x3&quot;\\\\n  input: &quot;mixed4b/5x5&quot;\\\\n  input: &quot;mixed4b/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b&quot;\\\\n  input: &quot;mixed4c/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b&quot;\\\\n  input: &quot;mixed4c/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c/3x3_bottleneck&quot;\\\\n  input: &quot;mixed4c/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b&quot;\\\\n  input: &quot;mixed4c/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c/5x5_bottleneck&quot;\\\\n  input: &quot;mixed4c/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed4b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c/pool&quot;\\\\n  input: &quot;mixed4c/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed4c/concat/dim&quot;\\\\n  input: &quot;mixed4c/1x1&quot;\\\\n  input: &quot;mixed4c/3x3&quot;\\\\n  input: &quot;mixed4c/5x5&quot;\\\\n  input: &quot;mixed4c/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c&quot;\\\\n  input: &quot;mixed4d/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c&quot;\\\\n  input: &quot;mixed4d/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d/3x3_bottleneck&quot;\\\\n  input: &quot;mixed4d/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c&quot;\\\\n  input: &quot;mixed4d/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d/5x5_bottleneck&quot;\\\\n  input: &quot;mixed4d/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed4c&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d/pool&quot;\\\\n  input: &quot;mixed4d/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed4d/concat/dim&quot;\\\\n  input: &quot;mixed4d/1x1&quot;\\\\n  input: &quot;mixed4d/3x3&quot;\\\\n  input: &quot;mixed4d/5x5&quot;\\\\n  input: &quot;mixed4d/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d&quot;\\\\n  input: &quot;mixed4e/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d&quot;\\\\n  input: &quot;mixed4e/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4e/3x3_bottleneck&quot;\\\\n  input: &quot;mixed4e/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d&quot;\\\\n  input: &quot;mixed4e/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4e/5x5_bottleneck&quot;\\\\n  input: &quot;mixed4e/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed4d&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4e/pool&quot;\\\\n  input: &quot;mixed4e/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed4e/concat/dim&quot;\\\\n  input: &quot;mixed4e/1x1&quot;\\\\n  input: &quot;mixed4e/3x3&quot;\\\\n  input: &quot;mixed4e/5x5&quot;\\\\n  input: &quot;mixed4e/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;maxpool10&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed4e&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool10&quot;\\\\n  input: &quot;mixed5a/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool10&quot;\\\\n  input: &quot;mixed5a/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a/3x3_bottleneck&quot;\\\\n  input: &quot;mixed5a/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool10&quot;\\\\n  input: &quot;mixed5a/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a/5x5_bottleneck&quot;\\\\n  input: &quot;mixed5a/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;maxpool10&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a/pool&quot;\\\\n  input: &quot;mixed5a/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed5a/concat/dim&quot;\\\\n  input: &quot;mixed5a/1x1&quot;\\\\n  input: &quot;mixed5a/3x3&quot;\\\\n  input: &quot;mixed5a/5x5&quot;\\\\n  input: &quot;mixed5a/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a&quot;\\\\n  input: &quot;mixed5b/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a&quot;\\\\n  input: &quot;mixed5b/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5b/3x3_bottleneck&quot;\\\\n  input: &quot;mixed5b/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a&quot;\\\\n  input: &quot;mixed5b/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5b/5x5_bottleneck&quot;\\\\n  input: &quot;mixed5b/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed5a&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5b/pool&quot;\\\\n  input: &quot;mixed5b/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed5b/concat/dim&quot;\\\\n  input: &quot;mixed5b/1x1&quot;\\\\n  input: &quot;mixed5b/3x3&quot;\\\\n  input: &quot;mixed5b/5x5&quot;\\\\n  input: &quot;mixed5b/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;avgpool0&quot;\\\\n  op: &quot;AvgPool&quot;\\\\n  input: &quot;mixed5b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 7\\\\n        i: 7\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;VALID&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/pool&quot;\\\\n  op: &quot;AvgPool&quot;\\\\n  input: &quot;mixed4a&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 5\\\\n        i: 5\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;VALID&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;head0/pool&quot;\\\\n  input: &quot;head0/bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;head0/bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;head0/bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;head0/bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck/reshape/shape&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\000\\\\\\\\010\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck/reshape&quot;\\\\n  op: &quot;Reshape&quot;\\\\n  input: &quot;head0/bottleneck&quot;\\\\n  input: &quot;head0/bottleneck/reshape/shape&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/pre_relu/matmul&quot;\\\\n  op: &quot;MatMul&quot;\\\\n  input: &quot;head0/bottleneck/reshape&quot;\\\\n  input: &quot;nn0/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_a&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_b&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;nn0/pre_relu/matmul&quot;\\\\n  input: &quot;nn0/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;nn0/pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/reshape/shape&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\000\\\\\\\\004\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/reshape&quot;\\\\n  op: &quot;Reshape&quot;\\\\n  input: &quot;nn0&quot;\\\\n  input: &quot;nn0/reshape/shape&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax0/pre_activation/matmul&quot;\\\\n  op: &quot;MatMul&quot;\\\\n  input: &quot;nn0/reshape&quot;\\\\n  input: &quot;softmax0/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_a&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_b&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax0/pre_activation&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;softmax0/pre_activation/matmul&quot;\\\\n  input: &quot;softmax0/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax0&quot;\\\\n  op: &quot;Softmax&quot;\\\\n  input: &quot;softmax0/pre_activation&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/pool&quot;\\\\n  op: &quot;AvgPool&quot;\\\\n  input: &quot;mixed4d&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 5\\\\n        i: 5\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;VALID&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;head1/pool&quot;\\\\n  input: &quot;head1/bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;head1/bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;head1/bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;head1/bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck/reshape/shape&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\000\\\\\\\\010\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck/reshape&quot;\\\\n  op: &quot;Reshape&quot;\\\\n  input: &quot;head1/bottleneck&quot;\\\\n  input: &quot;head1/bottleneck/reshape/shape&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/pre_relu/matmul&quot;\\\\n  op: &quot;MatMul&quot;\\\\n  input: &quot;head1/bottleneck/reshape&quot;\\\\n  input: &quot;nn1/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_a&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_b&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;nn1/pre_relu/matmul&quot;\\\\n  input: &quot;nn1/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;nn1/pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/reshape/shape&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\000\\\\\\\\004\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/reshape&quot;\\\\n  op: &quot;Reshape&quot;\\\\n  input: &quot;nn1&quot;\\\\n  input: &quot;nn1/reshape/shape&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax1/pre_activation/matmul&quot;\\\\n  op: &quot;MatMul&quot;\\\\n  input: &quot;nn1/reshape&quot;\\\\n  input: &quot;softmax1/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_a&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_b&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax1/pre_activation&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;softmax1/pre_activation/matmul&quot;\\\\n  input: &quot;softmax1/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax1&quot;\\\\n  op: &quot;Softmax&quot;\\\\n  input: &quot;softmax1/pre_activation&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;avgpool0/reshape/shape&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\000\\\\\\\\004\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;avgpool0/reshape&quot;\\\\n  op: &quot;Reshape&quot;\\\\n  input: &quot;avgpool0&quot;\\\\n  input: &quot;avgpool0/reshape/shape&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax2/pre_activation/matmul&quot;\\\\n  op: &quot;MatMul&quot;\\\\n  input: &quot;avgpool0/reshape&quot;\\\\n  input: &quot;softmax2/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_a&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_b&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax2/pre_activation&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;softmax2/pre_activation/matmul&quot;\\\\n  input: &quot;softmax2/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax2&quot;\\\\n  op: &quot;Softmax&quot;\\\\n  input: &quot;softmax2/pre_activation&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;output&quot;\\\\n  op: &quot;Identity&quot;\\\\n  input: &quot;softmax0&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;output1&quot;\\\\n  op: &quot;Identity&quot;\\\\n  input: &quot;softmax1&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;output2&quot;\\\\n  op: &quot;Identity&quot;\\\\n  input: &quot;softmax2&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\n';\\n\",\n  244         \"          }\\n\",\n  245         \"        </script>\\n\",\n  ...\n  673         \"        <script>\\n\",\n  674         \"          function load() {\\n\",\n  675:        \"            document.getElementById(&quot;graph0.536811672345&quot;).pbtxt = 'node {\\\\n  name: &quot;lap_in&quot;\\\\n  op: &quot;Placeholder&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;shape&quot;\\\\n    value {\\\\n      shape {\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;ExpandDims/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;ExpandDims&quot;\\\\n  op: &quot;ExpandDims&quot;\\\\n  input: &quot;lap_in&quot;\\\\n  input: &quot;ExpandDims/dim&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/Conv2D/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/Conv2D&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;ExpandDims&quot;\\\\n  input: &quot;split/Conv2D/filter&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;ExpandDims&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;split/Shape&quot;\\\\n  input: &quot;split/conv2d_transpose/filter&quot;\\\\n  input: &quot;split/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/sub&quot;\\\\n  op: &quot;Sub&quot;\\\\n  input: &quot;ExpandDims&quot;\\\\n  input: &quot;split/conv2d_transpose&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/Conv2D/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/Conv2D&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;split/Conv2D&quot;\\\\n  input: &quot;split_1/Conv2D/filter&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;split/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;split_1/Shape&quot;\\\\n  input: &quot;split_1/conv2d_transpose/filter&quot;\\\\n  input: &quot;split_1/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/sub&quot;\\\\n  op: &quot;Sub&quot;\\\\n  input: &quot;split/Conv2D&quot;\\\\n  input: &quot;split_1/conv2d_transpose&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/Conv2D/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/Conv2D&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;split_1/Conv2D&quot;\\\\n  input: &quot;split_2/Conv2D/filter&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;split_1/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;split_2/Shape&quot;\\\\n  input: &quot;split_2/conv2d_transpose/filter&quot;\\\\n  input: &quot;split_2/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/sub&quot;\\\\n  op: &quot;Sub&quot;\\\\n  input: &quot;split_1/Conv2D&quot;\\\\n  input: &quot;split_2/conv2d_transpose&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/Conv2D/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/Conv2D&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;split_2/Conv2D&quot;\\\\n  input: &quot;split_3/Conv2D/filter&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;split_2/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;split_3/Shape&quot;\\\\n  input: &quot;split_3/conv2d_transpose/filter&quot;\\\\n  input: &quot;split_3/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/sub&quot;\\\\n  op: &quot;Sub&quot;\\\\n  input: &quot;split_2/Conv2D&quot;\\\\n  input: &quot;split_3/conv2d_transpose&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Square&quot;\\\\n  op: &quot;Square&quot;\\\\n  input: &quot;split_3/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Rank&quot;\\\\n  op: &quot;Rank&quot;\\\\n  input: &quot;normalize/Square&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/range/start&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/range/delta&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/range&quot;\\\\n  op: &quot;Range&quot;\\\\n  input: &quot;normalize/range/start&quot;\\\\n  input: &quot;normalize/Rank&quot;\\\\n  input: &quot;normalize/range/delta&quot;\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Mean&quot;\\\\n  op: &quot;Mean&quot;\\\\n  input: &quot;normalize/Square&quot;\\\\n  input: &quot;normalize/range&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;keep_dims&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Sqrt&quot;\\\\n  op: &quot;Sqrt&quot;\\\\n  input: &quot;normalize/Mean&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Maximum/y&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n        }\\\\n        float_val: 1.00000001335e-10\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Maximum&quot;\\\\n  op: &quot;Maximum&quot;\\\\n  input: &quot;normalize/Sqrt&quot;\\\\n  input: &quot;normalize/Maximum/y&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/div&quot;\\\\n  op: &quot;Div&quot;\\\\n  input: &quot;split_3/Conv2D&quot;\\\\n  input: &quot;normalize/Maximum&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Square&quot;\\\\n  op: &quot;Square&quot;\\\\n  input: &quot;split_3/sub&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Rank&quot;\\\\n  op: &quot;Rank&quot;\\\\n  input: &quot;normalize_1/Square&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/range/start&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/range/delta&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/range&quot;\\\\n  op: &quot;Range&quot;\\\\n  input: &quot;normalize_1/range/start&quot;\\\\n  input: &quot;normalize_1/Rank&quot;\\\\n  input: &quot;normalize_1/range/delta&quot;\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Mean&quot;\\\\n  op: &quot;Mean&quot;\\\\n  input: &quot;normalize_1/Square&quot;\\\\n  input: &quot;normalize_1/range&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;keep_dims&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Sqrt&quot;\\\\n  op: &quot;Sqrt&quot;\\\\n  input: &quot;normalize_1/Mean&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Maximum/y&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n        }\\\\n        float_val: 1.00000001335e-10\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Maximum&quot;\\\\n  op: &quot;Maximum&quot;\\\\n  input: &quot;normalize_1/Sqrt&quot;\\\\n  input: &quot;normalize_1/Maximum/y&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/div&quot;\\\\n  op: &quot;Div&quot;\\\\n  input: &quot;split_3/sub&quot;\\\\n  input: &quot;normalize_1/Maximum&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Square&quot;\\\\n  op: &quot;Square&quot;\\\\n  input: &quot;split_2/sub&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Rank&quot;\\\\n  op: &quot;Rank&quot;\\\\n  input: &quot;normalize_2/Square&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/range/start&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/range/delta&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/range&quot;\\\\n  op: &quot;Range&quot;\\\\n  input: &quot;normalize_2/range/start&quot;\\\\n  input: &quot;normalize_2/Rank&quot;\\\\n  input: &quot;normalize_2/range/delta&quot;\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Mean&quot;\\\\n  op: &quot;Mean&quot;\\\\n  input: &quot;normalize_2/Square&quot;\\\\n  input: &quot;normalize_2/range&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;keep_dims&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Sqrt&quot;\\\\n  op: &quot;Sqrt&quot;\\\\n  input: &quot;normalize_2/Mean&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Maximum/y&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n        }\\\\n        float_val: 1.00000001335e-10\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Maximum&quot;\\\\n  op: &quot;Maximum&quot;\\\\n  input: &quot;normalize_2/Sqrt&quot;\\\\n  input: &quot;normalize_2/Maximum/y&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/div&quot;\\\\n  op: &quot;Div&quot;\\\\n  input: &quot;split_2/sub&quot;\\\\n  input: &quot;normalize_2/Maximum&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Square&quot;\\\\n  op: &quot;Square&quot;\\\\n  input: &quot;split_1/sub&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Rank&quot;\\\\n  op: &quot;Rank&quot;\\\\n  input: &quot;normalize_3/Square&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/range/start&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/range/delta&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/range&quot;\\\\n  op: &quot;Range&quot;\\\\n  input: &quot;normalize_3/range/start&quot;\\\\n  input: &quot;normalize_3/Rank&quot;\\\\n  input: &quot;normalize_3/range/delta&quot;\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Mean&quot;\\\\n  op: &quot;Mean&quot;\\\\n  input: &quot;normalize_3/Square&quot;\\\\n  input: &quot;normalize_3/range&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;keep_dims&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Sqrt&quot;\\\\n  op: &quot;Sqrt&quot;\\\\n  input: &quot;normalize_3/Mean&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Maximum/y&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n        }\\\\n        float_val: 1.00000001335e-10\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Maximum&quot;\\\\n  op: &quot;Maximum&quot;\\\\n  input: &quot;normalize_3/Sqrt&quot;\\\\n  input: &quot;normalize_3/Maximum/y&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/div&quot;\\\\n  op: &quot;Div&quot;\\\\n  input: &quot;split_1/sub&quot;\\\\n  input: &quot;normalize_3/Maximum&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Square&quot;\\\\n  op: &quot;Square&quot;\\\\n  input: &quot;split/sub&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Rank&quot;\\\\n  op: &quot;Rank&quot;\\\\n  input: &quot;normalize_4/Square&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/range/start&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/range/delta&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/range&quot;\\\\n  op: &quot;Range&quot;\\\\n  input: &quot;normalize_4/range/start&quot;\\\\n  input: &quot;normalize_4/Rank&quot;\\\\n  input: &quot;normalize_4/range/delta&quot;\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Mean&quot;\\\\n  op: &quot;Mean&quot;\\\\n  input: &quot;normalize_4/Square&quot;\\\\n  input: &quot;normalize_4/range&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;keep_dims&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Sqrt&quot;\\\\n  op: &quot;Sqrt&quot;\\\\n  input: &quot;normalize_4/Mean&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Maximum/y&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n        }\\\\n        float_val: 1.00000001335e-10\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Maximum&quot;\\\\n  op: &quot;Maximum&quot;\\\\n  input: &quot;normalize_4/Sqrt&quot;\\\\n  input: &quot;normalize_4/Maximum/y&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/div&quot;\\\\n  op: &quot;Div&quot;\\\\n  input: &quot;split/sub&quot;\\\\n  input: &quot;normalize_4/Maximum&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;normalize_1/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;merge/Shape&quot;\\\\n  input: &quot;merge/conv2d_transpose/filter&quot;\\\\n  input: &quot;normalize/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge/add&quot;\\\\n  op: &quot;Add&quot;\\\\n  input: &quot;merge/conv2d_transpose&quot;\\\\n  input: &quot;normalize_1/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_1/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;normalize_2/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_1/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_1/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;merge_1/Shape&quot;\\\\n  input: &quot;merge_1/conv2d_transpose/filter&quot;\\\\n  input: &quot;merge/add&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_1/add&quot;\\\\n  op: &quot;Add&quot;\\\\n  input: &quot;merge_1/conv2d_transpose&quot;\\\\n  input: &quot;normalize_2/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_2/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;normalize_3/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_2/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_2/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;merge_2/Shape&quot;\\\\n  input: &quot;merge_2/conv2d_transpose/filter&quot;\\\\n  input: &quot;merge_1/add&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_2/add&quot;\\\\n  op: &quot;Add&quot;\\\\n  input: &quot;merge_2/conv2d_transpose&quot;\\\\n  input: &quot;normalize_3/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_3/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;normalize_4/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_3/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_3/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;merge_3/Shape&quot;\\\\n  input: &quot;merge_3/conv2d_transpose/filter&quot;\\\\n  input: &quot;merge_2/add&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_3/add&quot;\\\\n  op: &quot;Add&quot;\\\\n  input: &quot;merge_3/conv2d_transpose&quot;\\\\n  input: &quot;normalize_4/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;Slice/begin&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 4\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;Slice/size&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 4\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\001\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;Slice&quot;\\\\n  op: &quot;Slice&quot;\\\\n  input: &quot;merge_3/add&quot;\\\\n  input: &quot;Slice/begin&quot;\\\\n  input: &quot;Slice/size&quot;\\\\n  attr {\\\\n    key: &quot;Index&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;Squeeze&quot;\\\\n  op: &quot;Squeeze&quot;\\\\n  input: &quot;Slice&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;squeeze_dims&quot;\\\\n    value {\\\\n      list {\\\\n        i: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\n';\\n\",\n  676         \"          }\\n\",\n  677         \"        </script>\\n\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\g3doc\\api_docs\\python\\functions_and_classes\\shard1\\tf.nn.conv1d.md:\n    1: ### `tf.nn.conv1d(value, filters, stride, padding, use_cudnn_on_gpu=None, data_format=None, name=None)` {#conv1d}\n    2  \n    3  Computes a 1-D convolution given 3-D input and filter tensors.\n    .\n   25      the filter is moved right at each step.\n   26  *  <b>`padding`</b>: 'SAME' or 'VALID'\n   27: *  <b>`use_cudnn_on_gpu`</b>: An optional `bool`.  Defaults to `True`.\n   28  *  <b>`data_format`</b>: An optional `string` from `\"NHWC\", \"NCHW\"`.  Defaults\n   29      to `\"NHWC\"`, the data is stored in the order of\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\g3doc\\api_docs\\python\\functions_and_classes\\shard8\\tf.nn.conv2d.md:\n    1: ### `tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)` {#conv2d}\n    2  \n    3  Computes a 2-D convolution given 4-D `input` and `filter` tensors.\n    .\n   35  *  <b>`padding`</b>: A `string` from: `\"SAME\", \"VALID\"`.\n   36      The type of padding algorithm to use.\n   37: *  <b>`use_cudnn_on_gpu`</b>: An optional `bool`. Defaults to `True`.\n   38  *  <b>`data_format`</b>: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n   39      Specify the data format of the input and output data. With the\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\g3doc\\api_docs\\python\\nn.md:\n  283  bottom and right sides always get the one additional padded pixel. For example,\n  284  when `pad_along_height` is 5, we pad 2 pixels at the top and 3 pixels at the\n  285: bottom. Note that this is different from existing libraries such as cuDNN and\n  286  Caffe, which explicitly specify the number of padded pixels and always pad the\n  287  same number of pixels on both sides.\n  ...\n  310  - - -\n  311  \n  312: ### `tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)` {#conv2d}\n  313  \n  314  Computes a 2-D convolution given 4-D `input` and `filter` tensors.\n  ...\n  346  *  <b>`padding`</b>: A `string` from: `\"SAME\", \"VALID\"`.\n  347      The type of padding algorithm to use.\n  348: *  <b>`use_cudnn_on_gpu`</b>: An optional `bool`. Defaults to `True`.\n  349  *  <b>`data_format`</b>: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n  350      Specify the data format of the input and output data. With the\n  ...\n  603  - - -\n  604  \n  605: ### `tf.nn.conv1d(value, filters, stride, padding, use_cudnn_on_gpu=None, data_format=None, name=None)` {#conv1d}\n  606  \n  607  Computes a 1-D convolution given 3-D input and filter tensors.\n  ...\n  629      the filter is moved right at each step.\n  630  *  <b>`padding`</b>: 'SAME' or 'VALID'\n  631: *  <b>`use_cudnn_on_gpu`</b>: An optional `bool`.  Defaults to `True`.\n  632  *  <b>`data_format`</b>: An optional `string` from `\"NHWC\", \"NCHW\"`.  Defaults\n  633      to `\"NHWC\"`, the data is stored in the order of\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\g3doc\\get_started\\os_setup.md:\n    9  \n   10  The GPU version works best with Cuda Toolkit 7.5 and\n   11: cuDNN v5.  Other versions are supported (Cuda toolkit >= 7.0 and\n   12: cuDNN >= v3) only when installing from sources.\n   13  Please see [Cuda installation](#optional-install-cuda-gpus-on-linux) for\n   14  details. For Mac OS X, please see [Setup GPU for\n   ..\n   67  \n   68  # Ubuntu/Linux 64-bit, GPU enabled, Python 2.7\n   69: # Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n   70  $ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\n   71  \n   ..\n   80  \n   81  # Ubuntu/Linux 64-bit, GPU enabled, Python 3.4\n   82: # Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n   83  $ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp34-cp34m-linux_x86_64.whl\n   84  \n   ..\n   87  \n   88  # Ubuntu/Linux 64-bit, GPU enabled, Python 3.5\n   89: # Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n   90  $ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl\n   91  \n   ..\n  163  \n  164  # Ubuntu/Linux 64-bit, GPU enabled, Python 2.7\n  165: # Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n  166  (tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\n  167  \n  ...\n  176  \n  177  # Ubuntu/Linux 64-bit, GPU enabled, Python 3.4\n  178: # Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n  179  (tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp34-cp34m-linux_x86_64.whl\n  180  \n  ...\n  183  \n  184  # Ubuntu/Linux 64-bit, GPU enabled, Python 3.5\n  185: # Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n  186  (tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl\n  187  \n  ...\n  302  \n  303  # Ubuntu/Linux 64-bit, GPU enabled, Python 2.7\n  304: # Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n  305  (tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp27-none-linux_x86_64.whl\n  306  \n  ...\n  315  \n  316  # Ubuntu/Linux 64-bit, GPU enabled, Python 3.4\n  317: # Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n  318  (tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp34-cp34m-linux_x86_64.whl\n  319  \n  ...\n  322  \n  323  # Ubuntu/Linux 64-bit, GPU enabled, Python 3.5\n  324: # Requires CUDA toolkit 7.5 and CuDNN v5. For other versions, see \"Install from sources\" below.\n  325  (tensorflow)$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.10.0-cp35-cp35m-linux_x86_64.whl\n  326  \n  ...\n  455  \n  456  If you installed the GPU version of TensorFlow, you must also install the Cuda\n  457: Toolkit 7.5 and cuDNN v5.  Please see [Cuda\n  458  installation](#optional-install-cuda-gpus-on-linux).\n  459  \n  ...\n  577  \n  578  In order to build or run TensorFlow with GPU support, both NVIDIA's Cuda Toolkit\n  579: (>= 7.0) and cuDNN (>= v3) need to be installed.\n  580  \n  581  TensorFlow GPU support requires having a GPU card with NVidia Compute Capability\n  ...\n  599  Install the toolkit into e.g. `/usr/local/cuda`\n  600  \n  601: ##### Download and install cuDNN\n  602  \n  603: https://developer.nvidia.com/cudnn\n  604  \n  605: Download cuDNN v5.\n  606  \n  607: Uncompress and copy the cuDNN files into the toolkit directory. Assuming the\n  608  toolkit is installed in `/usr/local/cuda`, run the following commands (edited\n  609: to reflect the cuDNN version you downloaded):\n  610  \n  611  ``` bash\n  612: tar xvzf cudnn-7.5-linux-x64-v5.1-ga.tgz\n  613: sudo cp cuda/include/cudnn.h /usr/local/cuda/include\n  614: sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\n  615: sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\n  616  ```\n  617  \n  ...\n  680  \n  681  Finally, you will also want to install the [CUDA Deep Neural\n  682: Network](https://developer.nvidia.com/cudnn) (cuDNN v5) library which currently\n  683  requires an [Accelerated Computing Developer\n  684  Program](https://developer.nvidia.com/accelerated-computing-developer) account.\n  ...\n  687  \n  688  ```bash\n  689: $ sudo mv include/cudnn.h /Developer/NVIDIA/CUDA-7.5/include/\n  690: $ sudo mv lib/libcudnn* /Developer/NVIDIA/CUDA-7.5/lib\n  691: $ sudo ln -s /Developer/NVIDIA/CUDA-7.5/lib/libcudnn* /usr/local/cuda/lib/\n  692  ```\n  693  \n  ...\n  722  Select the option `Y` when asked to build TensorFlow with GPU support.\n  723  \n  724: If you have several versions of Cuda or cuDNN installed, you should definitely\n  725  select one explicitly instead of relying on the system default.\n  726  \n  ...\n  737  Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\n  738  Please specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\n  739: Please specify the cuDNN version you want to use. [Leave empty to use system default]: 5\n  740: Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\n  741  Please specify a list of comma-separated Cuda compute capabilities you want to build with.\n  742  You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\n  ...\n  754  This creates a canonical set of symbolic links to the Cuda libraries on your\n  755  system.  Every time you change the Cuda library paths you need to run this step\n  756: again before you invoke the bazel build command. For the cuDNN libraries, use\n  757  '7.0' for R3, and '4.0.7' for R4.\n  758  \n  ...\n  855  Make sure you followed the GPU installation\n  856  [instructions](#optional-install-cuda-gpus-on-linux). If you built from source,\n  857: and you left the Cuda or cuDNN version empty, try specifying them explicitly.\n  858  \n  859  ### Protobuf library related issues\n  ...\n 1056  >>> import tensorflow\n 1057  I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.dylib locally\n 1058: I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.dylib locally\n 1059  I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.dylib locally\n 1060  \"import tensorflow\" terminated by signal SIGSEGV (Address boundary error)\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\python\\kernel_tests\\lrn_op_test.py:\n   53        shape[3] += 1\n   54        p = tf.placeholder(dtype, shape=shape)\n   55:       # random depth_radius, bias, alpha, beta. cuDNN requires depth_radius to\n   56        # be in [1, 7].\n   57        lrn_depth_radius = np.random.randint(1, min(8, shape[3]))\n   ..\n   59        bias = 1.0 + np.random.rand()\n   60        alpha = 2.0 * np.random.rand()\n   61:       # cuDNN requires beta >= 0.01.\n   62        beta = 0.01 + 2.0 * np.random.rand()\n   63        lrn_t = tf.nn.local_response_normalization(\n   ..\n  104        # Make depth at least 2 to make it meaningful\n  105        shape[3] += 1\n  106:       # random depth_radius, bias, alpha, beta. cuDNN requires depth_radius to\n  107        # be in [1, 7].\n  108        lrn_depth_radius = np.random.randint(1, min(8, shape[3]))\n  109        bias = 1.0 + np.random.rand()\n  110        alpha = 1.0 * np.random.rand()\n  111:       # cuDNN requires beta >= 0.01.\n  112        beta = 0.01 + 1.0 * np.random.rand()\n  113        if dtype == tf.float32:\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\python\\kernel_tests\\pooling_ops_test.py:\n  803        return\n  804  \n  805:     # Test the GPU implementation that uses cudnn for now.\n  806      # It does not propagate the diff in cases of NaNs\n  807:     expected_input_backprop_cudnn = [\n  808          0.0, 0.0, 0.0, 0.0,\n  809          0.0, 0.0, 0.0, 0.0,\n  ...\n  811          0.0, 0.0, 0.0, 0.0]\n  812      self._testMaxPoolGradDirect(\n  813:         input_data, output_backprop, expected_input_backprop_cudnn,\n  814          input_sizes=[1, 4, 4, 1], output_sizes=[1, 3, 3, 1],\n  815          window_rows=2, window_cols=2, row_stride=1, col_stride=1,\n  ...\n  837        return\n  838  \n  839:     # Test the GPU implementation that uses cudnn for now.\n  840      # It does not propagate the diff in cases of NaNs\n  841:     expected_input_backprop_cudnn = [\n  842          0.0, 0.0, 0.0, 0.0,\n  843          0.0, 0.0, 0.0, 0.0,\n  ...\n  845          0.0, 0.0, 0.0, 0.0]\n  846      self._testMaxPoolGradDirect(\n  847:         input_data, output_backprop, expected_input_backprop_cudnn,\n  848          input_sizes=[1, 4, 4, 1], output_sizes=[1, 3, 3, 1],\n  849          window_rows=2, window_cols=2, row_stride=1, col_stride=1,\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\python\\ops\\nn.py:\n   80  bottom and right sides always get the one additional padded pixel. For example,\n   81  when `pad_along_height` is 5, we pad 2 pixels at the top and 3 pixels at the\n   82: bottom. Note that this is different from existing libraries such as cuDNN and\n   83  Caffe, which explicitly specify the number of padded pixels and always pad the\n   84  same number of pixels on both sides.\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\python\\ops\\nn_grad.py:\n   43                                          op.inputs[2], op.get_attr(\"strides\"),\n   44                                          op.get_attr(\"padding\"),\n   45:                                         op.get_attr(\"use_cudnn_on_gpu\"),\n   46                                          op.get_attr(\"data_format\")),\n   47            nn_ops.conv2d(grad, op.inputs[1], op.get_attr(\"strides\"),\n   48:                         op.get_attr(\"padding\"), op.get_attr(\"use_cudnn_on_gpu\"),\n   49                          op.get_attr(\"data_format\"))]\n   50  \n   ..\n   57            op.get_attr(\"strides\"),\n   58            op.get_attr(\"padding\"),\n   59:           op.get_attr(\"use_cudnn_on_gpu\"),\n   60            op.get_attr(\"data_format\")),\n   61        None,\n   ..\n   64            op.get_attr(\"strides\"),\n   65            op.get_attr(\"padding\"),\n   66:           op.get_attr(\"use_cudnn_on_gpu\"),\n   67            op.get_attr(\"data_format\"))\n   68    ]\n   ..\n  296    return [nn_ops.conv2d_backprop_input(\n  297        array_ops.shape(op.inputs[0]), op.inputs[1], grad, op.get_attr(\"strides\"),\n  298:       op.get_attr(\"padding\"), op.get_attr(\"use_cudnn_on_gpu\"),\n  299        op.get_attr(\"data_format\")),\n  300            nn_ops.conv2d_backprop_filter(op.inputs[0],\n  ...\n  302                                          op.get_attr(\"strides\"),\n  303                                          op.get_attr(\"padding\"),\n  304:                                         op.get_attr(\"use_cudnn_on_gpu\"),\n  305                                          op.get_attr(\"data_format\"))]\n  306  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\python\\ops\\nn_ops.py:\n 1228  \n 1229  def conv1d(value, filters, stride, padding,\n 1230:            use_cudnn_on_gpu=None, data_format=None,\n 1231             name=None):\n 1232    \"\"\"Computes a 1-D convolution given 3-D input and filter tensors.\n ....\n 1252        the filter is moved right at each step.\n 1253      padding: 'SAME' or 'VALID'\n 1254:     use_cudnn_on_gpu: An optional `bool`.  Defaults to `True`.\n 1255      data_format: An optional `string` from `\"NHWC\", \"NCHW\"`.  Defaults\n 1256        to `\"NHWC\"`, the data is stored in the order of\n ....\n 1268      filters = array_ops.expand_dims(filters, 0)\n 1269      result = gen_nn_ops.conv2d(value, filters, [1, 1, stride, 1], padding,\n 1270:                                use_cudnn_on_gpu=use_cudnn_on_gpu,\n 1271                                 data_format=data_format)\n 1272      return array_ops.squeeze(result, [1])\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\BUILD:\n   29          \"//tensorflow/core:cuda\",\n   30          \"@local_config_cuda//cuda:cublas\",\n   31:         \"@local_config_cuda//cuda:cudnn\",\n   32          \"@local_config_cuda//cuda:cufft\",\n   33          \"@local_config_cuda//cuda:curand\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:\n   40  #include \"tensorflow/stream_executor/stream_executor_pimpl.h\"\n   41  // clang-format off\n   42: #include \"cuda/include/cudnn.h\"\n   43  // clang-format on\n   44  \n   ..\n   55  }\n   56  \n   57: // Returns the \"Compatibility\" version number from the CuDNN version number.\n   58  // This is the number that tries to indicate ABI compatibility.\n   59  //\n   60: // For example, if cudnn_version is 5107, the compatibility version\n   61  // number will be 5100.\n   62: size_t cudnnCompatibilityVersion(size_t cudnn_version) {\n   63:   return (cudnn_version / 100) * 100;\n   64  }\n   65  \n   ..\n   77  namespace cuda {\n   78  \n   79: PLUGIN_REGISTRY_DEFINE_PLUGIN_ID(kCuDnnPlugin);\n   80  \n   81: string ToString(cudnnStatus_t status) {\n   82    switch (status) {\n   83:     case CUDNN_STATUS_SUCCESS:\n   84:       return \"CUDNN_STATUS_SUCCESS\";\n   85:     case CUDNN_STATUS_NOT_INITIALIZED:\n   86:       return \"CUDNN_STATUS_NOT_INITIALIZED\";\n   87:     case CUDNN_STATUS_ALLOC_FAILED:\n   88:       return \"CUDNN_STATUS_ALLOC_FAILED\";\n   89:     case CUDNN_STATUS_BAD_PARAM:\n   90:       return \"CUDNN_STATUS_BAD_PARAM\";\n   91:     case CUDNN_STATUS_INTERNAL_ERROR:\n   92:       return \"CUDNN_STATUS_INTERNAL_ERROR\";\n   93:     case CUDNN_STATUS_INVALID_VALUE:\n   94:       return \"CUDNN_STATUS_INVALID_VALUE\";\n   95:     case CUDNN_STATUS_ARCH_MISMATCH:\n   96:       return \"CUDNN_STATUS_ARCH_MISMATCH\";\n   97:     case CUDNN_STATUS_MAPPING_ERROR:\n   98:       return \"CUDNN_STATUS_MAPPING_ERROR\";\n   99:     case CUDNN_STATUS_EXECUTION_FAILED:\n  100:       return \"CUDNN_STATUS_EXECUTION_FAILED\";\n  101:     case CUDNN_STATUS_NOT_SUPPORTED:\n  102:       return \"CUDNN_STATUS_NOT_SUPPORTED\";\n  103:     case CUDNN_STATUS_LICENSE_ERROR:\n  104:       return \"CUDNN_STATUS_LICENSE_ERROR\";\n  105      default:\n  106:       return port::StrCat(\"<unknown cudnn status: \", static_cast<int>(status),\n  107                            \">\");\n  108    }\n  ...\n  111  namespace dynload {\n  112  \n  113: static port::ThreadPool* InitCudnnThreadpool() {\n  114:   port::ThreadPool* cudnn_threadpool_;\n  115    port::ThreadOptions options;\n  116    // TBD(keveman): Conservatively setting the stack size and guard size to 2MB,\n  ...\n  119    options.stack_size = 2 * 1024 * 1024;\n  120    options.guard_size = 2 * 1024 * 1024;\n  121:   cudnn_threadpool_ = new port::ThreadPool(port::Env::Default(), options,\n  122:                                            \"cudnn_threadpool\", 1);\n  123:   CHECK(cudnn_threadpool_);\n  124:   return cudnn_threadpool_;\n  125  }\n  126  \n  127: static mutex cudnn_threadpool_mu(LINKER_INITIALIZED);\n  128  static port::ThreadPool* GetCudaThreadpool() {\n  129:   mutex_lock lock(cudnn_threadpool_mu);\n  130:   static port::ThreadPool* cudnn_threadpool = InitCudnnThreadpool();\n  131:   return cudnn_threadpool;\n  132  }\n  133  \n  134: // Retrieves the CUDNN DSO, dies on failure.\n  135  void* GetDsoHandle() {\n  136:   static auto result = internal::CachedDsoLoader::GetCudnnDsoHandle();\n  137    return result.ValueOrDie();\n  138  }\n  139  \n  140: // Calls cudnnGetVersion in the loaded DSO.\n  141: size_t cudnnGetVersion() {\n  142:   static void* f = dlsym(GetDsoHandle(), \"cudnnGetVersion\");\n  143    if (f == nullptr) {\n  144:     LOG(FATAL) << \"could not find cudnnGetVersion in cudnn DSO; dlerror: \"\n  145                 << dlerror();\n  146    }\n  ...\n  149  }\n  150  \n  151: #define PERFTOOLS_GPUTOOLS_CUDNN_WRAP(__name)                        \\\n  152    struct DynLoadShim__##__name {                                     \\\n  153      static const char* kName;                                        \\\n  ...\n  157        if (f == nullptr) {                                            \\\n  158          LOG(FATAL) << \"could not find \" << kName                     \\\n  159:                    << \" in cudnn DSO; dlerror: \" << dlerror();       \\\n  160        }                                                              \\\n  161        return reinterpret_cast<FuncPointerT>(f);                      \\\n  162      }                                                                \\\n  163      template <typename... Args>                                      \\\n  164:     cudnnStatus_t operator()(CUDAExecutor* parent, Args... args) {   \\\n  165        cuda::ScopedActivateExecutorContext sac{parent};               \\\n  166:       cudnnStatus_t retval = DynLoad()(args...);                     \\\n  167        return retval;                                                 \\\n  168      }                                                                \\\n  ...\n  171  \n  172  // clang-format off\n  173: #define CUDNN_DNN_ROUTINE_EACH(__macro)                   \\\n  174:   __macro(cudnnBatchNormalizationBackward)                \\\n  175:   __macro(cudnnBatchNormalizationForwardInference)        \\\n  176:   __macro(cudnnBatchNormalizationForwardTraining)         \\\n  177:   __macro(cudnnGetConvolutionNdForwardOutputDim)          \\\n  178:   __macro(cudnnGetConvolutionForwardAlgorithm)            \\\n  179:   __macro(cudnnCreateTensorDescriptor)                    \\\n  180:   __macro(cudnnDestroyTensorDescriptor)                   \\\n  181:   __macro(cudnnCreateFilterDescriptor)                    \\\n  182:   __macro(cudnnSetPoolingNdDescriptor)                    \\\n  183:   __macro(cudnnSetLRNDescriptor)                          \\\n  184:   __macro(cudnnDestroyFilterDescriptor)                   \\\n  185:   __macro(cudnnCreateConvolutionDescriptor)               \\\n  186:   __macro(cudnnCreatePoolingDescriptor)                   \\\n  187:   __macro(cudnnDestroyPoolingDescriptor)                  \\\n  188:   __macro(cudnnCreateLRNDescriptor)                       \\\n  189:   __macro(cudnnDestroyLRNDescriptor)                      \\\n  190:   __macro(cudnnDestroyConvolutionDescriptor)              \\\n  191:   __macro(cudnnCreate)                                    \\\n  192:   __macro(cudnnDestroy)                                   \\\n  193:   __macro(cudnnSetStream)                                 \\\n  194:   __macro(cudnnActivationForward)                         \\\n  195:   __macro(cudnnConvolutionForward)                        \\\n  196:   __macro(cudnnConvolutionBackwardBias)                   \\\n  197:   __macro(cudnnGetConvolutionForwardWorkspaceSize)        \\\n  198:   __macro(cudnnTransformTensor)                           \\\n  199:   __macro(cudnnSetConvolutionNdDescriptor)                \\\n  200:   __macro(cudnnSetTensorNdDescriptor)                     \\\n  201:   __macro(cudnnSetFilterNdDescriptor)                     \\\n  202:   __macro(cudnnPoolingForward)                            \\\n  203:   __macro(cudnnPoolingBackward)                           \\\n  204:   __macro(cudnnLRNCrossChannelForward)                    \\\n  205:   __macro(cudnnLRNCrossChannelBackward)                   \\\n  206:   __macro(cudnnAddTensor)                                 \\\n  207:   __macro(cudnnConvolutionBackwardData)                   \\\n  208:   __macro(cudnnConvolutionBackwardFilter)\n  209  // clang-format on\n  210  \n  211: CUDNN_DNN_ROUTINE_EACH(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n  212  \n  213  // APIs available after R3:\n  214: #if CUDNN_VERSION >= 3000\n  215: #define CUDNN_DNN_ROUTINE_EACH_AFTER_R3(__macro)              \\\n  216:   __macro(cudnnGetConvolutionBackwardFilterWorkspaceSize)     \\\n  217:   __macro(cudnnGetConvolutionBackwardDataAlgorithm)           \\\n  218:   __macro(cudnnGetConvolutionBackwardFilterAlgorithm)         \\\n  219:   __macro(cudnnGetConvolutionBackwardDataWorkspaceSize)\n  220: CUDNN_DNN_ROUTINE_EACH_AFTER_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n  221: #undef CUDNN_DNN_ROUTINE_EACH_AFTER_R3\n  222  #endif\n  223  \n  224  // APIs in R3 but not in R5\n  225  // clang-format off\n  226: #if CUDNN_VERSION >= 3000 && CUDNN_VERSION < 5000\n  227: #define CUDNN_DNN_ROUTINE_EACH_R3(__macro)                    \\\n  228:   __macro(cudnnAddTensor_v3)                                  \\\n  229:   __macro(cudnnConvolutionBackwardData_v3)                    \\\n  230:   __macro(cudnnConvolutionBackwardFilter_v3)\n  231  // clang-format on\n  232  \n  233: CUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n  234: #undef CUDNN_DNN_ROUTINE_EACH_R3\n  235  #endif\n  236  \n  237  // APIs in R5\n  238  // clang-format off\n  239: #if CUDNN_VERSION >= 5000\n  240: #define CUDNN_DNN_ROUTINE_EACH_R5(__macro)                    \\\n  241:   __macro(cudnnCreateActivationDescriptor)                    \\\n  242:   __macro(cudnnSetActivationDescriptor)                       \\\n  243:   __macro(cudnnGetActivationDescriptor)                       \\\n  244:   __macro(cudnnDestroyActivationDescriptor)                   \\\n  245:   __macro(cudnnCreateDropoutDescriptor)                       \\\n  246:   __macro(cudnnDestroyDropoutDescriptor)                      \\\n  247:   __macro(cudnnSetDropoutDescriptor)                          \\\n  248:   __macro(cudnnDropoutGetStatesSize)                          \\\n  249:   __macro(cudnnCreateRNNDescriptor)                           \\\n  250:   __macro(cudnnDestroyRNNDescriptor)                          \\\n  251:   __macro(cudnnGetRNNParamsSize)                              \\\n  252:   __macro(cudnnGetRNNWorkspaceSize)                           \\\n  253:   __macro(cudnnGetRNNTrainingReserveSize)                     \\\n  254:   __macro(cudnnGetRNNLinLayerMatrixParams)                    \\\n  255:   __macro(cudnnGetRNNLinLayerBiasParams)                      \\\n  256:   __macro(cudnnRNNForwardInference)                           \\\n  257:   __macro(cudnnRNNForwardTraining)                            \\\n  258:   __macro(cudnnRNNBackwardData)                               \\\n  259:   __macro(cudnnRNNBackwardWeights)                            \\\n  260:   __macro(cudnnSetRNNDescriptor)                              \\\n  261:   __macro(cudnnGetFilterNdDescriptor)\n  262  \n  263  // clang-format on\n  264  \n  265: CUDNN_DNN_ROUTINE_EACH_R5(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n  266: #undef CUDNN_DNN_ROUTINE_EACH_R5\n  267  #endif\n  268  \n  269: #undef CUDNN_DNN_ROUTINE_EACH\n  270  \n  271  }  // namespace dynload\n  ...\n  273  namespace {\n  274  \n  275: cudnnHandle_t ToHandle(void* opaque_handle) {\n  276:   return static_cast<cudnnHandle_t>(opaque_handle);\n  277  }\n  278  \n  279: cudnnConvolutionFwdAlgo_t ToConvForwardAlgo(dnn::AlgorithmType algorithm) {\n  280:   cudnnConvolutionFwdAlgo_t algo = cudnnConvolutionFwdAlgo_t(algorithm);\n  281    switch (algo) {\n  282:     case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM:\n  283:     case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM:\n  284:     case CUDNN_CONVOLUTION_FWD_ALGO_GEMM:\n  285:     case CUDNN_CONVOLUTION_FWD_ALGO_DIRECT:\n  286:     case CUDNN_CONVOLUTION_FWD_ALGO_FFT:\n  287:     case CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING:\n  288: #if CUDNN_VERSION >= 5000\n  289:     case CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD:\n  290  #endif\n  291        return algo;\n  292      default:\n  293:       LOG(FATAL) << \"Unsupported Cudnn convolution forward algorithm: \"\n  294                   << algorithm;\n  295    }\n  296  }\n  297  \n  298: cudnnConvolutionBwdDataAlgo_t ToConvBackwardDataAlgo(\n  299      dnn::AlgorithmType algorithm) {\n  300:   cudnnConvolutionBwdDataAlgo_t algo = cudnnConvolutionBwdDataAlgo_t(algorithm);\n  301    switch (algo) {\n  302:     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_0:\n  303:     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_1:\n  304:     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT:\n  305:     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING:\n  306: #if CUDNN_VERSION >= 5000\n  307:     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD:\n  308  #endif\n  309        return algo;\n  310      default:\n  311        LOG(FATAL)\n  312:           << \"Unsupported Cudnn convolution backward algorithm for data: \"\n  313            << algorithm;\n  314    }\n  315  }\n  316  \n  317: cudnnConvolutionBwdFilterAlgo_t ToConvBackwardFilterAlgo(\n  318      dnn::AlgorithmType algorithm) {\n  319:   cudnnConvolutionBwdFilterAlgo_t algo =\n  320:       cudnnConvolutionBwdFilterAlgo_t(algorithm);\n  321    switch (algo) {\n  322:     case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0:\n  323:     case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1:\n  324:     case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT:\n  325:     case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3:\n  326        return algo;\n  327      default:\n  328        LOG(FATAL)\n  329:           << \"Unsupported Cudnn convolution backward algorithm for filter: \"\n  330            << algorithm;\n  331    }\n  ...\n  334  }  // namespace\n  335  \n  336: CudnnSupport::CudnnSupport(CUDAExecutor* parent)\n  337      : parent_(parent), dnn_handle_(nullptr) {}\n  338  \n  339: CudnnSupport::~CudnnSupport() {\n  340:   auto status = dynload::cudnnDestroy(parent_, ToHandle(dnn_handle_));\n  341:   if (status != CUDNN_STATUS_SUCCESS) {\n  342:     LOG(ERROR) << \"could not destroy cudnn handle: \" << ToString(status);\n  343    }\n  344  }\n  345  \n  346: port::Status CudnnSupport::Init() {\n  347:   auto status = dynload::cudnnCreate(\n  348:       parent_, reinterpret_cast<cudnnHandle_t*>(&dnn_handle_));\n  349:   if (status == CUDNN_STATUS_SUCCESS) {\n  350:     // Check whether loaded version of CuDNN matches what the source\n  351      // was built with.\n  352:     size_t loaded_version = dynload::cudnnGetVersion();\n  353:     size_t loaded_compat_version = cudnnCompatibilityVersion(loaded_version);\n  354:     size_t compiled_compat_version = cudnnCompatibilityVersion(CUDNN_VERSION);\n  355      bool library_loaded_matches_source =\n  356          (loaded_compat_version == compiled_compat_version);\n  357      if (!library_loaded_matches_source) {\n  358        const string error =\n  359:           port::StrCat(\"Loaded runtime CuDNN library: \", loaded_version,\n  360                         \" (compatibility version \", loaded_compat_version,\n  361:                        \") but source was compiled with \", CUDNN_VERSION,\n  362                         \" (compatibility version \", compiled_compat_version,\n  363:                        \").  If using a binary install, upgrade your CuDNN \"\n  364                         \"library to match.  If building from sources, \"\n  365                         \"make sure the library loaded at runtime matches a \"\n  ...\n  373    }\n  374  \n  375:   LOG(ERROR) << \"could not create cudnn handle: \" << ToString(status);\n  376:   if (status == CUDNN_STATUS_NOT_INITIALIZED) {\n  377      // This is the error code that the driver returns when we're not running a\n  378:     // sufficient CUDA driver -- cudnn requires 6.5+ compatibility, which\n  379      // starts with the 340.XX driver series.\n  380      auto result = cuda::Diagnostician::FindKernelDriverVersion();\n  ...\n  389        if (std::get<0>(version) < 340) {\n  390          LOG(ERROR)\n  391:             << \"cudnn library is only supported on 340.XX+ driver versions\";\n  392        }\n  393  #endif\n  ...\n  396  \n  397    return port::Status{port::error::INTERNAL,\n  398:                       port::StrCat(\"cudnn library could not create a handle: \",\n  399                                     ToString(status))};\n  400  }\n  401  \n  402: // Turns a BatchDescriptor structure into a cudnn tensor handle within a scope.\n  403  class ScopedTensorDescriptor {\n  404   public:\n  405    ScopedTensorDescriptor(CUDAExecutor* parent,\n  406                           const BatchDescriptor& batch_descriptor,\n  407:                          cudnnDataType_t elem_type)\n  408        : parent_(parent), handle_(nullptr) {\n  409:     cudnnStatus_t status =\n  410:         dynload::cudnnCreateTensorDescriptor(parent_, &handle_);\n  411:     if (status != CUDNN_STATUS_SUCCESS) {\n  412:       LOG(FATAL) << \"could not create cudnn tensor descriptor: \"\n  413                   << ToString(status);\n  414      }\n  ...\n  425  \n  426      const int nd = batch_descriptor.ndims() + 2;\n  427:     // cuDNN requires the strides and dims to be ordered as BDYX.\n  428      std::vector<int64> strides64 =\n  429          batch_descriptor.full_strides(dnn::DataLayout::kBatchDepthYX);\n  ...\n  431          batch_descriptor.full_dims(dnn::DataLayout::kBatchDepthYX);\n  432  \n  433:     // cuDNN requires arrays of ints.\n  434      std::vector<int> strides(nd);\n  435      std::vector<int> dims(nd);\n  ...\n  438      std::transform(dims64.cbegin(), dims64.cend(), dims.begin(),\n  439                     &CheckedNarrowing<int64, int>);\n  440:     status = dynload::cudnnSetTensorNdDescriptor(\n  441          parent_, handle_, elem_type, nd, dims.data(), strides.data());\n  442  \n  443:     if (status != CUDNN_STATUS_SUCCESS) {\n  444:       LOG(FATAL) << \"could not set cudnn tensor descriptor: \"\n  445                   << ToString(status);\n  446      }\n  ...\n  448  \n  449    ~ScopedTensorDescriptor() {\n  450:     cudnnStatus_t status =\n  451:         dynload::cudnnDestroyTensorDescriptor(parent_, handle_);\n  452:     if (status != CUDNN_STATUS_SUCCESS) {\n  453:       LOG(ERROR) << \"could not destroy cudnn tensor descriptor: \"\n  454                   << ToString(status);\n  455      }\n  456    }\n  457  \n  458:   cudnnTensorDescriptor_t handle() const { return handle_; }\n  459  \n  460   private:\n  461    CUDAExecutor* parent_;            // Parent executor. Not owned.\n  462:   cudnnTensorDescriptor_t handle_;  // Owned.\n  463  \n  464    SE_DISALLOW_COPY_AND_ASSIGN(ScopedTensorDescriptor);\n  465  };\n  466  \n  467: // Turns a FilterDescriptor structure into a cudnn filter handle within a scope.\n  468  class ScopedFilterDescriptor {\n  469   public:\n  ...\n  471                           const FilterDescriptor& filter_descriptor,\n  472                           const BatchDescriptor& batch_descriptor,\n  473:                          cudnnDataType_t elem_type)\n  474        : parent_(parent), handle_(nullptr) {\n  475:     cudnnStatus_t status =\n  476:         dynload::cudnnCreateFilterDescriptor(parent_, &handle_);\n  477:     if (status != CUDNN_STATUS_SUCCESS) {\n  478:       LOG(FATAL) << \"could not create cudnn filter descriptor: \"\n  479                   << ToString(status);\n  480      }\n  481  \n  482: #if CUDNN_VERSION >= 5000\n  483      // TODO(b/23032134): Even if the filter layout is not supported,\n  484:     // cudnnSetFilter4DDescriptor_v4 will return CUDNN_STATUS_SUCCESS because it\n  485:     // does not take layout as an input. Maybe force cuDNN by giving wrong\n  486      // inputs intentionally?\n  487:     cudnnTensorFormat_t format;\n  488      switch (filter_descriptor.layout()) {\n  489        case dnn::FilterLayout::kOutputInputYX:\n  490:         format = CUDNN_TENSOR_NCHW;\n  491          break;\n  492        default:\n  ...\n  503      std::copy(spatial_dims.begin(), spatial_dims.end(), dims.begin() + 2);\n  504  \n  505:     status = dynload::cudnnSetFilterNdDescriptor(parent_, handle_, elem_type,\n  506: #if CUDNN_VERSION >= 5000\n  507                                                   format,\n  508  #endif\n  509                                                   dims.size(), dims.data());\n  510:     if (status != CUDNN_STATUS_SUCCESS) {\n  511:       LOG(FATAL) << \"could not set cudnn filter descriptor: \"\n  512                   << ToString(status);\n  513      }\n  ...\n  515  \n  516    ~ScopedFilterDescriptor() {\n  517:     cudnnStatus_t status =\n  518:         dynload::cudnnDestroyFilterDescriptor(parent_, handle_);\n  519:     if (status != CUDNN_STATUS_SUCCESS) {\n  520:       LOG(ERROR) << \"could not destroy cudnn filter descriptor: \"\n  521                   << ToString(status);\n  522      }\n  523    }\n  524  \n  525:   cudnnFilterDescriptor_t handle() const { return handle_; }\n  526  \n  527   private:\n  ...\n  529    CUDAExecutor* parent_;\n  530  \n  531:   // cudnn filter descriptor this object creates. Owned.\n  532:   cudnnFilterDescriptor_t handle_;\n  533  \n  534    SE_DISALLOW_COPY_AND_ASSIGN(ScopedFilterDescriptor);\n  535  };\n  536  \n  537: // Turns a ConvolutionDescriptor structure into a cudnn convolution handle\n  538  // within a scope.\n  539  class ScopedConvolutionDescriptor {\n  ...\n  541    ScopedConvolutionDescriptor(\n  542        CUDAExecutor* parent, const ConvolutionDescriptor& convolution_descriptor,\n  543:       cudnnDataType_t data_type)\n  544        : parent_(parent), handle_(nullptr) {\n  545:     cudnnStatus_t status =\n  546:         dynload::cudnnCreateConvolutionDescriptor(parent_, &handle_);\n  547:     if (status != CUDNN_STATUS_SUCCESS) {\n  548:       LOG(FATAL) << \"could not create cudnn convolution descriptor: \"\n  549                   << ToString(status);\n  550      }\n  ...\n  552      const auto& padding64 = convolution_descriptor.padding();\n  553  \n  554:     // cuDNN requires arrays of ints.\n  555      std::vector<int> strides(convolution_descriptor.ndims());\n  556      std::vector<int> padding(convolution_descriptor.ndims());\n  ...\n  561      std::vector<int> upscale(convolution_descriptor.ndims(), 1);\n  562  \n  563:     status = dynload::cudnnSetConvolutionNdDescriptor(\n  564          parent_, handle_, convolution_descriptor.ndims(), padding.data(),\n  565          strides.data(), upscale.data(),\n  566:         // NOTE(keveman): cuDNN supports convolution and cross correlation.\n  567          // However, almost all the use cases do cross correlation, so just\n  568          // hard coding it here.\n  569:         CUDNN_CROSS_CORRELATION, data_type);\n  570  \n  571:     if (status != CUDNN_STATUS_SUCCESS) {\n  572:       LOG(FATAL) << \"could not set cudnn convolution descriptor: \"\n  573                   << ToString(status);\n  574      }\n  ...\n  576  \n  577    ~ScopedConvolutionDescriptor() {\n  578:     cudnnStatus_t status =\n  579:         dynload::cudnnDestroyConvolutionDescriptor(parent_, handle_);\n  580:     if (status != CUDNN_STATUS_SUCCESS) {\n  581:       LOG(ERROR) << \"could not destroy cudnn convolution descriptor: \"\n  582                   << ToString(status);\n  583      }\n  584    }\n  585  \n  586:   cudnnConvolutionDescriptor_t handle() const { return handle_; }\n  587  \n  588   private:\n  589    CUDAExecutor* parent_;                 // Parent executor. Not owned.\n  590:   cudnnConvolutionDescriptor_t handle_;  // Owned.\n  591  \n  592    SE_DISALLOW_COPY_AND_ASSIGN(ScopedConvolutionDescriptor);\n  593  };\n  594  \n  595: // Turns a PoolingDescriptor structure into a cudnn pooling descriptor handle\n  596  // within a scope.\n  597  class ScopedPoolingDescriptor {\n  ...\n  600                            const PoolingDescriptor& pooling_descriptor)\n  601        : parent_(parent), handle_(nullptr) {\n  602:     cudnnStatus_t status =\n  603:         dynload::cudnnCreatePoolingDescriptor(parent_, &handle_);\n  604:     if (status != CUDNN_STATUS_SUCCESS) {\n  605:       LOG(FATAL) << \"could not create cudnn pooling descriptor: \"\n  606                   << ToString(status);\n  607      }\n  ...\n  621      std::transform(shape64.cbegin(), shape64.cend(), shape.begin(),\n  622                     &CheckedNarrowing<int64, int>);\n  623:     status = dynload::cudnnSetPoolingNdDescriptor(\n  624          parent_, handle_,\n  625          (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum\n  626:              ? CUDNN_POOLING_MAX\n  627:              : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING),\n  628: #if CUDNN_VERSION >= 5000\n  629          // Always propagate nans.\n  630:         CUDNN_PROPAGATE_NAN,\n  631  #endif\n  632          nd, shape.data(), padding.data(), strides.data());\n  633:     if (status != CUDNN_STATUS_SUCCESS) {\n  634:       LOG(FATAL) << \"could not set cudnn pooling descriptor: \"\n  635                   << ToString(status);\n  636      }\n  637    }\n  638    ~ScopedPoolingDescriptor() {\n  639:     cudnnStatus_t status =\n  640:         dynload::cudnnDestroyPoolingDescriptor(parent_, handle_);\n  641:     if (status != CUDNN_STATUS_SUCCESS) {\n  642:       LOG(ERROR) << \"could not destroy cudnn pooling descriptor: \"\n  643                   << ToString(status);\n  644      }\n  645    }\n  646  \n  647:   cudnnPoolingDescriptor_t handle() const { return handle_; }\n  648  \n  649   private:\n  650    CUDAExecutor* parent_;             // Parent executor. Not owned.\n  651:   cudnnPoolingDescriptor_t handle_;  // Owned.\n  652  \n  653    SE_DISALLOW_COPY_AND_ASSIGN(ScopedPoolingDescriptor);\n  654  };\n  655  \n  656: // Turns a NormalizeDescriptor structure into a cudnn LRN descriptor handle.\n  657  class ScopedNormalizeDescriptor {\n  658   public:\n  ...\n  660                              const NormalizeDescriptor& normalize_descriptor)\n  661        : parent_(parent), handle_(nullptr) {\n  662:     cudnnStatus_t status = dynload::cudnnCreateLRNDescriptor(parent_, &handle_);\n  663:     if (status != CUDNN_STATUS_SUCCESS) {\n  664:       LOG(FATAL) << \"could not create cudnn LRN descriptor: \"\n  665                   << ToString(status);\n  666      }\n  ...\n  676      //  U_i = V_i / ((bias +  alpha      * (sum_j V_j^2)) ^ beta)\n  677      //\n  678:     // but cuDNN defines it as\n  679      //\n  680      //  U_i = V_i / ((bias + (alpha / n) * (sum_j V_j^2)) ^ beta)\n  681      //\n  682      // i.e. there is a factor of n difference between the meaning of the alphas\n  683:     // in the two contexts. The cuDNN alpha is n times the SE alpha.\n  684      double lrnAlpha = lrnN * normalize_descriptor.alpha();\n  685  \n  686      double lrnBeta = normalize_descriptor.beta();\n  687      double lrnK = normalize_descriptor.bias();\n  688:     status = dynload::cudnnSetLRNDescriptor(parent_, handle_, lrnN, lrnAlpha,\n  689                                              lrnBeta, lrnK);\n  690:     if (status != CUDNN_STATUS_SUCCESS) {\n  691:       LOG(FATAL) << \"could not set cudnn LRN descriptor: \" << ToString(status);\n  692      }\n  693    }\n  694  \n  695    ~ScopedNormalizeDescriptor() {\n  696:     cudnnStatus_t status = dynload::cudnnDestroyLRNDescriptor(parent_, handle_);\n  697:     if (status != CUDNN_STATUS_SUCCESS) {\n  698:       LOG(ERROR) << \"could not destroy cudnn LRN descriptor: \"\n  699                   << ToString(status);\n  700      }\n  701    }\n  702  \n  703:   cudnnLRNDescriptor_t handle() const { return handle_; }\n  704  \n  705   private:\n  706    CUDAExecutor* parent_;         // Parent executor. Not owned.\n  707:   cudnnLRNDescriptor_t handle_;  // Owned.\n  708  \n  709    SE_DISALLOW_COPY_AND_ASSIGN(ScopedNormalizeDescriptor);\n  710  };\n  711  \n  712: #if CUDNN_VERSION >= 5000\n  713: // Turns a ActivationDescriptor structure into a cudnn activation\n  714  // descriptor handle within a scope.\n  715  class ScopedActivationDescriptor {\n  ...\n  719                               double value_max)\n  720        : parent_(parent), handle_(nullptr) {\n  721:     cudnnStatus_t status =\n  722:         dynload::cudnnCreateActivationDescriptor(parent_, &handle_);\n  723:     if (status != CUDNN_STATUS_SUCCESS) {\n  724:       LOG(FATAL) << \"could not create cudnn activation descriptor: \"\n  725                   << ToString(status);\n  726      }\n  727  \n  728      double relu_ceiling = 0.0;\n  729:     cudnnActivationMode_t mode;\n  730      switch (activation_mode) {\n  731        case dnn::ActivationMode::kRelu6:\n  732          relu_ceiling = 6.0;\n  733:         mode = CUDNN_ACTIVATION_CLIPPED_RELU;\n  734          break;\n  735        case dnn::ActivationMode::kReluX:\n  736          relu_ceiling = value_max;\n  737:         mode = CUDNN_ACTIVATION_CLIPPED_RELU;\n  738          break;\n  739        case dnn::ActivationMode::kRelu:\n  740:         mode = CUDNN_ACTIVATION_RELU;\n  741          break;\n  742        case dnn::ActivationMode::kSigmoid:\n  743:         mode = CUDNN_ACTIVATION_SIGMOID;\n  744          break;\n  745        case dnn::ActivationMode::kTanh:\n  746:         mode = CUDNN_ACTIVATION_TANH;\n  747          break;\n  748        default:\n  ...\n  752  \n  753      // Always propagate nans.\n  754:     cudnnNanPropagation_t nan_propagation = CUDNN_PROPAGATE_NAN;\n  755:     status = dynload::cudnnSetActivationDescriptor(\n  756          parent_, handle_,\n  757          mode, nan_propagation, relu_ceiling);\n  758:     if (status != CUDNN_STATUS_SUCCESS) {\n  759:       LOG(FATAL) << \"could not set cudnn activation descriptor: \"\n  760                   << ToString(status);\n  761      }\n  ...\n  763  \n  764    ~ScopedActivationDescriptor() {\n  765:     cudnnStatus_t status =\n  766:         dynload::cudnnDestroyActivationDescriptor(parent_, handle_);\n  767:     if (status != CUDNN_STATUS_SUCCESS) {\n  768:       LOG(ERROR) << \"could not destroy cudnn activation descriptor: \"\n  769                   << ToString(status);\n  770      }\n  771    }\n  772  \n  773:   cudnnActivationDescriptor_t handle() const { return handle_; }\n  774  \n  775   private:\n  776    CUDAExecutor* parent_;                // Parent executor. Not owned.\n  777:   cudnnActivationDescriptor_t handle_;  // Owned.\n  778  \n  779    SE_DISALLOW_COPY_AND_ASSIGN(ScopedActivationDescriptor);\n  ...\n  782  \n  783  namespace {\n  784: cudnnDataType_t ToCudnnDataType(dnn::DataType data_type) {\n  785    switch (data_type) {\n  786      case dnn::DataType::kFloat:\n  787      case dnn::DataType::kDouble:\n  788      case dnn::DataType::kHalf:\n  789:       return static_cast<cudnnDataType_t>(data_type);\n  790      default:\n  791        LOG(FATAL) << \"Invalid DNN data type: \" << static_cast<int>(data_type);\n  ...\n  793  }\n  794  \n  795: #if CUDNN_VERSION >= 5000\n  796  \n  797: cudnnRNNInputMode_t ToCudnnRnnInputMode(dnn::RnnInputMode input_mode) {\n  798    switch (input_mode) {\n  799      case dnn::RnnInputMode::kRnnLinearSkip:\n  800      case dnn::RnnInputMode::kRnnSkipInput:\n  801:       return static_cast<cudnnRNNInputMode_t>(input_mode);\n  802      default:\n  803        LOG(FATAL) << \"Invalid RNN input mode: \" << static_cast<int>(input_mode);\n  ...\n  805  }\n  806  \n  807: cudnnDirectionMode_t ToCudnnRnnDirectionMode(\n  808      dnn::RnnDirectionMode direction_mode) {\n  809    switch (direction_mode) {\n  810      case dnn::RnnDirectionMode::kRnnUnidirectional:\n  811      case dnn::RnnDirectionMode::kRnnBidirectional:\n  812:       return static_cast<cudnnDirectionMode_t>(direction_mode);\n  813      default:\n  814        LOG(FATAL) << \"Invalid RNN direction mode: \"\n  ...\n  817  }\n  818  \n  819: cudnnRNNMode_t ToCudnnRnnMode(dnn::RnnMode rnn_mode) {\n  820    switch (rnn_mode) {\n  821      case dnn::RnnMode::kRnnRelu:\n  ...\n  823      case dnn::RnnMode::kRnnLstm:\n  824      case dnn::RnnMode::kRnnGru:\n  825:       return static_cast<cudnnRNNMode_t>(rnn_mode);\n  826      default:\n  827        LOG(FATAL) << \"Invalid RNN Mode: \" << static_cast<int>(rnn_mode);\n  ...\n  829  }\n  830  \n  831: int CudnnDataTypeToByteSize(cudnnDataType_t data_type) {\n  832    switch (data_type) {\n  833:     case CUDNN_DATA_FLOAT:\n  834        return sizeof(float);\n  835:     case CUDNN_DATA_DOUBLE:\n  836        return sizeof(double);\n  837:     case CUDNN_DATA_HALF:\n  838        return sizeof(Eigen::half);\n  839      default:\n  ...\n  842  }\n  843  \n  844: #endif  // CUDNN_VERSION\n  845  \n  846  template <typename Base>\n  ...\n  851  }  // namespace\n  852  \n  853: #if CUDNN_VERSION >= 5000\n  854  \n  855: #define CUDNN_RETURN_IF_FAIL(STATUS, ...)                                \\\n  856:   if (!SE_PREDICT_TRUE((STATUS) == CUDNN_STATUS_SUCCESS)) {              \\\n  857      string error_msg = port::StrCat(ToString(STATUS), \" \", __VA_ARGS__); \\\n  858      SetFailure(port::Status(port::error::UNKNOWN, error_msg));           \\\n  ...\n  862  \n  863  template <typename Base>\n  864: class CudnnDescriptorCommon : public MixinBase<Base> {\n  865   public:\n  866    bool ok() const { return status_.ok(); }\n  ...\n  872  };\n  873  \n  874: class CudnnDropoutDescriptor : public CudnnDescriptorCommon<void> {\n  875   public:\n  876:   CudnnDropoutDescriptor(CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\n  877                           float dropout, uint64 seed,\n  878                           ScratchAllocator* state_allocator)\n  879        : parent_(parent), handle_(nullptr) {\n  880:     cudnnStatus_t status;\n  881:     status = dynload::cudnnCreateDropoutDescriptor(parent_, &handle_);\n  882:     CUDNN_RETURN_IF_FAIL(status, \"Failed to create dropout descriptor\");\n  883  \n  884      if (dropout == 0.f) {\n  ...\n  889      if (state_allocator) {\n  890        size_t state_sizes_in_bytes = 0;\n  891:       status = dynload::cudnnDropoutGetStatesSize(parent_, cudnn_handle,\n  892                                                    &state_sizes_in_bytes);\n  893:       CUDNN_RETURN_IF_FAIL(status, \"Failed to query dropout state sizes\");\n  894  \n  895        auto allocated =\n  ...\n  898            (state_memory = allocated.ValueOrDie()) == nullptr) {\n  899          string error_msg =\n  900:             port::StrCat(\"Fail to allocate Cudnn dropout state memory\");\n  901          status_ = port::Status(port::error::UNKNOWN, error_msg);\n  902          LOG(ERROR) << error_msg;\n  ...\n  904        }\n  905      }\n  906:     status = dynload::cudnnSetDropoutDescriptor(parent_, handle_, cudnn_handle,\n  907                                                  dropout, state_memory.opaque(),\n  908                                                  state_memory.size(), seed);\n  909:     CUDNN_RETURN_IF_FAIL(status, \"Failed to set dropout descriptor\");\n  910    }\n  911  \n  912:   ~CudnnDropoutDescriptor() {\n  913      if (handle_) {\n  914:       cudnnStatus_t status =\n  915:           dynload::cudnnDestroyDropoutDescriptor(parent_, handle_);\n  916:       CUDNN_RETURN_IF_FAIL(status, \"Failed to destroy Cudnn dropout handle: \");\n  917      }\n  918    }\n  919  \n  920:   cudnnDropoutDescriptor_t handle() const {\n  921      if (!ok()) return nullptr;\n  922      return handle_;\n  ...\n  925   private:\n  926    CUDAExecutor* parent_;\n  927:   cudnnDropoutDescriptor_t handle_;\n  928    float dropout_;\n  929    uint64 seed_;\n  930    port::Status status_;\n  931:   SE_DISALLOW_COPY_AND_ASSIGN(CudnnDropoutDescriptor);\n  932  };\n  933  \n  934: class CudnnRnnParamsDescriptor : public CudnnDescriptorCommon<void> {\n  935   public:\n  936    typedef dnn::RnnDescriptor::ParamsRegion ParamsRegion;\n  937    typedef dnn::RnnDescriptor::ParamsRegions ParamsRegions;\n  938:   CudnnRnnParamsDescriptor(CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\n  939:                            const CudnnRnnDescriptor& rnn_desc);\n  940:   ~CudnnRnnParamsDescriptor() {\n  941:     cudnnStatus_t status =\n  942:         dynload::cudnnDestroyFilterDescriptor(parent_, handle_);\n  943:     CUDNN_RETURN_IF_FAIL(status, \"Failed to destroy RNN filter desciptor\");\n  944    }\n  945:   cudnnFilterDescriptor_t handle() const {\n  946      if (!ok()) return nullptr;\n  947      return handle_;\n  ...\n  960    int GetRegionCountPerLayer() const;\n  961    CUDAExecutor* parent_;\n  962:   cudnnFilterDescriptor_t handle_;\n  963:   const CudnnRnnDescriptor* rnn_desc_;\n  964    int64 params_size_in_bytes_;\n  965    ParamsRegions weights_;\n  966    ParamsRegions biases_;\n  967    port::Status status_;\n  968:   SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnParamsDescriptor);\n  969  };\n  970  \n  971: class CudnnRnnDescriptor : public CudnnDescriptorCommon<dnn::RnnDescriptor> {\n  972   public:\n  973:   CudnnRnnDescriptor(CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\n  974                       int num_layers, int hidden_size, int input_size,\n  975:                      cudnnRNNInputMode_t input_mode,\n  976:                      cudnnDirectionMode_t direction_mode,\n  977:                      cudnnRNNMode_t rnn_mode, cudnnDataType_t data_type,\n  978                       float dropout, uint64 seed,\n  979                       ScratchAllocator* state_allocator)\n  ...\n  988          data_type_(data_type) {\n  989      // Create the dropout handle.\n  990:     cudnn_dropout_desc_.reset(new CudnnDropoutDescriptor(\n  991:         parent, cudnn_handle, dropout, seed, state_allocator));\n  992:     if (!cudnn_dropout_desc_->ok()) {\n  993:       SetFailure(cudnn_dropout_desc_->Status());\n  994        return;\n  995      }\n  996  \n  997      // Create the RNN handle\n  998:     cudnnStatus_t status =\n  999:         dynload::cudnnCreateRNNDescriptor(parent_, &rnn_desc_);\n 1000:     CUDNN_RETURN_IF_FAIL(status, \"Unable to create RNN descriptor\");\n 1001:     status = dynload::cudnnSetRNNDescriptor(\n 1002          parent, rnn_desc_ /*rnnDesc*/, hidden_size /*hiddenSize*/,\n 1003          num_layers /*numLayers*/, dropout_handle() /*dropoutDesc*/,\n 1004          input_mode /*inputMode*/, direction_mode /*direction*/,\n 1005          rnn_mode /*mode*/, data_type /*dataType*/);\n 1006:     CUDNN_RETURN_IF_FAIL(status, \"Unable to update RNN descriptor\");\n 1007  \n 1008      // Create the params handle.\n 1009:     cudnn_params_desc_.reset(\n 1010:         new CudnnRnnParamsDescriptor(parent, cudnn_handle, *this));\n 1011:     if (!cudnn_params_desc_->ok()) {\n 1012:       SetFailure(cudnn_params_desc_->Status());\n 1013        return;\n 1014      }\n 1015    }\n 1016:   ~CudnnRnnDescriptor() override {\n 1017      if (rnn_desc_) {\n 1018:       cudnnStatus_t status =\n 1019:           dynload::cudnnDestroyRNNDescriptor(parent_, rnn_desc_);\n 1020:       CUDNN_RETURN_IF_FAIL(status, \"Unable to destroy RNN descriptor\");\n 1021      }\n 1022    }\n 1023:   cudnnRNNDescriptor_t handle() const {\n 1024      if (!ok()) return nullptr;\n 1025      return rnn_desc_;\n ....\n 1028    int hidden_size() const { return hidden_size_; }\n 1029    int input_size() const { return input_size_; }\n 1030:   cudnnRNNInputMode_t input_mode() const { return input_mode_; }\n 1031:   cudnnDirectionMode_t direction_mode() const { return direction_mode_; }\n 1032:   cudnnRNNMode_t rnn_mode() const { return rnn_mode_; }\n 1033:   cudnnDataType_t data_type() const { return data_type_; }\n 1034    int64 ParamsSizeInBytes() const override {\n 1035:     return cudnn_params_desc_->params_size_in_bytes();\n 1036    }\n 1037:   cudnnDropoutDescriptor_t dropout_handle() const {\n 1038:     if (!cudnn_dropout_desc_) return nullptr;\n 1039:     return cudnn_dropout_desc_->handle();\n 1040    }\n 1041:   cudnnFilterDescriptor_t params_handle() const {\n 1042:     if (!cudnn_params_desc_) return nullptr;\n 1043:     return cudnn_params_desc_->handle();\n 1044    }\n 1045    ParamsRegions ParamsWeightRegions() const override {\n 1046      if (!ok()) return ParamsRegions();\n 1047:     return cudnn_params_desc_->params_weights();\n 1048    }\n 1049    ParamsRegions ParamsBiasRegions() const override {\n 1050      if (!ok()) return ParamsRegions();\n 1051:     return cudnn_params_desc_->params_biases();\n 1052    }\n 1053  \n 1054   private:\n 1055    CUDAExecutor* parent_;\n 1056:   cudnnRNNDescriptor_t rnn_desc_;\n 1057    int num_layers_;\n 1058    int hidden_size_;\n 1059    int input_size_;\n 1060:   cudnnRNNInputMode_t input_mode_;\n 1061:   cudnnDirectionMode_t direction_mode_;\n 1062:   cudnnRNNMode_t rnn_mode_;\n 1063:   cudnnDataType_t data_type_;\n 1064    port::Status status_;\n 1065:   std::unique_ptr<CudnnDropoutDescriptor> cudnn_dropout_desc_;\n 1066:   std::unique_ptr<CudnnRnnParamsDescriptor> cudnn_params_desc_;\n 1067:   SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnDescriptor);\n 1068  };\n 1069  \n 1070: CudnnRnnParamsDescriptor::CudnnRnnParamsDescriptor(\n 1071:     CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\n 1072:     const CudnnRnnDescriptor& rnn_desc)\n 1073      : parent_(parent),\n 1074        handle_(nullptr),\n 1075        rnn_desc_(&rnn_desc),\n 1076        params_size_in_bytes_(0) {\n 1077:   cudnnTensorDescriptor_t input_desc = nullptr;\n 1078    {\n 1079      // Query the params size.\n 1080:     auto status = dynload::cudnnCreateTensorDescriptor(parent, &input_desc);\n 1081:     CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to create tensor descriptor\");\n 1082      int dims[] = {1, rnn_desc.input_size(), 1};\n 1083      int strides[] = {dims[1] * dims[2], dims[2], 1};\n 1084:     status = dynload::cudnnSetTensorNdDescriptor(\n 1085          parent, input_desc /*tensorDesc*/, rnn_desc.data_type() /*dataType*/,\n 1086          sizeof(dims) / sizeof(dims[0]) /*nbDims*/, dims /*dimA*/,\n 1087          strides /*strideA*/);\n 1088:     CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to set tensor descriptor\");\n 1089  \n 1090      size_t params_size = 0;\n 1091:     status = dynload::cudnnGetRNNParamsSize(\n 1092:         parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n 1093          input_desc /*xDesc*/, &params_size /*sizeInBytes*/,\n 1094          rnn_desc.data_type() /*dataType*/);\n 1095:     CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to get RNN parameter size\");\n 1096      params_size_in_bytes_ = static_cast<int64>(params_size);\n 1097    }\n ....\n 1099    {\n 1100      // Create the params descriptor.\n 1101:     auto status = dynload::cudnnCreateFilterDescriptor(parent, &handle_);\n 1102:     CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to create RNN filter descriptor\");\n 1103      int dims[] = {static_cast<int>(params_size_in_bytes_), 1, 1};\n 1104:     status = dynload::cudnnSetFilterNdDescriptor(\n 1105          parent, handle_ /*filterDesc*/, rnn_desc.data_type() /*dataType*/,\n 1106:         CUDNN_TENSOR_NCHW /*format*/, sizeof(dims) / sizeof(dims[0]) /*nbDims*/,\n 1107          dims /*filterDimA*/);\n 1108:     CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to update RNN filter descriptor\");\n 1109    }\n 1110  \n ....\n 1112      // Create the weights and biases into the params buffer\n 1113      int region_count_per_layer = GetRegionCountPerLayer();\n 1114:     cudnnFilterDescriptor_t region_desc_handle = nullptr;\n 1115      auto status =\n 1116:         dynload::cudnnCreateFilterDescriptor(parent, &region_desc_handle);\n 1117:     CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to create filter descriptor\");\n 1118      for (int layer = 0; layer < rnn_desc.num_layers(); layer++) {\n 1119        for (int region = 0; region < region_count_per_layer; region++) {\n ....\n 1121            void* offset = nullptr;\n 1122            if (type == 0) {\n 1123:             status = dynload::cudnnGetRNNLinLayerMatrixParams(\n 1124:                 parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n 1125                  layer /*layer*/, input_desc /*xDesc*/, handle_ /*wDesc*/,\n 1126                  nullptr /*w*/, region /*linLayerID*/,\n 1127                  region_desc_handle /*linLayerMatDesc*/,\n 1128                  &offset /*linLayerMat*/);\n 1129:             CUDNN_RETURN_IF_FAIL(\n 1130:                 status, \"Cudnn fails to call cudnnGetRNNLinLayerMatrixParams\");\n 1131            } else {\n 1132:             status = dynload::cudnnGetRNNLinLayerBiasParams(\n 1133:                 parent, cudnn_handle /*rnnDesc*/, rnn_desc.handle() /*rnnDesc*/,\n 1134                  layer /*layer*/, input_desc /*xDesc*/, handle_ /*wDesc*/,\n 1135                  nullptr /*w*/, region /*linLayerID*/,\n 1136                  region_desc_handle /*linLayerBiasDesc*/,\n 1137                  &offset /*linLayerBias*/);\n 1138:             CUDNN_RETURN_IF_FAIL(\n 1139:                 status, \"Cudnn fails to call cudnnGetRNNLinLayerBiasParams\");\n 1140            }\n 1141            int dims[] = {1, 1, 1};\n 1142:           cudnnDataType_t data_type;\n 1143:           cudnnTensorFormat_t tensor_format;\n 1144            int n_dims;\n 1145:           status = dynload::cudnnGetFilterNdDescriptor(\n 1146                parent, region_desc_handle /*filterDesc*/,\n 1147                sizeof(dims) / sizeof(dims[0]) /*nbDimsRequested*/,\n 1148                &data_type /*dataType*/, &tensor_format /*format*/,\n 1149                &n_dims /*nbDims*/, dims /*filterDimA*/);\n 1150:           CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to get filter description\");\n 1151            int64 size = dims[0] * dims[1] * dims[2] *\n 1152:                        CudnnDataTypeToByteSize(rnn_desc.data_type());\n 1153            auto region = ParamsRegion{reinterpret_cast<int64>(offset), size};\n 1154            if (type == 0) {\n ....\n 1160        }\n 1161      }\n 1162:     status = dynload::cudnnDestroyFilterDescriptor(parent, region_desc_handle);\n 1163:     CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to destroy filter descriptor\");\n 1164    }\n 1165  \n 1166    {\n 1167      // Release the dummy input tensor descriptor.\n 1168:     auto status = dynload::cudnnDestroyTensorDescriptor(parent, input_desc);\n 1169:     CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to destroy tensor descriptor\");\n 1170    }\n 1171  }\n 1172  \n 1173: int CudnnRnnParamsDescriptor::GetRegionCountPerLayer() const {\n 1174    auto rnn_mode = rnn_desc_->rnn_mode();\n 1175    switch (rnn_mode) {\n 1176:     case CUDNN_RNN_RELU:\n 1177:     case CUDNN_RNN_TANH:\n 1178        return 2;\n 1179:     case CUDNN_LSTM:\n 1180        return 8;\n 1181:     case CUDNN_GRU:\n 1182        return 6;\n 1183      default:\n ....\n 1186  }\n 1187  \n 1188: class CudnnRnnSequenceTensorDescriptor\n 1189:     : public CudnnDescriptorCommon<dnn::RnnSequenceTensorDescriptor> {\n 1190   public:\n 1191:   CudnnRnnSequenceTensorDescriptor(CUDAExecutor* parent, int seq_length,\n 1192                                     int batch_size, int data_size,\n 1193:                                    cudnnDataType_t data_type)\n 1194        : parent_(parent),\n 1195          seq_length_(seq_length),\n ....\n 1197          data_size_(data_size),\n 1198          data_type_(data_type) {\n 1199:     cudnnTensorDescriptor_t handle = nullptr;\n 1200      if (seq_length <= 0) {\n 1201        string error_msg =\n ....\n 1205        return;\n 1206      }\n 1207:     cudnnStatus_t status =\n 1208:         dynload::cudnnCreateTensorDescriptor(parent, &handle);\n 1209:     CUDNN_RETURN_IF_FAIL(status, \"Failed to create tensor descriptor\");\n 1210      int dims[] = {batch_size, data_size, 1};\n 1211      int strides[] = {dims[1] * dims[2], dims[2], 1};\n 1212:     status = dynload::cudnnSetTensorNdDescriptor(\n 1213          parent, handle /*tensorDesc*/, data_type /*dataType*/,\n 1214          sizeof(dims) / sizeof(dims[0]) /*nbDims*/, dims /*dimA*/,\n 1215          strides /*strideA*/);\n 1216:     CUDNN_RETURN_IF_FAIL(status, \"Failed to update tensor descriptor\");\n 1217      // Replicate handle across the number of steps.\n 1218      handles_.assign(seq_length, handle);\n 1219    }\n 1220  \n 1221:   ~CudnnRnnSequenceTensorDescriptor() override {\n 1222      // Only the first one needs to be destroyed. All others are the same.\n 1223:     cudnnStatus_t status =\n 1224:         dynload::cudnnDestroyTensorDescriptor(parent_, handles_[0]);\n 1225:     CUDNN_RETURN_IF_FAIL(status, \"Failed to destroy sequence tensor desciptor\");\n 1226    }\n 1227  \n 1228:   const cudnnTensorDescriptor_t* handles() const {\n 1229      if (!ok()) return nullptr;\n 1230      CHECK(!handles_.empty()) << \"handles cannot be empty\";\n ....\n 1241    int batch_size_;\n 1242    int data_size_;\n 1243:   cudnnDataType_t data_type_;\n 1244:   std::vector<cudnnTensorDescriptor_t> handles_;\n 1245    port::Status status_;\n 1246:   SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnSequenceTensorDescriptor);\n 1247  };\n 1248  \n 1249: class CudnnRnnStateTensorDescriptor\n 1250:     : public CudnnDescriptorCommon<dnn::RnnStateTensorDescriptor> {\n 1251   public:\n 1252:   CudnnRnnStateTensorDescriptor(CUDAExecutor* parent, int num_layers,\n 1253                                  int batch_size, int data_size,\n 1254:                                 cudnnDataType_t data_type)\n 1255        : parent_(parent),\n 1256          handle_(nullptr),\n ....\n 1259          data_size_(data_size),\n 1260          data_type_(data_type) {\n 1261:     cudnnStatus_t status =\n 1262:         dynload::cudnnCreateTensorDescriptor(parent, &handle_);\n 1263:     CUDNN_RETURN_IF_FAIL(status, \"Failed to create tensor descriptor\");\n 1264      int dims[] = {num_layers, batch_size, data_size};\n 1265      int strides[] = {dims[1] * dims[2], dims[2], 1};\n 1266:     status = dynload::cudnnSetTensorNdDescriptor(\n 1267          parent, handle_ /*tensorDesc*/, data_type /*dataType*/,\n 1268          sizeof(dims) / sizeof(dims[0]) /*nbDims*/, dims /*dimA*/,\n 1269          strides /*strideA*/);\n 1270:     CUDNN_RETURN_IF_FAIL(status, \"Failed to update tensor descriptor\");\n 1271    }\n 1272  \n 1273:   ~CudnnRnnStateTensorDescriptor() override {\n 1274      if (!handle_) {\n 1275:       cudnnStatus_t status =\n 1276:           dynload::cudnnDestroyTensorDescriptor(parent_, handle_);\n 1277:       CUDNN_RETURN_IF_FAIL(status, \"Unable to destroy RNN state tensor\");\n 1278      }\n 1279    }\n 1280  \n 1281:   cudnnTensorDescriptor_t handle() const {\n 1282      if (!ok()) return nullptr;\n 1283      return handle_;\n ....\n 1289   private:\n 1290    CUDAExecutor* parent_;\n 1291:   cudnnTensorDescriptor_t handle_;\n 1292    int num_layers_;\n 1293    int batch_size_;\n 1294    int data_size_;\n 1295    port::Status status_;\n 1296:   cudnnDataType_t data_type_;\n 1297:   SE_DISALLOW_COPY_AND_ASSIGN(CudnnRnnStateTensorDescriptor);\n 1298  };\n 1299  \n ....\n 1311  template <class T>\n 1312  bool ExtractAndCheckRnnForward(\n 1313:     const CudnnRnnDescriptor& rnn_desc,\n 1314:     const CudnnRnnSequenceTensorDescriptor& input_desc,\n 1315      const DeviceMemory<T>& input_data,\n 1316:     const CudnnRnnStateTensorDescriptor& input_h_desc,\n 1317      const DeviceMemory<T>& input_h_data,\n 1318:     const CudnnRnnStateTensorDescriptor& input_c_desc,\n 1319      const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n 1320:     const CudnnRnnSequenceTensorDescriptor& output_desc,\n 1321      const DeviceMemory<T>& output_data,\n 1322:     const CudnnRnnStateTensorDescriptor& output_h_desc,\n 1323      const DeviceMemory<T>& output_h_data,\n 1324:     const CudnnRnnStateTensorDescriptor& output_c_desc,\n 1325      const DeviceMemory<T>& output_c_data, RnnModelDims* model_dims) {\n 1326    // extract model parameters\n ....\n 1331    model_dims->input_size = input_desc.data_size();\n 1332    model_dims->dir_count =\n 1333:       (rnn_desc.direction_mode() == CUDNN_BIDIRECTIONAL) ? 2 : 1;\n 1334  \n 1335    // check parameters\n ....\n 1370  }\n 1371  \n 1372: bool CheckRNNParameterSize(CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\n 1373:                            const CudnnRnnDescriptor& rnn_desc,\n 1374:                            const CudnnRnnSequenceTensorDescriptor& input_desc) {\n 1375    size_t params_size_in_bytes = 0;\n 1376:   cudnnStatus_t status = dynload::cudnnGetRNNParamsSize(\n 1377:       parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n 1378        input_desc.handles()[0] /*xDesc*/, &params_size_in_bytes /*sizeInBytes*/,\n 1379        rnn_desc.data_type() /*dataType*/);\n 1380:   if (status != CUDNN_STATUS_SUCCESS) {\n 1381      LOG(ERROR) << \"Unable to check RNN param size: \" << ToString(status);\n 1382      return false;\n ....\n 1387  \n 1388  bool CreateRnnWorkspace(Stream* stream, CUDAExecutor* parent,\n 1389:                         cudnnHandle_t cudnn_handle,\n 1390:                         const CudnnRnnDescriptor& rnn_desc,\n 1391:                         const CudnnRnnSequenceTensorDescriptor& input_desc,\n 1392                          ScratchAllocator* workspace_allocator,\n 1393                          DeviceMemory<uint8>* workspace) {\n 1394    // Query the workspace size.\n 1395    size_t workspace_size_in_bytes = 0;\n 1396:   cudnnStatus_t status = dynload::cudnnGetRNNWorkspaceSize(\n 1397:       parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n 1398        input_desc.seq_length() /*seqLength*/, input_desc.handles() /*xDesc*/,\n 1399        &workspace_size_in_bytes /*sizeInBytes*/);\n 1400:   if (status != CUDNN_STATUS_SUCCESS) {\n 1401      LOG(ERROR) << \"Unable to query workspace size: \" << ToString(status);\n 1402      return false;\n ....\n 1419  \n 1420  template <class T>\n 1421: bool CudnnSupport::DoRnnForwardImpl(\n 1422:     Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n 1423:     const CudnnRnnSequenceTensorDescriptor& input_desc,\n 1424      const DeviceMemory<T>& input_data,\n 1425:     const CudnnRnnStateTensorDescriptor& input_h_desc,\n 1426      const DeviceMemory<T>& input_h_data,\n 1427:     const CudnnRnnStateTensorDescriptor& input_c_desc,\n 1428      const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n 1429:     const CudnnRnnSequenceTensorDescriptor& output_desc,\n 1430      DeviceMemory<T>* output_data,\n 1431:     const CudnnRnnStateTensorDescriptor& output_h_desc,\n 1432      DeviceMemory<T>* output_h_data,\n 1433:     const CudnnRnnStateTensorDescriptor& output_c_desc,\n 1434      DeviceMemory<T>* output_c_data, bool is_training,\n 1435      ScratchAllocator* reserve_space_allocator,\n ....\n 1469    if (is_training) {\n 1470      size_t reserve_space_size_in_bytes = 0;\n 1471:     cudnnStatus_t status = dynload::cudnnGetRNNTrainingReserveSize(\n 1472          parent_, ToHandle(dnn_handle_) /*handle*/,\n 1473          rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n 1474          input_desc.handles() /*xDesc*/,\n 1475          &reserve_space_size_in_bytes /*sizeInBytes*/);\n 1476:     if (status != CUDNN_STATUS_SUCCESS) {\n 1477        LOG(ERROR) << \"Unable to query reserve space size: \" << ToString(status);\n 1478        return false;\n ....\n 1492    // make the forward call\n 1493    if (!is_training) {\n 1494:     cudnnStatus_t status = dynload::cudnnRNNForwardInference(\n 1495          parent_, ToHandle(dnn_handle_) /*handle*/,\n 1496          rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n ....\n 1504          workspace.opaque() /*workspace*/,\n 1505          workspace.size() /*workSpaceSizeInBytes*/);\n 1506:     if (status != CUDNN_STATUS_SUCCESS) {\n 1507:       LOG(ERROR) << \"Failed to call cudnnRNNForwardInference: \"\n 1508                   << ToString(status);\n 1509        return false;\n 1510      }\n 1511    } else {\n 1512:     cudnnStatus_t status = dynload::cudnnRNNForwardTraining(\n 1513          parent_, ToHandle(dnn_handle_) /*handle*/,\n 1514          rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n ....\n 1524          reserve_space.opaque() /*reserveSpace*/,\n 1525          reserve_space.size() /*reserveSpaceSizeInBytes*/);\n 1526:     if (status != CUDNN_STATUS_SUCCESS) {\n 1527:       LOG(ERROR) << \"Failed to call cudnnRNNForwardTraining\"\n 1528                   << ToString(status);\n 1529        return false;\n ....\n 1535  \n 1536  template <class T>\n 1537: bool CudnnSupport::DoRnnBackwardImpl(\n 1538:     Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n 1539:     const CudnnRnnSequenceTensorDescriptor& input_desc,\n 1540      const DeviceMemory<T>& input_data,\n 1541:     const CudnnRnnStateTensorDescriptor& input_h_desc,\n 1542      const DeviceMemory<T>& input_h_data,\n 1543:     const CudnnRnnStateTensorDescriptor& input_c_desc,\n 1544      const DeviceMemory<T>& input_c_data, const DeviceMemory<T>& params,\n 1545:     const CudnnRnnSequenceTensorDescriptor& output_desc,\n 1546      const DeviceMemory<T>& output_data,\n 1547:     const CudnnRnnStateTensorDescriptor& output_h_desc,\n 1548      const DeviceMemory<T>& output_h_data,\n 1549:     const CudnnRnnStateTensorDescriptor& output_c_desc,\n 1550      const DeviceMemory<T>& output_c_data,\n 1551      const DeviceMemory<float>& output_backprop_data,\n ....\n 1587  \n 1588    // make the backward data call\n 1589:   cudnnStatus_t status = dynload::cudnnRNNBackwardData(\n 1590        parent_, ToHandle(dnn_handle_) /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n 1591        model_dims.seq_length /*seqLength*/, output_desc.handles() /*yDesc*/,\n ....\n 1606        reserve_space_data->opaque() /*reserveSpace*/,\n 1607        reserve_space_data->size() /*reserveSpaceSizeInBytes*/);\n 1608:   if (status != CUDNN_STATUS_SUCCESS) {\n 1609:     LOG(ERROR) << \"Failed to call cudnnRNNBackwardData: \" << ToString(status);\n 1610      return false;\n 1611    }\n ....\n 1615      stream->ThenMemZero(params_backprop_data, params_backprop_data->size());\n 1616      // make the backward weight call\n 1617:     status = dynload::cudnnRNNBackwardWeights(\n 1618          parent_, ToHandle(dnn_handle_) /*handle*/,\n 1619          rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n ....\n 1627          reserve_space_data->opaque() /*reserveSpace*/,\n 1628          reserve_space_data->size() /*reserveSpaceSizeInBytes*/);\n 1629:     if (status != CUDNN_STATUS_SUCCESS) {\n 1630:       LOG(ERROR) << \"Failed to call cudnnRNNBackwardWeights: \"\n 1631                   << ToString(status);\n 1632        return false;\n ....\n 1637  }\n 1638  \n 1639: #endif  // CUDNN_VERSION\n 1640  \n 1641  port::StatusOr<std::unique_ptr<dnn::RnnDescriptor>>\n 1642: CudnnSupport::createRnnDescriptor(int num_layers, int hidden_size,\n 1643                                    int input_size, dnn::RnnInputMode input_mode,\n 1644                                    dnn::RnnDirectionMode direction_mode,\n ....\n 1647                                    uint64 seed,\n 1648                                    ScratchAllocator* state_allocator) {\n 1649: #if CUDNN_VERSION >= 5000\n 1650    mutex_lock lock{dnn_handle_mutex_};\n 1651:   std::unique_ptr<CudnnRnnDescriptor> rnn_desc(new CudnnRnnDescriptor(\n 1652        parent_, ToHandle(dnn_handle_), num_layers, hidden_size, input_size,\n 1653:       ToCudnnRnnInputMode(input_mode), ToCudnnRnnDirectionMode(direction_mode),\n 1654:       ToCudnnRnnMode(rnn_mode), ToCudnnDataType(data_type), dropout, seed,\n 1655        state_allocator));\n 1656    if (!rnn_desc->ok()) {\n ....\n 1661  #else\n 1662    string error_msg =\n 1663:       port::StrCat(\"createRnnDescriptor needs at least Cudnn 5.0 to work. \",\n 1664:                    \"Current Cudnn version: \", CUDNN_VERSION, \". \");\n 1665    LOG(ERROR) << error_msg;\n 1666    return port::Status{port::error::UNIMPLEMENTED, error_msg};\n 1667: #endif  // CUDNN_VERSION\n 1668  }\n 1669  \n 1670  port::StatusOr<std::unique_ptr<dnn::RnnSequenceTensorDescriptor>>\n 1671: CudnnSupport::createRnnSequenceTensorDescriptor(int seq_length, int batch_size,\n 1672                                                  int data_size,\n 1673                                                  dnn::DataType data_type) {\n 1674: #if CUDNN_VERSION >= 5000\n 1675:   std::unique_ptr<CudnnRnnSequenceTensorDescriptor> seq_desc(\n 1676:       new CudnnRnnSequenceTensorDescriptor(parent_, seq_length, batch_size,\n 1677                                             data_size,\n 1678:                                            ToCudnnDataType(data_type)));\n 1679    if (!seq_desc->ok()) {\n 1680      return seq_desc->Status();\n ....\n 1684  #else\n 1685    string error_msg = port::StrCat(\n 1686:       \"createRnnSequenceTensorDescriptor needs at least Cudnn 5.0 to work. \",\n 1687:       \"Current Cudnn version: \", CUDNN_VERSION, \". \");\n 1688    LOG(ERROR) << error_msg;\n 1689    return port::Status{port::error::UNIMPLEMENTED, error_msg};\n 1690: #endif  // CUDNN_VERSION\n 1691  }\n 1692  \n 1693  port::StatusOr<std::unique_ptr<dnn::RnnStateTensorDescriptor>>\n 1694: CudnnSupport::createRnnStateTensorDescriptor(int num_layer, int batch_size,\n 1695                                               int data_size,\n 1696                                               dnn::DataType data_type) {\n 1697: #if CUDNN_VERSION >= 5000\n 1698:   std::unique_ptr<CudnnRnnStateTensorDescriptor> state_desc(\n 1699:       new CudnnRnnStateTensorDescriptor(parent_, num_layer, batch_size,\n 1700:                                         data_size, ToCudnnDataType(data_type)));\n 1701    if (!state_desc->ok()) {\n 1702      return state_desc->Status();\n ....\n 1706  #else\n 1707    string error_msg = port::StrCat(\n 1708:       \"createRnnStateTensorDescriptor needs at least Cudnn 5.0 to work. \",\n 1709:       \"Current Cudnn version: \", CUDNN_VERSION, \". \");\n 1710    LOG(ERROR) << error_msg;\n 1711    return port::Status{port::error::UNIMPLEMENTED, error_msg};\n 1712: #endif  // CUDNN_VERSION\n 1713  }\n 1714  \n 1715: bool CudnnSupport::DoRnnForward(\n 1716      Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n 1717      const dnn::RnnSequenceTensorDescriptor& input_desc,\n ....\n 1729      ScratchAllocator* reserve_space_allocator,\n 1730      ScratchAllocator* workspace_allocator) {\n 1731: #if CUDNN_VERSION >= 5000\n 1732:   const CudnnRnnDescriptor& cudnn_rnn_desc =\n 1733:       static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n 1734:   const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n 1735:       static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n 1736:   const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n 1737:       static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n 1738:   const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n 1739:       static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n 1740:   const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n 1741:       static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n 1742:   const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n 1743:       static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n 1744:   const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n 1745:       static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n 1746  \n 1747    return DoRnnForwardImpl<float>(\n 1748:       stream, cudnn_rnn_desc, cudnn_input_desc, input_data, cudnn_input_h_desc,\n 1749:       input_h_data, cudnn_input_c_desc, input_c_data, params, cudnn_output_desc,\n 1750:       output_data, cudnn_output_h_desc, output_h_data, cudnn_output_c_desc,\n 1751        output_c_data, is_training, reserve_space_allocator, workspace_allocator);\n 1752  #else\n 1753    return false;\n 1754: #endif  // CUDNN_VERSION\n 1755  }\n 1756  \n 1757: bool CudnnSupport::DoRnnBackward(\n 1758      Stream* stream, const dnn::RnnDescriptor& rnn_desc,\n 1759      const dnn::RnnSequenceTensorDescriptor& input_desc,\n ....\n 1778      DeviceMemory<uint8>* reserve_space_data,\n 1779      ScratchAllocator* workspace_allocator) {\n 1780: #if CUDNN_VERSION >= 5000\n 1781:   const CudnnRnnDescriptor& cudnn_rnn_desc =\n 1782:       static_cast<const CudnnRnnDescriptor&>(rnn_desc);\n 1783:   const CudnnRnnSequenceTensorDescriptor& cudnn_input_desc =\n 1784:       static_cast<const CudnnRnnSequenceTensorDescriptor&>(input_desc);\n 1785:   const CudnnRnnStateTensorDescriptor& cudnn_input_h_desc =\n 1786:       static_cast<const CudnnRnnStateTensorDescriptor&>(input_h_desc);\n 1787:   const CudnnRnnStateTensorDescriptor& cudnn_input_c_desc =\n 1788:       static_cast<const CudnnRnnStateTensorDescriptor&>(input_c_desc);\n 1789:   const CudnnRnnSequenceTensorDescriptor& cudnn_output_desc =\n 1790:       static_cast<const CudnnRnnSequenceTensorDescriptor&>(output_desc);\n 1791:   const CudnnRnnStateTensorDescriptor& cudnn_output_h_desc =\n 1792:       static_cast<const CudnnRnnStateTensorDescriptor&>(output_h_desc);\n 1793:   const CudnnRnnStateTensorDescriptor& cudnn_output_c_desc =\n 1794:       static_cast<const CudnnRnnStateTensorDescriptor&>(output_c_desc);\n 1795  \n 1796    return DoRnnBackwardImpl<float>(\n 1797:       stream, cudnn_rnn_desc, cudnn_input_desc, input_data, cudnn_input_h_desc,\n 1798:       input_h_data, cudnn_input_c_desc, input_c_data, params, cudnn_output_desc,\n 1799:       output_data, cudnn_output_h_desc, output_h_data, cudnn_output_c_desc,\n 1800        output_c_data, output_backprop_data, output_h_backprop_data,\n 1801        output_c_backprop_data, input_backprop_data, input_h_backprop_data,\n ....\n 1804  #else\n 1805    return false;\n 1806: #endif  // CUDNN_VERSION\n 1807  }\n 1808  \n 1809  template <class T>\n 1810: bool CudnnSupport::DoConvolveImpl(\n 1811:     Stream* stream, int cudnn_type,  // Actually cudnnDataType_t.\n 1812      const BatchDescriptor& batch_descriptor, const DeviceMemory<T>& input_data,\n 1813      const FilterDescriptor& filter_descriptor,\n ....\n 1819      dnn::ProfileResult* output_profile_result) {\n 1820    ScopedTensorDescriptor input_nd{parent_, batch_descriptor,\n 1821:       static_cast<cudnnDataType_t>(cudnn_type)};\n 1822    ScopedTensorDescriptor output_nd{parent_, output_descriptor,\n 1823:       static_cast<cudnnDataType_t>(cudnn_type)};\n 1824    ScopedFilterDescriptor filter{parent_, filter_descriptor, batch_descriptor,\n 1825:       static_cast<cudnnDataType_t>(cudnn_type)};\n 1826:   // TODO(sesse): Figure out under what circumstances cuDNN would\n 1827:   // accept CUDNN_DATA_HALF here; probably related to compute capability\n 1828:   // and cuDNN version; at least cuDNN 4 on TITAN X only supports\n 1829:   // CUDNN_DATA_FLOAT even for half input.\n 1830    ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n 1831:       CUDNN_DATA_FLOAT};\n 1832  \n 1833    mutex_lock lock{dnn_handle_mutex_};\n 1834:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1835                                          AsCUDAStreamValue(stream));\n 1836:   if (status != CUDNN_STATUS_SUCCESS) {\n 1837:     LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 1838    }\n 1839    // Alpha is the scaling factor for input.\n ....\n 1843  \n 1844    const bool is_profiling = output_profile_result != nullptr;\n 1845:   cudnnConvolutionFwdAlgo_t algo;\n 1846    DeviceMemory<uint8> scratch;\n 1847  \n 1848    if (algorithm_config.algorithm() == dnn::kDefaultAlgorithm) {\n 1849:     // With the default algorithm, use Cudnn's heuristics.\n 1850      auto get_algorithm = [&](bool specify_limit)\n 1851          SHARED_LOCKS_REQUIRED(dnn_handle_mutex_) {\n 1852:           cudnnConvolutionFwdPreference_t preference =\n 1853:               specify_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\n 1854:                             : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\n 1855  \n 1856            auto memory_limit_bytes =\n ....\n 1862            }\n 1863  \n 1864:           cudnnConvolutionFwdAlgo_t algo_to_use;\n 1865:           status = dynload::cudnnGetConvolutionForwardAlgorithm(\n 1866                parent_, ToHandle(dnn_handle_), input_nd.handle(),\n 1867                filter.handle(), conv.handle(), output_nd.handle(),\n ....\n 1869                /*memoryLimitInBytes=*/memory_limit_bytes,\n 1870                /*algo=*/&algo_to_use);\n 1871:           CHECK_EQ(status, CUDNN_STATUS_SUCCESS)\n 1872                << \"Unable to find a suitable \"\n 1873                   \"algorithm for doing forward \"\n ....\n 1880      if (scratch_allocator != nullptr) {\n 1881        size_t size_in_bytes;\n 1882:       status = dynload::cudnnGetConvolutionForwardWorkspaceSize(\n 1883            parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 1884            /*filterDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n 1885            /*destDesc=*/output_nd.handle(), /*algo=*/algo,\n 1886            /*sizeInBytes=*/&size_in_bytes);\n 1887:       if (status == CUDNN_STATUS_SUCCESS && size_in_bytes != 0) {\n 1888          auto allocated =\n 1889              scratch_allocator->AllocateBytes(stream, size_in_bytes);\n ....\n 1904  \n 1905      size_t size_in_bytes;\n 1906:     status = dynload::cudnnGetConvolutionForwardWorkspaceSize(\n 1907          parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 1908          /*filterDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n 1909          /*destDesc=*/output_nd.handle(), /*algo=*/algo,\n 1910          /*sizeInBytes=*/&size_in_bytes);\n 1911:     if (status != CUDNN_STATUS_SUCCESS) {\n 1912        if (is_profiling) {\n 1913          // Silently return when we are profiling.\n ....\n 1946        return false;\n 1947      }\n 1948:     // The start and stop of the timer should be as close to the Cudnn call as\n 1949      // possible. It is still possible for other threads to issue workload on\n 1950      // to this stream. So it could take multiple profiling measurements.\n ....\n 1954      }\n 1955    }\n 1956:   status = dynload::cudnnConvolutionForward(\n 1957        parent_, ToHandle(dnn_handle_),\n 1958        /*alpha=*/&alpha, /*srcDesc=*/input_nd.handle(),\n ....\n 1974    }\n 1975  \n 1976:   if (status != CUDNN_STATUS_SUCCESS) {\n 1977      // Silently return when we are profiling.\n 1978      if (!is_profiling) {\n ....\n 1986  }\n 1987  \n 1988: bool CudnnSupport::GetConvolveAlgorithms(\n 1989      std::vector<dnn::AlgorithmType>* out_algorithms) {\n 1990    out_algorithms->assign({\n 1991        // clang-format off\n 1992:       CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,\n 1993:       CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM,\n 1994:       CUDNN_CONVOLUTION_FWD_ALGO_GEMM,\n 1995:       CUDNN_CONVOLUTION_FWD_ALGO_DIRECT,\n 1996:       CUDNN_CONVOLUTION_FWD_ALGO_FFT,\n 1997:       CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING,\n 1998: #if CUDNN_VERSION >= 5000\n 1999:       CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD,\n 2000  #endif\n 2001        // clang-format on\n ....\n 2004  }\n 2005  \n 2006: bool CudnnSupport::GetConvolveBackwardDataAlgorithms(\n 2007      std::vector<dnn::AlgorithmType>* out_algorithms) {\n 2008    out_algorithms->assign({\n 2009        // clang-format off\n 2010:       CUDNN_CONVOLUTION_BWD_DATA_ALGO_0,\n 2011:       CUDNN_CONVOLUTION_BWD_DATA_ALGO_1,\n 2012:       CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT,\n 2013:       CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING,\n 2014: #if CUDNN_VERSION >= 5000\n 2015:       CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD,\n 2016  #endif\n 2017        // clang-format on\n ....\n 2020  }\n 2021  \n 2022: bool CudnnSupport::GetConvolveBackwardFilterAlgorithms(\n 2023      std::vector<dnn::AlgorithmType>* out_algorithms) {\n 2024    out_algorithms->assign({\n 2025        // clang-format off\n 2026:       CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0,\n 2027:       CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1,\n 2028:       CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT,\n 2029:       CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3,\n 2030        // clang-format on\n 2031    });\n ....\n 2033  }\n 2034  \n 2035: bool CudnnSupport::DoBatchNormalizationForward(\n 2036      Stream* stream, const DeviceMemory<float>& x,\n 2037      const DeviceMemory<float>& scale, const DeviceMemory<float>& offset,\n ....\n 2053  \n 2054  template <class T>\n 2055: bool CudnnSupport::DoBatchNormalizationForwardImpl(\n 2056      Stream* stream, dnn::DataType data_type, const DeviceMemory<T>& x,\n 2057      const DeviceMemory<T>& scale, const DeviceMemory<T>& offset,\n ....\n 2065      std::function<void()> inv_var_to_var) {\n 2066    mutex_lock lock{dnn_handle_mutex_};\n 2067:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2068                                          AsCUDAStreamValue(stream));\n 2069:   if (status != CUDNN_STATUS_SUCCESS) {\n 2070:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 2071      return false;\n 2072    }\n 2073  \n 2074    ScopedTensorDescriptor x_descriptor{parent_, x_desc,\n 2075:                                       ToCudnnDataType(data_type)};\n 2076    ScopedTensorDescriptor scale_offset_descriptor{parent_, scale_offset_desc,\n 2077:                                                  ToCudnnDataType(data_type)};\n 2078:   cudnnBatchNormMode_t mode = CUDNN_BATCHNORM_SPATIAL;\n 2079    float one = 1.0;\n 2080    float zero = 0.0;\n ....\n 2083      stream->ThenMemZero(batch_mean, batch_mean->size());\n 2084      stream->ThenMemZero(batch_var, batch_var->size());\n 2085:     status = dynload::cudnnBatchNormalizationForwardTraining(\n 2086          parent_, ToHandle(dnn_handle_), mode, &one, &zero,\n 2087          x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y->opaque(),\n ....\n 2089          batch_mean->opaque(), batch_var->opaque(), epsilon,\n 2090          saved_mean->opaque(), saved_inv_var->opaque());\n 2091: #if CUDNN_VERSION < 5000\n 2092      CHECK(inv_var_to_var);\n 2093      inv_var_to_var();\n 2094  #endif\n 2095    } else {\n 2096: #if CUDNN_VERSION < 5000\n 2097      CHECK(var_to_inv_var);\n 2098      const void* maybe_inv_var = var_to_inv_var().opaque();\n ....\n 2100      const void* maybe_inv_var = estimated_variance.opaque();\n 2101  #endif\n 2102:     status = dynload::cudnnBatchNormalizationForwardInference(\n 2103          parent_, ToHandle(dnn_handle_), mode, &one, &zero,\n 2104          x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y->opaque(),\n ....\n 2106          estimated_mean.opaque(), maybe_inv_var, epsilon);\n 2107    }\n 2108:   if (status != CUDNN_STATUS_SUCCESS) {\n 2109      LOG(ERROR) << \"failed to enqueue forward batch normalization on stream: \"\n 2110                 << ToString(status);\n ....\n 2114  }\n 2115  \n 2116: bool CudnnSupport::DoBatchNormalizationBackward(\n 2117      Stream* stream, const DeviceMemory<float>& y_backprop,\n 2118      const DeviceMemory<float>& x, const DeviceMemory<float>& scale,\n ....\n 2123      DeviceMemory<float>* offset_backprop) {\n 2124    return DoBatchNormalizationBackwardImpl(\n 2125:       stream, CUDNN_DATA_FLOAT, y_backprop, x, scale, mean, variance, x_desc,\n 2126        scale_offset_desc, epsilon, x_backprop, scale_backprop, offset_backprop);\n 2127  }\n 2128  \n 2129  template <class T>\n 2130: bool CudnnSupport::DoBatchNormalizationBackwardImpl(\n 2131:     Stream* stream, int cudnn_type, const DeviceMemory<T>& y_backprop,\n 2132      const DeviceMemory<T>& x, const DeviceMemory<T>& scale,\n 2133      const DeviceMemory<T>& mean, const DeviceMemory<T>& variance,\n ....\n 2137      DeviceMemory<T>* offset_backprop) {\n 2138    mutex_lock lock{dnn_handle_mutex_};\n 2139:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2140                                          AsCUDAStreamValue(stream));\n 2141:   if (status != CUDNN_STATUS_SUCCESS) {\n 2142:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 2143      return false;\n 2144    }\n 2145  \n 2146    ScopedTensorDescriptor x_descriptor{parent_, x_desc,\n 2147:                                       static_cast<cudnnDataType_t>(cudnn_type)};\n 2148    ScopedTensorDescriptor scale_offset_descriptor{\n 2149:       parent_, scale_offset_desc, static_cast<cudnnDataType_t>(cudnn_type)};\n 2150:   cudnnBatchNormMode_t mode = CUDNN_BATCHNORM_SPATIAL;\n 2151    float one = 1.0;\n 2152    float zero = 0.0;\n 2153  \n 2154:   status = dynload::cudnnBatchNormalizationBackward(\n 2155        parent_, ToHandle(dnn_handle_), mode, &one, &zero, &one, &zero,\n 2156        x_descriptor.handle(), x.opaque(), x_descriptor.handle(),\n ....\n 2159        scale_backprop->opaque(), offset_backprop->opaque(), epsilon,\n 2160        mean.opaque(), variance.opaque());\n 2161:   if (status != CUDNN_STATUS_SUCCESS) {\n 2162      LOG(ERROR) << \"failed to enqueue backward batch normalization on stream: \"\n 2163                 << ToString(status);\n ....\n 2167  }\n 2168  \n 2169: bool CudnnSupport::DoConvolve(\n 2170      Stream* stream, const BatchDescriptor& batch_descriptor,\n 2171      const DeviceMemory<float>& input_data,\n ....\n 2178      dnn::ProfileResult* output_profile_result) {\n 2179    return DoConvolveImpl<float>(\n 2180:       stream, CUDNN_DATA_FLOAT, batch_descriptor, input_data, filter_descriptor,\n 2181        filter_data, convolution_descriptor, output_descriptor, output_data,\n 2182        scratch_allocator, algorithm_config, output_profile_result);\n 2183  }\n 2184  \n 2185: bool CudnnSupport::DoConvolve(\n 2186      Stream* stream, const BatchDescriptor& batch_descriptor,\n 2187      const DeviceMemory<double>& input_data,\n ....\n 2195  }\n 2196  \n 2197: bool CudnnSupport::DoConvolve(\n 2198      Stream* stream, const BatchDescriptor& batch_descriptor,\n 2199      const DeviceMemory<Eigen::half>& input_data,\n ....\n 2206      dnn::ProfileResult* output_profile_result) {\n 2207    return DoConvolveImpl<Eigen::half>(\n 2208:       stream, CUDNN_DATA_HALF, batch_descriptor, input_data, filter_descriptor,\n 2209        filter_data, convolution_descriptor, output_descriptor, output_data,\n 2210        scratch_allocator, algorithm_config, output_profile_result);\n ....\n 2212  \n 2213  template<class T>\n 2214: DeviceMemory<T> CudnnSupport::MaybeTransformLayout(\n 2215      Stream* stream,\n 2216:     int cudnn_type,  // Actually cudnnDataType_t.\n 2217      BatchDescriptor* output_descriptor,\n 2218      DeviceMemory<T> backward_output_data,\n ....\n 2229    transformed_output_descriptor.set_layout(dnn::DataLayout::kBatchDepthYX);\n 2230    ScopedTensorDescriptor orig_out_back_nd{\n 2231:       parent_, *output_descriptor, static_cast<cudnnDataType_t>(cudnn_type)};\n 2232    ScopedTensorDescriptor transformed_out_back_nd{\n 2233        parent_, transformed_output_descriptor,\n 2234:       static_cast<cudnnDataType_t>(cudnn_type)};\n 2235  \n 2236    float alpha = 1.0f;\n 2237    float beta = 0.0f;\n 2238:   auto status = dynload::cudnnTransformTensor(\n 2239        parent_, ToHandle(dnn_handle_), &alpha, orig_out_back_nd.handle(),\n 2240        backward_output_data.opaque(), &beta, transformed_out_back_nd.handle(),\n 2241        (*transform_scratch)->mutable_device_memory()->opaque());\n 2242  \n 2243:   if (status != CUDNN_STATUS_SUCCESS) {\n 2244      LOG(FATAL) << \"Failed to transform the data layout.\";\n 2245    }\n ....\n 2249  \n 2250  template <class T>\n 2251: bool CudnnSupport::DoConvolveBackwardDataImpl(\n 2252      Stream* stream,\n 2253:     int cudnn_type,  // Actually cudnnDataType_t.\n 2254      const FilterDescriptor& filter_descriptor,\n 2255      const DeviceMemory<T>& filter_data,\n ....\n 2262      dnn::ProfileResult* output_profile_result) {\n 2263    mutex_lock lock{dnn_handle_mutex_};\n 2264:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2265                                          AsCUDAStreamValue(stream));\n 2266:   if (status != CUDNN_STATUS_SUCCESS) {\n 2267:     LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 2268    }\n 2269  \n ....\n 2273    float beta = 0.0;\n 2274  \n 2275:   // TBD(keveman): remove once cuDNN supports kBatchYXDepth for backward pass.\n 2276    BatchDescriptor output_descriptor;\n 2277    output_descriptor.CloneFrom(output_descriptor_in);\n 2278    std::unique_ptr<TemporaryDeviceMemory<T>> transform_scratch;\n 2279    backward_output_data = MaybeTransformLayout(\n 2280:       stream, cudnn_type, &output_descriptor, backward_output_data,\n 2281        &transform_scratch);\n 2282  \n 2283    ScopedTensorDescriptor out_back_nd{parent_, output_descriptor,\n 2284:                                      static_cast<cudnnDataType_t>(cudnn_type)};\n 2285    ScopedTensorDescriptor in_back_nd{parent_, input_descriptor,\n 2286:                                     static_cast<cudnnDataType_t>(cudnn_type)};\n 2287    ScopedFilterDescriptor filter{parent_, filter_descriptor, input_descriptor,\n 2288:                                 static_cast<cudnnDataType_t>(cudnn_type)};\n 2289:   // TODO(sesse): Figure out under what circumstances cuDNN would\n 2290:   // accept CUDNN_DATA_HALF here; probably related to compute capability\n 2291:   // and cuDNN version; at least cuDNN 4 on TITAN X only supports\n 2292:   // CUDNN_DATA_FLOAT even for half input.\n 2293    ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n 2294:                                    CUDNN_DATA_FLOAT};\n 2295  \n 2296    const bool is_profiling = output_profile_result != nullptr;\n 2297:   cudnnConvolutionBwdDataAlgo_t algo;\n 2298    DeviceMemory<uint8> scratch;\n 2299  \n 2300    if (algorithm_config.algorithm() == dnn::kDefaultAlgorithm) {\n 2301:     // With the default algorithm, use Cudnn's heuristics.\n 2302      auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n 2303:         dnn_handle_mutex_) -> cudnnConvolutionBwdDataAlgo_t {\n 2304:       cudnnConvolutionBwdDataPreference_t preference =\n 2305:           specify_limit ? CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT\n 2306:                         : CUDNN_CONVOLUTION_BWD_DATA_NO_WORKSPACE;\n 2307  \n 2308        auto memory_limit_bytes =\n ....\n 2314        }\n 2315  \n 2316:       cudnnConvolutionBwdDataAlgo_t algo_to_use;\n 2317:       cudnnStatus_t status = dynload::cudnnGetConvolutionBackwardDataAlgorithm(\n 2318            parent_, ToHandle(dnn_handle_),\n 2319            /*filterDesc=*/filter.handle(),\n ....\n 2324            /*memoryLimitInBytes=*/memory_limit_bytes,\n 2325            /*algo=*/&algo_to_use);\n 2326:       CHECK_EQ(status, CUDNN_STATUS_SUCCESS) << \"Unable to find a suitable \"\n 2327                                                  \"algorithm for doing backward \"\n 2328                                                  \"filter convolution\";\n ....\n 2334      if (scratch_allocator != nullptr) {\n 2335        size_t size_in_bytes;\n 2336:       status = dynload::cudnnGetConvolutionBackwardDataWorkspaceSize(\n 2337            parent_, ToHandle(dnn_handle_),\n 2338            /*filterDesc=*/filter.handle(),\n ....\n 2342            /*algo=*/algo,\n 2343            /*sizeInBytes=*/&size_in_bytes);\n 2344:       if (status == CUDNN_STATUS_SUCCESS && size_in_bytes != 0) {\n 2345          auto allocated =\n 2346              scratch_allocator->AllocateBytes(stream, size_in_bytes);\n ....\n 2360      algo = ToConvBackwardDataAlgo(algorithm_config.algorithm());\n 2361      size_t size_in_bytes;\n 2362:     status = dynload::cudnnGetConvolutionBackwardDataWorkspaceSize(\n 2363          parent_, ToHandle(dnn_handle_),\n 2364          /*filterDesc=*/filter.handle(),\n ....\n 2368          /*algo=*/algo,\n 2369          /*sizeInBytes=*/&size_in_bytes);\n 2370:     if (status != CUDNN_STATUS_SUCCESS) {\n 2371        if (is_profiling) {\n 2372          // Silently return when we are profiling.\n ....\n 2403      timer.reset(new CUDATimer(parent_));\n 2404      timer->Init();\n 2405:     // The start and stop of the timer should be as close to the Cudnn call as\n 2406      // possible. It is still possible for other threads to issue workload on\n 2407      // to this stream. So it could take multiple profiling measurements.\n ....\n 2409    }\n 2410  \n 2411: #if CUDNN_VERSION >= 5000\n 2412:   status = dynload::cudnnConvolutionBackwardData(\n 2413  #else\n 2414:   status = dynload::cudnnConvolutionBackwardData_v3(\n 2415  #endif\n 2416        parent_, ToHandle(dnn_handle_),\n ....\n 2435      timer->Destroy();\n 2436    }\n 2437:   if (status != CUDNN_STATUS_SUCCESS) {\n 2438      // Silently return when we are profiling.\n 2439      if (!is_profiling) {\n ....\n 2446  }\n 2447  \n 2448: bool CudnnSupport::DoConvolveBackwardData(\n 2449      Stream* stream, const FilterDescriptor& filter_descriptor,\n 2450      const DeviceMemory<float>& filter_data,\n ....\n 2458      dnn::ProfileResult* output_profile_result) {\n 2459    return DoConvolveBackwardDataImpl(\n 2460:       stream, CUDNN_DATA_FLOAT, filter_descriptor, filter_data,\n 2461        output_descriptor_in, backward_output_data, convolution_descriptor,\n 2462        input_descriptor, backward_input_data, scratch_allocator,\n ....\n 2464  }\n 2465  \n 2466: bool CudnnSupport::DoConvolveBackwardData(\n 2467      Stream* stream, const FilterDescriptor& filter_descriptor,\n 2468      const DeviceMemory<Eigen::half>& filter_data,\n ....\n 2476      dnn::ProfileResult* output_profile_result) {\n 2477    return DoConvolveBackwardDataImpl(\n 2478:       stream, CUDNN_DATA_HALF, filter_descriptor, filter_data,\n 2479        output_descriptor_in, backward_output_data, convolution_descriptor,\n 2480        input_descriptor, backward_input_data, scratch_allocator,\n ....\n 2483  \n 2484  template <class T>\n 2485: bool CudnnSupport::DoConvolveBackwardFilterImpl(\n 2486:     Stream* stream, int cudnn_type,  // Actually cudnnDataType_t.\n 2487      const dnn::BatchDescriptor& input_descriptor,\n 2488      const DeviceMemory<T>& input_data,\n ....\n 2495      dnn::ProfileResult* output_profile_result) {\n 2496    mutex_lock lock{dnn_handle_mutex_};\n 2497:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2498                                          AsCUDAStreamValue(stream));\n 2499:   if (status != CUDNN_STATUS_SUCCESS) {\n 2500:     LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 2501    }\n 2502  \n ....\n 2506    float beta = 0.0;\n 2507  \n 2508:   // TBD(keveman): remove once cuDNN supports kBatchYXDepth for backward pass.\n 2509    BatchDescriptor output_descriptor;\n 2510    output_descriptor.CloneFrom(output_descriptor_in);\n 2511    std::unique_ptr<TemporaryDeviceMemory<T>> transform_scratch;\n 2512    backward_output_data = MaybeTransformLayout(\n 2513:       stream, static_cast<cudnnDataType_t>(cudnn_type),\n 2514        &output_descriptor, backward_output_data,\n 2515        &transform_scratch);\n 2516  \n 2517    ScopedTensorDescriptor out_back_nd{parent_, output_descriptor,\n 2518:         static_cast<cudnnDataType_t>(cudnn_type)};\n 2519    ScopedTensorDescriptor input_nd{parent_, input_descriptor,\n 2520:           static_cast<cudnnDataType_t>(cudnn_type)};\n 2521    ScopedFilterDescriptor filter{parent_, filter_descriptor, input_descriptor,\n 2522:         static_cast<cudnnDataType_t>(cudnn_type)};\n 2523:   // TODO(sesse): Figure out under what circumstances cuDNN would\n 2524:   // accept CUDNN_DATA_HALF here; probably related to compute capability\n 2525:   // and cuDNN version; at least cuDNN 4 on TITAN X only supports\n 2526:   // CUDNN_DATA_FLOAT even for half input.\n 2527    ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n 2528:       CUDNN_DATA_FLOAT};\n 2529  \n 2530    const bool is_profiling = output_profile_result != nullptr;\n 2531:   cudnnConvolutionBwdFilterAlgo_t algo;\n 2532    DeviceMemory<uint8> scratch;\n 2533  \n 2534    if (algorithm_config.algorithm() == dnn::kDefaultAlgorithm) {\n 2535:     // With the default algorithm, use Cudnn's heuristics.\n 2536  \n 2537      // Lambda that retrieves the algorithm.\n ....\n 2540      auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n 2541          dnn_handle_mutex_) {\n 2542:       cudnnConvolutionBwdFilterPreference_t preference =\n 2543:           specify_limit ? CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT\n 2544:                         : CUDNN_CONVOLUTION_BWD_FILTER_NO_WORKSPACE;\n 2545  \n 2546        auto memory_limit_bytes =\n ....\n 2552        }\n 2553  \n 2554:       cudnnConvolutionBwdFilterAlgo_t algo_to_use;\n 2555:       cudnnStatus_t status =\n 2556:           dynload::cudnnGetConvolutionBackwardFilterAlgorithm(\n 2557                parent_, ToHandle(dnn_handle_),\n 2558                /*srcDesc=*/input_nd.handle(),\n ....\n 2563                /*memoryLimitInBytes=*/memory_limit_bytes,\n 2564                /*algo=*/&algo_to_use);\n 2565:       CHECK_EQ(status, CUDNN_STATUS_SUCCESS) << \"Unable to find a suitable \"\n 2566                                                  \"algorithm for doing backward \"\n 2567                                                  \"filter convolution\";\n ....\n 2573      if (scratch_allocator != nullptr) {\n 2574        size_t size_in_bytes;\n 2575:       status = dynload::cudnnGetConvolutionBackwardFilterWorkspaceSize(\n 2576            parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 2577            /*diffDesc=*/out_back_nd.handle(), /*convDesc=*/conv.handle(),\n 2578            /*gradDesc=*/filter.handle(), /*algo=*/algo,\n 2579            /*sizeInBytes=*/&size_in_bytes);\n 2580:       if (status == CUDNN_STATUS_SUCCESS && size_in_bytes != 0) {\n 2581          auto allocated =\n 2582              scratch_allocator->AllocateBytes(stream, size_in_bytes);\n ....\n 2597  \n 2598      size_t size_in_bytes;\n 2599:     status = dynload::cudnnGetConvolutionBackwardFilterWorkspaceSize(\n 2600          parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 2601          /*diffDesc=*/out_back_nd.handle(), /*convDesc=*/conv.handle(),\n 2602          /*gradDesc=*/filter.handle(), /*algo=*/algo,\n 2603          /*sizeInBytes=*/&size_in_bytes);\n 2604:     if (status != CUDNN_STATUS_SUCCESS) {\n 2605        if (is_profiling) {\n 2606          // Silently return when we are profiling.\n ....\n 2638      timer.reset(new CUDATimer(parent_));\n 2639      timer->Init();\n 2640:     // The start and stop of the timer should be as close to the Cudnn call as\n 2641      // possible. It is still possible for other threads to issue workload on\n 2642      // to this stream. So it could take multiple profiling measurements.\n ....\n 2644    }\n 2645  \n 2646: #if CUDNN_VERSION >= 5000\n 2647:   status = dynload::cudnnConvolutionBackwardFilter(\n 2648  #else\n 2649:   status = dynload::cudnnConvolutionBackwardFilter_v3(\n 2650  #endif\n 2651        parent_, ToHandle(dnn_handle_), /*alpha=*/&alpha,\n ....\n 2669      timer->Destroy();\n 2670    }\n 2671:   if (status != CUDNN_STATUS_SUCCESS) {\n 2672      // Silently return when we are profiling.\n 2673      if (!is_profiling) {\n ....\n 2680  }\n 2681  \n 2682: bool CudnnSupport::DoConvolveBackwardFilter(\n 2683      Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n 2684      const DeviceMemory<float>& input_data,\n ....\n 2692      dnn::ProfileResult* output_profile_result) {\n 2693    return DoConvolveBackwardFilterImpl(\n 2694:       stream, CUDNN_DATA_FLOAT, input_descriptor, input_data,\n 2695        output_descriptor_in, backward_output_data, convolution_descriptor,\n 2696        filter_descriptor, backward_filter_data, scratch_allocator,\n ....\n 2698  }\n 2699  \n 2700: bool CudnnSupport::DoConvolveBackwardFilter(\n 2701      Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n 2702      const DeviceMemory<Eigen::half>& input_data,\n ....\n 2710      dnn::ProfileResult* output_profile_result) {\n 2711    return DoConvolveBackwardFilterImpl(\n 2712:       stream, CUDNN_DATA_HALF, input_descriptor, input_data,\n 2713        output_descriptor_in, backward_output_data, convolution_descriptor,\n 2714        filter_descriptor, backward_filter_data, scratch_allocator,\n ....\n 2717  \n 2718  template <class T>\n 2719: bool CudnnSupport::DoConvolveBackwardBiasImpl(\n 2720:     Stream* stream, int cudnn_type,  // Actually cudnnDataType_t.\n 2721      const dnn::BatchDescriptor& input_descriptor,\n 2722      const DeviceMemory<T>& input_data,\n ....\n 2724      DeviceMemory<T>* backward_bias_data) {\n 2725    mutex_lock lock{dnn_handle_mutex_};\n 2726:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2727                                          AsCUDAStreamValue(stream));\n 2728:   if (status != CUDNN_STATUS_SUCCESS) {\n 2729:     LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 2730    }\n 2731  \n 2732    ScopedTensorDescriptor input_nd{parent_, input_descriptor,\n 2733:                                   static_cast<cudnnDataType_t>(cudnn_type)};\n 2734    ScopedTensorDescriptor bias_nd{parent_, bias_descriptor,\n 2735:                                  static_cast<cudnnDataType_t>(cudnn_type)};\n 2736  \n 2737    // Alpha is the scaling factor for input.\n ....\n 2740    float beta = 0.0;\n 2741  \n 2742:   status = dynload::cudnnConvolutionBackwardBias(\n 2743        parent_, ToHandle(dnn_handle_), &alpha, input_nd.handle(),\n 2744        input_data.opaque(), &beta, bias_nd.handle(),\n 2745        backward_bias_data->opaque());\n 2746:   if (status != CUDNN_STATUS_SUCCESS) {\n 2747      LOG(FATAL) << \"failed to enqueue backward convolution on stream: \"\n 2748                 << ToString(status);\n ....\n 2752  }\n 2753  \n 2754: bool CudnnSupport::DoConvolveBackwardBias(\n 2755      Stream* stream, const BatchDescriptor& input_descriptor,\n 2756      const DeviceMemory<double>& input_data,\n 2757      const BatchDescriptor& bias_descriptor,\n 2758      DeviceMemory<double>* backward_bias_data) {\n 2759:   return DoConvolveBackwardBiasImpl(stream, CUDNN_DATA_DOUBLE, input_descriptor,\n 2760                                      input_data, bias_descriptor,\n 2761                                      backward_bias_data);\n 2762  }\n 2763  \n 2764: bool CudnnSupport::DoConvolveBackwardBias(\n 2765      Stream* stream, const BatchDescriptor& input_descriptor,\n 2766      const DeviceMemory<float>& input_data,\n 2767      const BatchDescriptor& bias_descriptor,\n 2768      DeviceMemory<float>* backward_bias_data) {\n 2769:   return DoConvolveBackwardBiasImpl(stream, CUDNN_DATA_FLOAT, input_descriptor,\n 2770                                      input_data, bias_descriptor,\n 2771                                      backward_bias_data);\n 2772  }\n 2773  \n 2774: bool CudnnSupport::DoConvolveBackwardBias(\n 2775      Stream* stream, const BatchDescriptor& input_descriptor,\n 2776      const DeviceMemory<Eigen::half>& input_data,\n 2777      const BatchDescriptor& bias_descriptor,\n 2778      DeviceMemory<Eigen::half>* backward_bias_data) {\n 2779:   return DoConvolveBackwardBiasImpl(stream, CUDNN_DATA_HALF, input_descriptor,\n 2780                                      input_data, bias_descriptor,\n 2781                                      backward_bias_data);\n 2782  }\n 2783  \n 2784: bool CudnnSupport::DoMatMul(Stream* stream,\n 2785                              const DeviceMemory<float>& input_data,\n 2786                              const DeviceMemory<float>& weights,\n ....\n 2916  }\n 2917  \n 2918: bool CudnnSupport::DoBiasAdd(Stream* stream,\n 2919                               const DeviceMemory<float>& input_data,\n 2920                               const DeviceMemory<float>& biases,\n ....\n 2922                               DeviceMemory<float>* output_data) {\n 2923    ScopedTensorDescriptor input_descriptor{parent_, dimensions,\n 2924:                                           CUDNN_DATA_FLOAT};\n 2925  \n 2926    BatchDescriptor bias_dimensions;\n ....\n 2931        .set_layout(dnn::DataLayout::kBatchYXDepth);\n 2932    ScopedTensorDescriptor bias_descriptor{parent_, bias_dimensions,\n 2933:                                          CUDNN_DATA_FLOAT};\n 2934  \n 2935:   // cudnnAddTensor after R3 is in-place, so we need to copy input_data to\n 2936    // output_data before doing the addition, unless the input and\n 2937    // output are at the same address.\n ....\n 2948  \n 2949    mutex_lock lock{dnn_handle_mutex_};\n 2950:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2951                                          AsCUDAStreamValue(stream));\n 2952:   if (status != CUDNN_STATUS_SUCCESS) {\n 2953:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 2954      return false;\n 2955    }\n ....\n 2958    const float beta = 1.0f;\n 2959  \n 2960: #if CUDNN_VERSION >= 5000\n 2961:   status = dynload::cudnnAddTensor(\n 2962  #else\n 2963:   status = dynload::cudnnAddTensor_v3(\n 2964  #endif\n 2965        parent_, ToHandle(dnn_handle_), &alpha, bias_descriptor.handle(),\n ....\n 2967        output_data->opaque());\n 2968  \n 2969:   if (status != CUDNN_STATUS_SUCCESS) {\n 2970      LOG(ERROR) << \"stream \" << stream << \" could not enqueue bias addition.\";\n 2971      return false;\n ....\n 2975  }\n 2976  \n 2977: bool CudnnSupport::DoActivate(Stream* stream,\n 2978                                dnn::ActivationMode activation_mode,\n 2979                                const dnn::BatchDescriptor& dimensions,\n ....\n 2981                                DeviceMemory<float>* output_data) {\n 2982    mutex_lock lock{dnn_handle_mutex_};\n 2983:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2984                                          AsCUDAStreamValue(stream));\n 2985:   if (status != CUDNN_STATUS_SUCCESS) {\n 2986:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 2987      return false;\n 2988    }\n 2989  \n 2990: #if CUDNN_VERSION >= 5000\n 2991    ScopedActivationDescriptor activation_desc{parent_, activation_mode,\n 2992                                               dimensions.value_max()};\n 2993  #else\n 2994:   cudnnActivationMode_t mode;\n 2995    switch (activation_mode) {\n 2996      case dnn::ActivationMode::kRelu6:\n 2997        // TODO(leary) should probably do a post-pass to clip at 6?\n 2998        LOG(WARNING) << \"user requested Relu6, but providing Relu instead\";\n 2999:       mode = CUDNN_ACTIVATION_RELU;\n 3000        break;\n 3001      case dnn::ActivationMode::kReluX:\n 3002        // TODO(broune) should probably do a post-pass to clip at X?\n 3003        LOG(WARNING) << \"user requested ReluX, but providing Relu instead\";\n 3004:       mode = CUDNN_ACTIVATION_RELU;\n 3005        break;\n 3006      case dnn::ActivationMode::kRelu:\n 3007:       mode = CUDNN_ACTIVATION_RELU;\n 3008        break;\n 3009      case dnn::ActivationMode::kSigmoid:\n 3010:       mode = CUDNN_ACTIVATION_SIGMOID;\n 3011        break;\n 3012      case dnn::ActivationMode::kTanh:\n 3013:       mode = CUDNN_ACTIVATION_TANH;\n 3014        break;\n 3015      default:\n ....\n 3020  #endif\n 3021  \n 3022:   ScopedTensorDescriptor input_nd{parent_, dimensions, CUDNN_DATA_FLOAT};\n 3023    // Alpha is the input scaling factor.\n 3024    float alpha = 1.0;\n 3025    // Beta is the output scaling factor.\n 3026    float beta = 0.0;\n 3027:   status = dynload::cudnnActivationForward(\n 3028        parent_, ToHandle(dnn_handle_),\n 3029: #if CUDNN_VERSION >= 5000\n 3030        activation_desc.handle(),\n 3031  #else\n ....\n 3034        &alpha, input_nd.handle(), input_data.opaque(), &beta, input_nd.handle(),\n 3035        output_data->opaque());\n 3036:   if (status != CUDNN_STATUS_SUCCESS) {\n 3037      LOG(ERROR) << \"stream \" << stream\n 3038                 << \" could not enqueue activation: \" << ToString(status);\n ....\n 3043  }\n 3044  \n 3045: bool CudnnSupport::DoPoolForward(\n 3046      Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n 3047      const dnn::BatchDescriptor& input_dimensions,\n ....\n 3050      DeviceMemory<float>* output_data) {\n 3051    mutex_lock lock{dnn_handle_mutex_};\n 3052:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3053                                          AsCUDAStreamValue(stream));\n 3054:   if (status != CUDNN_STATUS_SUCCESS) {\n 3055:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 3056      return false;\n 3057    }\n ....\n 3062    float beta = 0.0;\n 3063  \n 3064:   ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_FLOAT};\n 3065    ScopedTensorDescriptor dest_desc{parent_, output_dimensions,\n 3066:                                    CUDNN_DATA_FLOAT};\n 3067    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 3068:   status = dynload::cudnnPoolingForward(\n 3069        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 3070        src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(),\n 3071        output_data->opaque());\n 3072:   if (status != CUDNN_STATUS_SUCCESS) {\n 3073      LOG(ERROR) << \"failed to enqueue forward pooling on stream: \"\n 3074                 << ToString(status);\n ....\n 3078  }\n 3079  \n 3080: bool CudnnSupport::DoPoolForward(\n 3081      Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n 3082      const dnn::BatchDescriptor& input_dimensions,\n ....\n 3085      DeviceMemory<Eigen::half>* output_data) {\n 3086    mutex_lock lock{dnn_handle_mutex_};\n 3087:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3088                                          AsCUDAStreamValue(stream));\n 3089:   if (status != CUDNN_STATUS_SUCCESS) {\n 3090:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 3091      return false;\n 3092    }\n ....\n 3097    float beta = 0.0;\n 3098  \n 3099:   ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_HALF};\n 3100:   ScopedTensorDescriptor dest_desc{parent_, output_dimensions, CUDNN_DATA_HALF};\n 3101    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 3102:   status = dynload::cudnnPoolingForward(\n 3103        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 3104        src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(),\n 3105        output_data->opaque());\n 3106:   if (status != CUDNN_STATUS_SUCCESS) {\n 3107      LOG(ERROR) << \"failed to enqueue forward pooling on stream: \"\n 3108                 << ToString(status);\n ....\n 3112  }\n 3113  \n 3114: bool CudnnSupport::DoPoolBackward(\n 3115      Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n 3116      const dnn::BatchDescriptor& input_dimensions,\n ....\n 3121      DeviceMemory<float>* output_diff_data) {\n 3122    mutex_lock lock{dnn_handle_mutex_};\n 3123:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3124                                          AsCUDAStreamValue(stream));\n 3125:   if (status != CUDNN_STATUS_SUCCESS) {\n 3126:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 3127      return false;\n 3128    }\n ....\n 3133    float beta = 0.0;\n 3134  \n 3135:   ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_FLOAT};\n 3136    ScopedTensorDescriptor dest_desc{parent_, output_dimensions,\n 3137:                                    CUDNN_DATA_FLOAT};\n 3138    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 3139:   status = dynload::cudnnPoolingBackward(\n 3140        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 3141        dest_desc.handle(), output_data.opaque(), dest_desc.handle(),\n 3142        input_diff_data.opaque(), src_desc.handle(), input_data.opaque(), &beta,\n 3143        src_desc.handle(), output_diff_data->opaque());\n 3144:   if (status != CUDNN_STATUS_SUCCESS) {\n 3145      LOG(ERROR) << \"failed to enqueue backward pooling on stream: \"\n 3146                 << ToString(status);\n ....\n 3150  }\n 3151  \n 3152: bool CudnnSupport::DoPoolBackward(\n 3153      Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n 3154      const dnn::BatchDescriptor& input_dimensions,\n ....\n 3159      DeviceMemory<Eigen::half>* output_diff_data) {\n 3160    mutex_lock lock{dnn_handle_mutex_};\n 3161:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3162                                          AsCUDAStreamValue(stream));\n 3163:   if (status != CUDNN_STATUS_SUCCESS) {\n 3164:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 3165      return false;\n 3166    }\n ....\n 3171    float beta = 0.0;\n 3172  \n 3173:   ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_HALF};\n 3174:   ScopedTensorDescriptor dest_desc{parent_, output_dimensions, CUDNN_DATA_HALF};\n 3175    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 3176:   status = dynload::cudnnPoolingBackward(\n 3177        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 3178        dest_desc.handle(), output_data.opaque(), dest_desc.handle(),\n 3179        input_diff_data.opaque(), src_desc.handle(), input_data.opaque(), &beta,\n 3180        src_desc.handle(), output_diff_data->opaque());\n 3181:   if (status != CUDNN_STATUS_SUCCESS) {\n 3182      LOG(ERROR) << \"failed to enqueue backward pooling on stream: \"\n 3183                 << ToString(status);\n ....\n 3187  }\n 3188  \n 3189: bool CudnnSupport::DoNormalize(\n 3190      Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n 3191      const DeviceMemory<float>& input_data, DeviceMemory<float>* output_data) {\n ....\n 3193  }\n 3194  \n 3195: bool CudnnSupport::DoNormalizeWithDimensions(\n 3196      Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n 3197      const dnn::BatchDescriptor& dimensions,\n ....\n 3209    // Launch the normalization.\n 3210    mutex_lock lock{dnn_handle_mutex_};\n 3211:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3212                                          AsCUDAStreamValue(stream));\n 3213:   if (status != CUDNN_STATUS_SUCCESS) {\n 3214:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 3215      return false;\n 3216    }\n 3217  \n 3218:   ScopedTensorDescriptor dims{parent_, dimensions, CUDNN_DATA_FLOAT};\n 3219    ScopedNormalizeDescriptor normalize{parent_, normalize_descriptor};\n 3220  \n ....\n 3224    float beta = 0.0f;\n 3225  \n 3226:   status = dynload::cudnnLRNCrossChannelForward(\n 3227        parent_, ToHandle(dnn_handle_), normalize.handle(),\n 3228:       CUDNN_LRN_CROSS_CHANNEL_DIM1, &alpha, dims.handle(), input_data.opaque(),\n 3229        &beta, dims.handle(), output_data->opaque());\n 3230:   if (status != CUDNN_STATUS_SUCCESS) {\n 3231:     LOG(ERROR) << \"failed to run cudnnLRNCrossChannelForward\";\n 3232      return false;\n 3233    }\n ....\n 3235  }\n 3236  \n 3237: bool CudnnSupport::DoNormalizeBackwardWithDimensions(\n 3238      Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n 3239      const dnn::BatchDescriptor& dimensions, const DeviceMemory<float>& raw_data,\n ....\n 3252  \n 3253    mutex_lock lock{dnn_handle_mutex_};\n 3254:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3255                                          AsCUDAStreamValue(stream));\n 3256:   if (status != CUDNN_STATUS_SUCCESS) {\n 3257:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 3258      return false;\n 3259    }\n 3260  \n 3261:   ScopedTensorDescriptor dims{parent_, dimensions, CUDNN_DATA_FLOAT};\n 3262    ScopedNormalizeDescriptor normalize{parent_, normalize_descriptor};\n 3263  \n ....\n 3265    float beta = 0.0f;\n 3266  \n 3267:   status = dynload::cudnnLRNCrossChannelBackward(\n 3268        parent_, ToHandle(dnn_handle_), normalize.handle(),\n 3269:       CUDNN_LRN_CROSS_CHANNEL_DIM1, &alpha, dims.handle(),\n 3270        normalized_data.opaque(), dims.handle(),\n 3271        normalized_variable_gradient.opaque(), dims.handle(), raw_data.opaque(),\n 3272        &beta, dims.handle(), raw_variable_gradient->opaque());\n 3273:   if (status != CUDNN_STATUS_SUCCESS) {\n 3274:     LOG(ERROR) << \"failed to run cudnnLRNCrossChannelBackward\";\n 3275      return false;\n 3276    }\n ....\n 3278  }\n 3279  \n 3280: bool CudnnSupport::DoDepthConcatenate(\n 3281      Stream* stream, port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n 3282      port::ArraySlice<const DeviceMemory<float>*> input_data,\n ....\n 3286    for (const auto& dimensions : input_dimensions) {\n 3287      if (dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n 3288:       LOG(ERROR) << \"CudnnSupport::DoDepthConcatenate currently only \"\n 3289                      \"supports the kBatchDepthYX layout.\";\n 3290        return false;\n ....\n 3330  }\n 3331  \n 3332: bool CudnnSupport::DoElementwiseOperate(\n 3333      Stream* stream, dnn::ElementwiseOperation operation,\n 3334      port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n ....\n 3340  }\n 3341  \n 3342: bool CudnnSupport::DoXYPad(Stream* stream,\n 3343                             const dnn::BatchDescriptor& dimensions,\n 3344                             const DeviceMemory<float>& input_data,\n ....\n 3349  }\n 3350  \n 3351: bool CudnnSupport::DoXYSlice(Stream* stream,\n 3352                               const dnn::BatchDescriptor& dimensions,\n 3353                               const DeviceMemory<float>& input_data,\n ....\n 3359  }\n 3360  \n 3361: bool CudnnSupport::DoMemcpyD2HQuantized(\n 3362      Stream* stream, const DeviceMemory<float>& gpu_unquantized_src,\n 3363      dnn::QuantizedActivationMode mode, void* host_dst, int64 size) {\n 3364:   LOG(ERROR) << \"quantized memcpy not supported by cuDNN\";\n 3365    return false;\n 3366  }\n 3367  \n 3368: bool CudnnSupport::DoMemcpyH2DQuantized(\n 3369      Stream* stream, const void* host_src, int64 size,\n 3370      dnn::QuantizedActivationMode mode,\n 3371      DeviceMemory<float>* gpu_unquantized_dst) {\n 3372:   LOG(ERROR) << \"quantized memcpy not supported by cuDNN\";\n 3373    return false;\n 3374  }\n 3375  \n 3376: bool CudnnSupport::DeriveOutputBatchDescriptor(\n 3377      const BatchDescriptor& batch_descriptor,\n 3378      const FilterDescriptor& filter_descriptor,\n 3379      const dnn::ConvolutionDescriptor& convolution_descriptor,\n 3380      dnn::BatchDescriptor* output_batch_descriptor) {\n 3381:   ScopedTensorDescriptor input_nd{parent_, batch_descriptor, CUDNN_DATA_FLOAT};\n 3382    ScopedFilterDescriptor filter{parent_, filter_descriptor, batch_descriptor,\n 3383:                                 CUDNN_DATA_FLOAT};\n 3384    ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n 3385:                                    CUDNN_DATA_FLOAT};\n 3386  \n 3387    int dn = batch_descriptor.ndims() + 2;\n 3388    std::vector<int> dims(dn);  // in BDYX\n 3389:   auto status = dynload::cudnnGetConvolutionNdForwardOutputDim(\n 3390        parent_, conv.handle(), input_nd.handle(), filter.handle(), dn,\n 3391        dims.data());\n 3392:   if (status != CUDNN_STATUS_SUCCESS) {\n 3393      LOG(ERROR) << \"could not get output tensor for convolution: \"\n 3394                 << ToString(status);\n ....\n 3412  namespace gpu = ::perftools::gputools;\n 3413  \n 3414: void initialize_cudnn() {\n 3415    gpu::port::Status status =\n 3416        gpu::PluginRegistry::Instance()\n 3417            ->RegisterFactory<gpu::PluginRegistry::DnnFactory>(\n 3418:               gpu::cuda::kCudaPlatformId, gpu::cuda::kCuDnnPlugin, \"cuDNN\",\n 3419                [](gpu::internal::StreamExecutorInterface*\n 3420                       parent) -> gpu::dnn::DnnSupport* {\n ....\n 3428                  }\n 3429  \n 3430:                 gpu::cuda::CudnnSupport* dnn =\n 3431:                     new gpu::cuda::CudnnSupport(cuda_executor);\n 3432                  if (!dnn->Init().ok()) {\n 3433                    // Note: Init() will log a more specific error.\n ....\n 3439  \n 3440    if (!status.ok()) {\n 3441:     LOG(ERROR) << \"Unable to register cuDNN factory: \"\n 3442                 << status.error_message();\n 3443    }\n 3444  \n 3445:   // Prime the cuDNN DSO. The loader will log more information.\n 3446:   auto statusor = gpu::internal::CachedDsoLoader::GetCudnnDsoHandle();\n 3447    if (!statusor.ok()) {\n 3448:     LOG(INFO) << \"Unable to load cuDNN DSO\";\n 3449    }\n 3450  \n 3451    gpu::PluginRegistry::Instance()->SetDefaultFactory(gpu::cuda::kCudaPlatformId,\n 3452                                                       gpu::PluginKind::kDnn,\n 3453:                                                      gpu::cuda::kCuDnnPlugin);\n 3454  }\n 3455  \n ....\n 3457  }  // namespace perftools\n 3458  \n 3459: REGISTER_MODULE_INITIALIZER(register_cudnn,\n 3460:                             { perftools::gputools::initialize_cudnn(); });\n 3461  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.h:\n   32  \n   33  class CUDAExecutor;\n   34: class CudnnRnnDescriptor;\n   35: class CudnnRnnSequenceTensorDescriptor;\n   36: class CudnnRnnStateTensorDescriptor;\n   37  \n   38: // Opaque and unique identifier for the cuDNN plugin.\n   39: extern const PluginId kCuDnnPlugin;\n   40  \n   41: // cudnn-library based DNN support. For details on overridden interface\n   42  // functions, see dnn.h.\n   43: class CudnnSupport : public dnn::DnnSupport {\n   44   public:\n   45:   explicit CudnnSupport(CUDAExecutor* parent);\n   46:   ~CudnnSupport() override;\n   47  \n   48    port::Status Init() override;\n   ..\n  174        const dnn::BatchDescriptor& output_descriptor,\n  175        DeviceMemory<float>* output_data) override {\n  176:     LOG(ERROR) << \"separable convolution not supported by CUDNN\";\n  177      return false;\n  178    }\n  ...\n  256                           const dnn::BatchDescriptor& output_dimensions,\n  257                           DeviceMemory<float>* output_data) override {\n  258:     LOG(ERROR) << \"DNN MatMulQuantized not supported by CUDNN\";\n  259      return false;\n  260    }\n  ...\n  266                           const dnn::BatchDescriptor& output_dimensions,\n  267                           DeviceMemory<float>* output_data) override {\n  268:     LOG(ERROR) << \"DNN MatMulQuantized not supported by CUDNN\";\n  269      return false;\n  270    }\n  ...\n  377    CUDAExecutor* parent_;  // Parent executor object. Not owned.\n  378  \n  379:   // cudnn library handle. cudnnHandle_t type is not present in this header to\n  380    // prevent third-party library header inclusions from leaking outside the\n  381    // single cuda_dnn translation unit.\n  382    void* dnn_handle_ GUARDED_BY(dnn_handle_mutex_);\n  383  \n  384:   // NOTE(keveman): Temporary data layout transformation until cuDNN supports\n  385    // kBatchYXDepth for backward pass. This function allocates temporary memory,\n  386    // lays out the source data into the temporary but in the kBatchDepthXY\n  ...\n  395    DeviceMemory<T> MaybeTransformLayout(\n  396        Stream* stream,\n  397:       int cudnn_type,  // Actually cudnnDataType_t.\n  398        dnn::BatchDescriptor* output_descriptor,\n  399        DeviceMemory<T> backward_output_data,\n  ...\n  417    template <class T>\n  418    bool DoBatchNormalizationBackwardImpl(\n  419:       Stream* stream, int cudnn_type, const DeviceMemory<T>& y_backprop,\n  420        const DeviceMemory<T>& x, const DeviceMemory<T>& scale,\n  421        const DeviceMemory<T>& mean, const DeviceMemory<T>& variance,\n  ...\n  427    template <class T>\n  428    bool DoConvolveImpl(Stream* stream,\n  429:                       int cudnn_type,  // Actually cudnnDataType_t.\n  430                        const dnn::BatchDescriptor& batch_descriptor,\n  431                        const DeviceMemory<T>& input_data,\n  ...\n  442    bool DoConvolveBackwardDataImpl(\n  443        Stream* stream,\n  444:       int cudnn_type,  // Actually cudnnDataType_t.\n  445        const dnn::FilterDescriptor& filter_descriptor,\n  446        const DeviceMemory<T>& filter_data,\n  ...\n  455    template <class T>\n  456    bool DoConvolveBackwardFilterImpl(\n  457:       Stream* stream, int cudnn_type,  // Actually cudnnDataType_t.\n  458        const dnn::BatchDescriptor& input_descriptor,\n  459        const DeviceMemory<T>& input_data,\n  ...\n  469    template <class T>\n  470    bool DoConvolveBackwardBiasImpl(Stream* stream,\n  471:                                   int cudnn_type,  // Actually cudnnDataType_t.\n  472                                    const dnn::BatchDescriptor& input_descriptor,\n  473                                    const DeviceMemory<T>& input_data,\n  ...\n  476  \n  477    template <class T>\n  478:   bool DoRnnForwardImpl(Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n  479:                         const CudnnRnnSequenceTensorDescriptor& input_desc,\n  480                          const DeviceMemory<T>& input_data,\n  481:                         const CudnnRnnStateTensorDescriptor& input_h_desc,\n  482                          const DeviceMemory<T>& input_h_data,\n  483:                         const CudnnRnnStateTensorDescriptor& input_c_desc,\n  484                          const DeviceMemory<T>& input_c_data,\n  485                          const DeviceMemory<T>& params,\n  486:                         const CudnnRnnSequenceTensorDescriptor& output_desc,\n  487                          DeviceMemory<T>* output_data,\n  488:                         const CudnnRnnStateTensorDescriptor& output_h_desc,\n  489                          DeviceMemory<T>* output_h_data,\n  490:                         const CudnnRnnStateTensorDescriptor& output_c_desc,\n  491                          DeviceMemory<T>* output_c_data, bool is_training,\n  492                          ScratchAllocator* reserve_space_allocator,\n  ...\n  494  \n  495    template <class T>\n  496:   bool DoRnnBackwardImpl(Stream* stream, const CudnnRnnDescriptor& rnn_desc,\n  497:                          const CudnnRnnSequenceTensorDescriptor& input_desc,\n  498                           const DeviceMemory<T>& input_data,\n  499:                          const CudnnRnnStateTensorDescriptor& input_h_desc,\n  500                           const DeviceMemory<T>& input_h_data,\n  501:                          const CudnnRnnStateTensorDescriptor& input_c_desc,\n  502                           const DeviceMemory<T>& input_c_data,\n  503                           const DeviceMemory<T>& params,\n  504:                          const CudnnRnnSequenceTensorDescriptor& output_desc,\n  505                           const DeviceMemory<T>& output_data,\n  506:                          const CudnnRnnStateTensorDescriptor& output_h_desc,\n  507                           const DeviceMemory<T>& output_h_data,\n  508:                          const CudnnRnnStateTensorDescriptor& output_c_desc,\n  509                           const DeviceMemory<T>& output_c_data,\n  510                           const DeviceMemory<float>& output_backprop_data,\n  ...\n  518                           ScratchAllocator* workspace_allocator);\n  519  \n  520:   SE_DISALLOW_COPY_AND_ASSIGN(CudnnSupport);\n  521  };\n  522  \n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\dnn.h:\n   17  //\n   18  // This is an abstract interface for a platform to optionally support common\n   19: // neural net operations; it accommodates implementations such as the cudnn\n   20  // library operations.\n   21  \n   ..\n   50    kYXBatchDepth,      // Same as dist_belief::DF_BATCH_MAJOR.\n   51    kBatchYXDepth,      // Same as run_brain output, and tensorflow's layout.\n   52:   kBatchDepthYX,      // cuDNN's NCHW layout, data laid out as image, feature,\n   53                        // maps, rows, columns.\n   54  };\n   ..\n  327  // Specify int64 so there's no padding in FilterDescriptor.\n  328  enum class FilterLayout : int64 {\n  329:   kOutputInputYX = 0,  // cuDNN's default filter layout, laid out as:\n  330                         // (major) output feature maps >> input feature maps >>\n  331                         // rows >> columns (minor).\n  ...\n  517    std::vector<int64> filter_strides_;\n  518    int ndims_;\n  519:   // TODO(leary) cudnn provides these fields, but need to characterize what\n  520    // their effect is -- they may be boolean rather than integral.\n  521    // int64 upscale_input_x;\n  ...\n  813    //  is_training: Set to true for training, false for inference.\n  814    //  var_to_inv_var: a function to convert the variance to inverted variance\n  815:   //    for cuDNN v4 forward inference.\n  816    //  inv_var_to_var: a function to convert the inverted variance to\n  817:   //    variance for cuDNN v4 forward training, to be used for TensorFlow\n  818    //    to calculate the running variance.\n  819    virtual bool DoBatchNormalizationForward(\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:\n   43  \n   44  string GetCudaVersion() { return TF_CUDA_VERSION; }\n   45: string GetCudnnVersion() { return TF_CUDNN_VERSION; }\n   46  \n   47  /* static */ port::Status DsoLoader::GetCublasDsoHandle(void** dso_handle) {\n   ..\n   52  }\n   53  \n   54: /* static */ port::Status DsoLoader::GetCudnnDsoHandle(void** dso_handle) {\n   55:   // libcudnn is versioned differently than the other libraries and may have a\n   56    // different version number than other CUDA libraries.  See b/22397368 for\n   57    // some details about the complications surrounding this.\n   58    return GetDsoHandle(FindDsoPath(tensorflow::internal::FormatLibraryFileName(\n   59:                                       \"cudnn\", GetCudnnVersion()),\n   60                                    GetCudaLibraryDirPath()),\n   61                        dso_handle);\n   ..\n  230  }\n  231  \n  232: /* static */ port::StatusOr<void*> CachedDsoLoader::GetCudnnDsoHandle() {\n  233    static port::StatusOr<void*> result =\n  234:       FetchHandleResult(DsoLoader::GetCudnnDsoHandle);\n  235    return result;\n  236  }\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\dso_loader.h:\n   45  \n   46    static port::Status GetCublasDsoHandle(void** dso_handle);\n   47:   static port::Status GetCudnnDsoHandle(void** dso_handle);\n   48    static port::Status GetCufftDsoHandle(void** dso_handle);\n   49    static port::Status GetCurandDsoHandle(void** dso_handle);\n   ..\n  107    // Cached versions of the corresponding DsoLoader methods above.\n  108    static port::StatusOr<void*> GetCublasDsoHandle();\n  109:   static port::StatusOr<void*> GetCudnnDsoHandle();\n  110    static port::StatusOr<void*> GetCufftDsoHandle();\n  111    static port::StatusOr<void*> GetCurandDsoHandle();\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\multi_platform_manager.h:\n   58  //\n   59  //    //perftools/gputools/executor/cuda:pluton_blas_plugin\n   60: //    //perftools/gputools/executor/cuda:cudnn_plugin\n   61  //    //perftools/gputools/executor/cuda:cublas_plugin\n   62  //    //perftools/gputools/executor/cuda:curand_plugin\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\tools\\ci_build\\Dockerfile.gpu:\n    1: FROM nvidia/cuda:7.5-cudnn5-devel\n    2  \n    3  MAINTAINER Jan Prach <jendap@google.com>\n    .\n   23  # Configure the build for our CUDA configuration.\n   24  ENV CUDA_TOOLKIT_PATH /usr/local/cuda\n   25: ENV CUDNN_INSTALL_PATH /usr/lib/x86_64-linux-gnu\n   26  ENV TF_NEED_CUDA 1\n   27  ENV TF_CUDA_COMPUTE_CAPABILITIES 3.0,5.2\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\tools\\docker\\Dockerfile.devel-gpu:\n    1: FROM nvidia/cuda:7.5-cudnn5-devel\n    2  \n    3  MAINTAINER Craig Citro <craigcitro@google.com>\n    .\n   88  ENV CUDA_PATH /usr/local/cuda\n   89  ENV CUDA_TOOLKIT_PATH /usr/local/cuda\n   90: ENV CUDNN_INSTALL_PATH /usr/lib/x86_64-linux-gnu\n   91  ENV LD_LIBRARY_PATH /usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\n   92  ENV TF_NEED_CUDA 1\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\tools\\docker\\Dockerfile.gpu:\n    1: FROM nvidia/cuda:7.5-cudnn5-devel\n    2  \n    3  MAINTAINER Craig Citro <craigcitro@google.com>\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda\\BUILD.tpl:\n    3  load(\"@local_config_cuda//cuda:platform.bzl\", \"cuda_library_path\")\n    4  load(\"@local_config_cuda//cuda:platform.bzl\", \"cuda_static_library_path\")\n    5: load(\"@local_config_cuda//cuda:platform.bzl\", \"cudnn_library_path\")\n    6  load(\"@local_config_cuda//cuda:platform.bzl\", \"cupti_library_path\")\n    7  load(\"@local_config_cuda//cuda:platform.bzl\", \"readlink_command\")\n    .\n   93  \n   94  cc_library(\n   95:     name = \"cudnn\",\n   96      srcs = [\n   97:         cudnn_library_path(),\n   98      ],\n   99      data = [\n  100:         cudnn_library_path(),\n  101      ],\n  102      includes = [\"include/\"],\n  ...\n  137          \":cudart\",\n  138          \":cublas\",\n  139:         \":cudnn\",\n  140          \":cufft\",\n  141          \":curand\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda\\cuda_config.h.tpl:\n   20  \n   21  #define TF_CUDA_VERSION \"%{cuda_version}\"\n   22: #define TF_CUDNN_VERSION \"%{cudnn_version}\"\n   23  \n   24  #endif  // CUDA_CUDA_CONFIG_H_\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda\\platform.bzl.tpl:\n    1  CUDA_VERSION = \"%{cuda_version}\"\n    2: CUDNN_VERSION = \"%{cudnn_version}\"\n    3  PLATFORM = \"%{platform}\"\n    4  \n    .\n    6    return CUDA_VERSION\n    7  \n    8: def cudnn_sdk_version():\n    9:   return CUDNN_VERSION\n   10  \n   11  def cuda_library_path(name, version = cuda_sdk_version()):\n   ..\n   27      return \"lib64/lib{}_static.a\".format(name)\n   28  \n   29: def cudnn_library_path(version = cudnn_sdk_version()):\n   30    if PLATFORM == \"Darwin\":\n   31      if not version:\n   32:       return \"lib/libcudnn.dylib\"\n   33      else:\n   34:       return \"lib/libcudnn.{}.dylib\".format(version)\n   35    else:\n   36      if not version:\n   37:       return \"lib64/libcudnn.so\"\n   38      else:\n   39:       return \"lib64/libcudnn.so.{}\".format(version)\n   40  \n   41  def cupti_library_path(version = cuda_sdk_version()):\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda_configure.bzl:\n   10    * `TF_CUDA_VERSION`: The version of the CUDA toolkit. If this is blank, then\n   11      use the system default.\n   12:   * `TF_CUDNN_VERSION`: The version of the cuDNN library.\n   13:   * `CUDNN_INSTALL_PATH`: The path to the cuDNN library. Default is\n   14      `/usr/local/cuda`.\n   15    * `TF_CUDA_COMPUTE_CAPABILITIES`: The CUDA compute capabilities. Default is\n   ..\n   20  _CUDA_TOOLKIT_PATH = \"CUDA_TOOLKIT_PATH\"\n   21  _TF_CUDA_VERSION = \"TF_CUDA_VERSION\"\n   22: _TF_CUDNN_VERSION = \"TF_CUDNN_VERSION\"\n   23: _CUDNN_INSTALL_PATH = \"CUDNN_INSTALL_PATH\"\n   24  _TF_CUDA_COMPUTE_CAPABILITIES = \"TF_CUDA_COMPUTE_CAPABILITIES\"\n   25  \n   26  _DEFAULT_CUDA_VERSION = \"\"\n   27: _DEFAULT_CUDNN_VERSION = \"\"\n   28  _DEFAULT_CUDA_TOOLKIT_PATH = \"/usr/local/cuda\"\n   29: _DEFAULT_CUDNN_INSTALL_PATH = \"/usr/local/cuda\"\n   30  _DEFAULT_CUDA_COMPUTE_CAPABILITIES = [\"3.5\", \"5.2\"]\n   31  \n   ..\n  149  \n  150  \n  151: def _cudnn_install_basedir(repository_ctx):\n  152:   \"\"\"Finds the cudnn install directory.\"\"\"\n  153:   cudnn_install_path = _DEFAULT_CUDNN_INSTALL_PATH\n  154:   if _CUDNN_INSTALL_PATH in repository_ctx.os.environ:\n  155:     cudnn_install_path = repository_ctx.os.environ[_CUDNN_INSTALL_PATH].strip()\n  156:   if not repository_ctx.path(cudnn_install_path).exists:\n  157:     auto_configure_fail(\"Cannot find cudnn install path.\")\n  158:   return cudnn_install_path\n  159  \n  160  \n  ...\n  167  \n  168  \n  169: def _cudnn_version(repository_ctx):\n  170:   \"\"\"Detects the cudnn version.\"\"\"\n  171:   if _TF_CUDNN_VERSION in repository_ctx.os.environ:\n  172:     return repository_ctx.os.environ[_TF_CUDNN_VERSION].strip()\n  173    else:\n  174      return \"\"\n  ...\n  201  \n  202  \n  203: def _cuda_symlink_files(cpu_value, cuda_version, cudnn_version):\n  204    \"\"\"Returns a struct containing platform-specific paths.\n  205  \n  ...\n  207      cpu_value: The string representing the host OS.\n  208      cuda_version: The cuda version as returned by _cuda_version\n  209:     cudnn_version: The cudnn version as returned by _cudnn_version\n  210    \"\"\"\n  211    cuda_ext = \".%s\" % cuda_version if cuda_version else \"\"\n  212:   cudnn_ext = \".%s\" % cudnn_version if cudnn_version else \"\"\n  213    if cpu_value == \"Linux\":\n  214      return struct(\n  ...\n  217          cuda_rt_lib_static = \"lib64/libcudart_static.a\",\n  218          cuda_blas_lib = \"lib64/libcublas.so%s\" % cuda_ext,\n  219:         cuda_dnn_lib = \"lib64/libcudnn.so%s\" % cudnn_ext,\n  220:         cuda_dnn_lib_alt = \"libcudnn.so%s\" % cudnn_ext,\n  221          cuda_rand_lib = \"lib64/libcurand.so%s\" % cuda_ext,\n  222          cuda_fft_lib = \"lib64/libcufft.so%s\" % cuda_ext,\n  ...\n  228          cuda_rt_lib_static = \"lib/libcudart_static.a\",\n  229          cuda_blas_lib = \"lib/libcublas%s.dylib\" % cuda_ext,\n  230:         cuda_dnn_lib = \"lib/libcudnn%s.dylib\" % cudnn_ext,\n  231:         cuda_dnn_lib_alt = \"libcudnn%s.dylib\" % cudnn_ext,\n  232          cuda_rand_lib = \"lib/libcurand%s.dylib\" % cuda_ext,\n  233          cuda_fft_lib = \"lib/libcufft%s.dylib\" % cuda_ext,\n  ...\n  239          cuda_rt_lib_static = \"lib/cudart_static.lib\",\n  240          cuda_blas_lib = \"lib/cublas%s.dll\" % cuda_ext,\n  241:         cuda_dnn_lib = \"lib/cudnn%s.dll\" % cudnn_ext,\n  242:         cuda_dnn_lib_alt = \"cudnn%s.dll\" % cudnn_ext,\n  243          cuda_rand_lib = \"lib/curand%s.dll\" % cuda_ext,\n  244          cuda_fft_lib = \"lib/cufft%s.dll\" % cuda_ext,\n  ...\n  272  \n  273  \n  274: def _find_cudnn_header_dir(repository_ctx, cudnn_install_basedir):\n  275:   \"\"\"Returns the path to the directory containing cudnn.h\n  276  \n  277    Args:\n  278      repository_ctx: The repository context.\n  279:     cudnn_install_basedir: The cudnn install directory as returned by\n  280:       _cudnn_install_basedir.\n  281  \n  282    Returns:\n  283:     The path of the directory containing the cudnn header.\n  284    \"\"\"\n  285:   if repository_ctx.path(cudnn_install_basedir + \"/cudnn.h\").exists:\n  286:     return cudnn_install_basedir\n  287:   if repository_ctx.path(cudnn_install_basedir + \"/include/cudnn.h\").exists:\n  288:     return cudnn_install_basedir + \"/include\"\n  289:   if repository_ctx.path(\"/usr/include/cudnn.h\").exists:\n  290      return \"/usr/include\"\n  291:   auto_configure_fail(\"Cannot find cudnn.h under %s\" % cudnn_install_basedir)\n  292  \n  293  \n  294: def _find_cudnn_lib_path(repository_ctx, cudnn_install_basedir, symlink_files):\n  295:   \"\"\"Returns the path to the directory containing libcudnn\n  296  \n  297    Args:\n  298      repository_ctx: The repository context.\n  299:     cudnn_install_basedir: The cudnn install dir as returned by\n  300:       _cudnn_install_basedir.\n  301      symlink_files: The symlink files as returned by _cuda_symlink_files.\n  302  \n  303    Returns:\n  304:     The path of the directory containing the cudnn libraries.\n  305    \"\"\"\n  306:   lib_dir = cudnn_install_basedir + \"/\" + symlink_files.cuda_dnn_lib\n  307    if repository_ctx.path(lib_dir).exists:\n  308      return lib_dir\n  309:   alt_lib_dir = cudnn_install_basedir + \"/\" + symlink_files.cuda_dnn_lib_alt\n  310    if repository_ctx.path(alt_lib_dir).exists:\n  311      return alt_lib_dir\n  ...\n  313    auto_configure_fail(\"Cannot find %s or %s under %s\" %\n  314         (symlink_files.cuda_dnn_lib, symlink_files.cuda_dnn_lib_alt,\n  315:         cudnn_install_basedir))\n  316  \n  317  \n  ...\n  362    cpu_value = _cpu_value(repository_ctx)\n  363    symlink_files = _cuda_symlink_files(cpu_value, _DEFAULT_CUDA_VERSION,\n  364:                                       _DEFAULT_CUDNN_VERSION)\n  365  \n  366    # Set up BUILD file for cuda/.\n  ...\n  370         {\n  371             \"%{cuda_version}\": _DEFAULT_CUDA_VERSION,\n  372:            \"%{cudnn_version}\": _DEFAULT_CUDNN_VERSION,\n  373             \"%{platform}\": cpu_value,\n  374         })\n  ...\n  378    repository_ctx.file(\"cuda/include/cuda.h\", \"\")\n  379    repository_ctx.file(\"cuda/include/cublas.h\", \"\")\n  380:   repository_ctx.file(\"cuda/include/cudnn.h\", \"\")\n  381    repository_ctx.file(\"cuda/extras/CUPTI/include/cupti.h\", \"\")\n  382    repository_ctx.file(\"cuda/%s\" % symlink_files.cuda_rt_lib, \"\")\n  ...\n  393         {\n  394             \"%{cuda_version}\": _DEFAULT_CUDA_VERSION,\n  395:            \"%{cudnn_version}\": _DEFAULT_CUDNN_VERSION,\n  396             \"%{cuda_compute_capabilities}\": \",\".join([\n  397                 \"CudaVersion(\\\"%s\\\")\" % c\n  ...\n  423    cuda_version = _cuda_version(repository_ctx)\n  424    cuda_toolkit_path = _cuda_toolkit_path(repository_ctx, cuda_version)\n  425:   cudnn_install_basedir = _cudnn_install_basedir(repository_ctx)\n  426:   cudnn_version = _cudnn_version(repository_ctx)\n  427    compute_capabilities = _compute_capabilities(repository_ctx)\n  428  \n  429    cpu_value = _cpu_value(repository_ctx)\n  430:   symlink_files = _cuda_symlink_files(cpu_value, cuda_version, cudnn_version)\n  431    _check_lib(repository_ctx, cuda_toolkit_path, symlink_files.cuda_rt_lib)\n  432    _check_lib(repository_ctx, cuda_toolkit_path, symlink_files.cuda_cupti_lib)\n  433:   _check_dir(repository_ctx, cudnn_install_basedir)\n  434  \n  435:   cudnn_header_dir = _find_cudnn_header_dir(repository_ctx,\n  436:                                             cudnn_install_basedir)\n  437:   cudnn_lib_path = _find_cudnn_lib_path(repository_ctx, cudnn_install_basedir,\n  438                                          symlink_files)\n  439  \n  ...\n  452                           \"cuda/\" + symlink_files.cuda_cupti_lib)\n  453  \n  454:   # Set up the symbolic links for cudnn if cudnn was was not installed to\n  455    # CUDA_TOOLKIT_PATH.\n  456:   if not repository_ctx.path(\"cuda/include/cudnn.h\").exists:\n  457:     repository_ctx.symlink(cudnn_header_dir + \"/cudnn.h\",\n  458:                            \"cuda/include/cudnn.h\")\n  459    if not repository_ctx.path(\"cuda/\" + symlink_files.cuda_dnn_lib).exists:\n  460:     repository_ctx.symlink(cudnn_lib_path, \"cuda/\" + symlink_files.cuda_dnn_lib)\n  461  \n  462    # Set up BUILD file for cuda/\n  ...\n  466         {\n  467             \"%{cuda_version}\": cuda_version,\n  468:            \"%{cudnn_version}\": cudnn_version,\n  469             \"%{platform}\": cpu_value,\n  470         })\n  ...\n  493         {\n  494             \"%{cuda_version}\": cuda_version,\n  495:            \"%{cudnn_version}\": cudnn_version,\n  496             \"%{cuda_compute_capabilities}\": \",\".join(\n  497                 [\"CudaVersion(\\\"%s\\\")\" % c for c in compute_capabilities]),\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\py\\numpy\\numpy_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_lib:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\configure:\n  153  done\n  154  \n  155: # Find out where the cuDNN library is installed\n  156  while true; do\n  157:   # Configure the Cudnn version to use.\n  158:   if [ -z \"$TF_CUDNN_VERSION\" ]; then\n  159:     read -p \"Please specify the Cudnn version you want to use. [Leave empty to use system default]: \" TF_CUDNN_VERSION\n  160    fi\n  161  \n  162    fromuser=\"\"\n  163:   if [ -z \"$CUDNN_INSTALL_PATH\" ]; then\n  164:     default_cudnn_path=${CUDA_TOOLKIT_PATH}\n  165:     read -p \"Please specify the location where cuDNN $TF_CUDNN_VERSION library is installed. Refer to README.md for more details. [Default is $default_cudnn_path]: \" CUDNN_INSTALL_PATH\n  166      fromuser=\"1\"\n  167:     if [ -z \"$CUDNN_INSTALL_PATH\" ]; then\n  168:       CUDNN_INSTALL_PATH=$default_cudnn_path\n  169      fi\n  170      # Result returned from \"read\" will be used unexpanded. That make \"~\" unuseable.\n  171      # Going through one more level of expansion to handle that.\n  172:     CUDNN_INSTALL_PATH=`${PYTHON_BIN_PATH} -c \"import os; print(os.path.realpath(os.path.expanduser('${CUDNN_INSTALL_PATH}')))\"`\n  173    fi\n  174  \n  175:   if [[ -z \"$TF_CUDNN_VERSION\" ]]; then\n  176:     TF_CUDNN_EXT=\"\"\n  177    else\n  178:     TF_CUDNN_EXT=\".$TF_CUDNN_VERSION\"\n  179    fi\n  180  \n  181    if [ \"$OSNAME\" == \"Linux\" ]; then\n  182:     CUDA_DNN_LIB_PATH=\"lib64/libcudnn.so${TF_CUDNN_EXT}\"\n  183:     CUDA_DNN_LIB_ALT_PATH=\"libcudnn.so${TF_CUDNN_EXT}\"\n  184    elif [ \"$OSNAME\" == \"Darwin\" ]; then\n  185:     CUDA_DNN_LIB_PATH=\"lib/libcudnn${TF_CUDNN_EXT}.dylib\"\n  186:     CUDA_DNN_LIB_ALT_PATH=\"libcudnn${TF_CUDNN_EXT}.dylib\"\n  187    fi\n  188  \n  189:   if [ -e \"$CUDNN_INSTALL_PATH/${CUDA_DNN_LIB_ALT_PATH}\" -o -e \"$CUDNN_INSTALL_PATH/${CUDA_DNN_LIB_PATH}\" ]; then\n  190      break\n  191    fi\n  192  \n  193    if [ \"$OSNAME\" == \"Linux\" ]; then\n  194:     CUDNN_PATH_FROM_LDCONFIG=\"$(ldconfig -p | sed -n 's/.*libcudnn.so .* => \\(.*\\)/\\1/p')\"\n  195:     if [ -e \"${CUDNN_PATH_FROM_LDCONFIG}${TF_CUDNN_EXT}\" ]; then\n  196:       CUDNN_INSTALL_PATH=\"$(dirname ${CUDNN_PATH_FROM_LDCONFIG})\"\n  197        break\n  198      fi\n  199    fi\n  200:   echo \"Invalid path to cuDNN ${CUDNN_VERSION} toolkit. Neither of the following two files can be found:\"\n  201:   echo \"${CUDNN_INSTALL_PATH}/${CUDA_DNN_LIB_PATH}\"\n  202:   echo \"${CUDNN_INSTALL_PATH}/${CUDA_DNN_LIB_ALT_PATH}\"\n  203    if [ \"$OSNAME\" == \"Linux\" ]; then\n  204:     echo \"${CUDNN_PATH_FROM_LDCONFIG}${TF_CUDNN_EXT}\"\n  205    fi\n  206  \n  ...\n  209    fi\n  210    # Retry\n  211:   TF_CUDNN_VERSION=\"\"\n  212:   CUDNN_INSTALL_PATH=\"\"\n  213  done\n  214  \n  ...\n  216  # CUDA_TOOLKIT_PATH refers to the CUDA toolkit.\n  217  CUDA_TOOLKIT_PATH=\"$CUDA_TOOLKIT_PATH\"\n  218: # CUDNN_INSTALL_PATH refers to the cuDNN toolkit. The cuDNN header and library\n  219  # files can be either in this directory, or under include/ and lib64/\n  220  # directories separately.\n  221: CUDNN_INSTALL_PATH=\"$CUDNN_INSTALL_PATH\"\n  222  \n  223  # The Cuda SDK version that should be used in this build (empty to use libcudart.so symlink)\n  224  TF_CUDA_VERSION=$TF_CUDA_VERSION\n  225  \n  226: # The Cudnn version that should be used in this build\n  227: TF_CUDNN_VERSION=$TF_CUDNN_VERSION\n  228  EOF\n  229  \n  ...\n  240  perl -pi -e \"s,CUDA_VERSION = \\\"[0-9\\.]*\\\",CUDA_VERSION = \\\"$TF_CUDA_VERSION\\\",s\" third_party/gpus/cuda/platform.bzl\n  241  \n  242: # Configure the Cudnn version to work with.\n  243: perl -pi -e \"s,(GetCudnnVersion.*return )\\\"[0-9\\.]*\\\",\\1\\\"$TF_CUDNN_VERSION\\\",s\" tensorflow/stream_executor/dso_loader.cc\n  244: perl -pi -e \"s,CUDNN_VERSION = \\\"[0-9\\.]*\\\",CUDNN_VERSION = \\\"$TF_CUDNN_VERSION\\\",s\" third_party/gpus/cuda/platform.bzl\n  245  \n  246  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\ISSUE_TEMPLATE.md:\n   11  Operating System:\n   12  \n   13: Installed version of CUDA and cuDNN: \n   14  (please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n   15  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\RELEASE.md:\n   23  ## Big Fixes and Other Changes\n   24  \n   25: * Turned on CuDNN Autotune.\n   26  * Added support for using third-party Python optimization algorithms (contrib.opt).\n   27  * Google Cloud Storage filesystem support.\n   ..\n   94  * Utility for inspecting checkpoints\n   95  * Basic tracing and timeline support\n   96: * Allow building against cuDNN 5 (not incl. RNN/LSTM support) \n   97  * Added instructions and binaries for ProtoBuf library with fast serialization and without 64MB limit\n   98  * Added special functions\n   ..\n  124  * Added gfile.Open and gfile.Copy, used by input_data.py.\n  125  * Fixed Saver bug when MakeDirs tried to create empty directory.\n  126: * GPU Pip wheels are built with cuda 7.5 and cudnn-v4, making them\n  127:   required for the binary releases. Lower versions of cuda/cudnn can\n  128    be supported by installing from sources and setting the options\n  129    during ./configure\n  ...\n  139  ## Major Features and Improvements\n  140  \n  141: * Allow using any installed Cuda >= 7.0 and cuDNN >= R2, and add support\n  142:   for cuDNN R4\n  143  * Added a `contrib/` directory for unsupported or experimental features, \n  144    including higher level `layers` module\n  ...\n  249  * Some improvements to GPU performance and memory usage:\n  250    [convnet benchmarks](https://github.com/soumith/convnet-benchmarks/issues/66)\n  251:   roughly equivalent with native cudnn v2 performance.  Improvements mostly due\n  252    to moving to 32-bit indices, faster shuffling kernels.  More improvements to\n  253    come in later releases.\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\contrib\\cmake\\tf_stream_executor.cmake:\n   23  #        \"//tensorflow/core:cuda\",\n   24  #        \"//third_party/gpus/cuda:cublas\",\n   25: #        \"//third_party/gpus/cuda:cudnn\",\n   26  #    ],\n   27  #    linkopts = [\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\contrib\\makefile\\tf_cc_files.txt:\n   91  tensorflow/core/util/work_sharder.cc\n   92  tensorflow/core/util/util.cc\n   93: tensorflow/core/util/use_cudnn.cc\n   94  tensorflow/core/util/tensor_slice_writer.cc\n   95  tensorflow/core/util/tensor_slice_set.cc\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\BUILD:\n  252          \"util/tensor_slice_reader_cache.h\",\n  253          \"util/tensor_slice_writer.h\",\n  254:         \"util/use_cudnn.h\",\n  255          \"util/util.h\",\n  256          \"util/work_sharder.h\",\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\avgpooling_op.cc:\n  359  #if GOOGLE_CUDA\n  360  \n  361: // A CUDNN based AvgPoolingGrad implementation. It includes the padding as the\n  362  // candidates for the pooling operation.\n  363  template <class T>\n  ...\n  423                              .TypeConstraint<float>(\"T\")\n  424                              .HostMemory(\"orig_input_shape\")\n  425:                             .Label(\"cudnn\"),\n  426                          AvgPoolingGradOp<GPUDevice, float>);\n  427  REGISTER_KERNEL_BUILDER(Name(\"AvgPoolGrad\")\n  ...\n  429                              .TypeConstraint<Eigen::half>(\"T\")\n  430                              .HostMemory(\"orig_input_shape\")\n  431:                             .Label(\"cudnn\"),\n  432                          AvgPoolingGradOp<GPUDevice, Eigen::half>);\n  433  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\BUILD:\n 1362      srcs = [\n 1363          \"avgpooling_op.cc\",\n 1364:         \"cudnn_pooling_gpu.cc\",\n 1365          \"maxpooling_op.cc\",\n 1366          \"pooling_ops_3d.cc\",\n ....\n 1369      hdrs = [\n 1370          \"avgpooling_op.h\",\n 1371:         \"cudnn_pooling_gpu.h\",\n 1372          \"maxpooling_op.h\",\n 1373          \"pooling_ops_common.h\",\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\conv_grad_ops.cc:\n   38  #include \"tensorflow/core/util/padding.h\"\n   39  #include \"tensorflow/core/util/tensor_format.h\"\n   40: #include \"tensorflow/core/util/use_cudnn.h\"\n   41  #include \"tensorflow/core/util/work_sharder.h\"\n   42  \n   ..\n  918          errors::InvalidArgument(\"Current implementation does not yet support \"\n  919                                  \"strides in the batch and depth dimensions.\"));\n  920:     OP_REQUIRES_OK(context, context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_));\n  921:     use_cudnn_ &= CanUseCudnn();\n  922:     cudnn_use_autotune_ = CudnnUseAutotune();\n  923      OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n  924    }\n  ...\n  965                                     dims.cols.input_size);\n  966  \n  967:     // TODO(keveman): cuDNN only supports equal padding on both sides, so only\n  968:     // calling it when that is true. Remove this check when (if?) cuDNN starts\n  969      // supporting different padding.\n  970      bool rows_odd = (padding_rows % 2 != 0);\n  ...\n  974      OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));\n  975  \n  976:     if (!use_cudnn_) {\n  977        context->SetStatus(errors::Unimplemented(\n  978            \"Conv2DBackpropInput for GPU is not currently supported \"\n  979:           \"without cudnn\"));\n  980        return;\n  981      }\n  ...\n 1016      if (rows_odd || cols_odd) {\n 1017        // If a padding dimension is odd, we have one more element on the right\n 1018:       // side or the bottom side. This is unsupported in cudnn. Therefore,\n 1019        // we pad that extra element and make it compatible.\n 1020        compatible_input_shape = ShapeFromFormat(\n ....\n 1049  \n 1050      // NOTE(keveman):\n 1051:     // cuDNN only supports the following layouts :\n 1052      // Input  : B x D x R x C\n 1053      // Filter : OD x ID x R x C\n ....\n 1058      // The first TransformDepth performs\n 1059      // (B x R x C x D) => (B x D x R x C).\n 1060:     // Since the tensor returned from cuDNN is B x D x R x C also,\n 1061      // the second TransformDepth performs\n 1062      // (B x D x R x C) => (B x R x C x D).\n ....\n 1113                         pre_transformed_in_backprop.template flat<T>().size());\n 1114  \n 1115:     static int64 ConvolveBackwardDataScratchSize = GetCudnnWorkspaceLimit(\n 1116:         \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32  // 4GB by default\n 1117          );\n 1118:     CudnnScratchAllocator scratch_allocator(ConvolveBackwardDataScratchSize,\n 1119                                              context);\n 1120      int device_id = stream->parent()->device_ordinal();\n ....\n 1134      };\n 1135      AlgorithmConfig algorithm_config;\n 1136:     if (cudnn_use_autotune_ &&\n 1137          !autotune_results_.Find(conv_parameters, &algorithm_config)) {\n 1138        std::vector<AlgorithmType> algorithms;\n ....\n 1143          // TODO(zhengxq): profile each algorithm multiple times to better\n 1144          // accuracy.\n 1145:         CudnnScratchAllocator scratch_allocator(ConvolveBackwardDataScratchSize,\n 1146                                                  context);\n 1147          ProfileResult profile_result;\n 1148:         bool cudnn_launch_status =\n 1149              stream\n 1150                  ->ThenConvolveBackwardDataWithAlgorithm(\n ....\n 1153                      AlgorithmConfig(profile_algorithm), &profile_result)\n 1154                  .ok();\n 1155:         if (cudnn_launch_status) {\n 1156            if (profile_result.is_valid()) {\n 1157              if (profile_result.elapsed_time_in_ms() <\n ....\n 1179        autotune_results_.Insert(conv_parameters, algorithm_config);\n 1180      }\n 1181:     bool cudnn_launch_status =\n 1182          stream\n 1183              ->ThenConvolveBackwardDataWithAlgorithm(\n ....\n 1187              .ok();\n 1188  \n 1189:     if (!cudnn_launch_status) {\n 1190        context->SetStatus(errors::Internal(\n 1191:           \"cuDNN Backward Data function launch failure : input shape(\",\n 1192            input_shape.DebugString(), \") filter shape(\",\n 1193            filter_shape.DebugString(), \")\"));\n ....\n 1233    std::vector<int32> strides_;\n 1234    Padding padding_;\n 1235:   bool use_cudnn_;\n 1236    TensorFormat data_format_;\n 1237    AutoTuneMap<ConvParameters, perftools::gputools::dnn::AlgorithmConfig>\n 1238        autotune_results_;\n 1239:   bool cudnn_use_autotune_;\n 1240  \n 1241    TF_DISALLOW_COPY_AND_ASSIGN(Conv2DSlowBackpropInputOp);\n ....\n 1259          errors::InvalidArgument(\"Current implementation does not yet support \"\n 1260                                  \"strides in the batch and depth dimensions.\"));\n 1261:     OP_REQUIRES_OK(context, context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_));\n 1262:     use_cudnn_ &= CanUseCudnn();\n 1263:     cudnn_use_autotune_ = CudnnUseAutotune();\n 1264      OP_REQUIRES_OK(context, context->GetAttr(\"padding\", &padding_));\n 1265    }\n ....\n 1306                                     dims.cols.input_size);\n 1307  \n 1308:     // TODO(zhengxq): cuDNN only supports equal padding on both sides, so only\n 1309:     // calling it when that is true. Remove this check when (if?) cuDNN starts\n 1310      // supporting different padding.\n 1311      bool rows_odd = (padding_rows % 2 != 0);\n ....\n 1315      OP_REQUIRES(context, stream, errors::Internal(\"No GPU stream available.\"));\n 1316  \n 1317:     if (!use_cudnn_) {\n 1318        context->SetStatus(errors::Unimplemented(\n 1319            \"Conv2DBackprop for GPU is not currently supported \"\n 1320:           \"without cudnn\"));\n 1321        return;\n 1322      }\n ....\n 1364      if (rows_odd || cols_odd) {\n 1365        // If a padding dimension is odd, we have one more element on the right\n 1366:       // side or the bottom side. This is unsupported in cudnn. Therefore,\n 1367        // we pad that extra element and make it compatible.\n 1368        OP_REQUIRES_OK(\n ....\n 1407  \n 1408      // NOTE(zhengxq):\n 1409:     // cuDNN only supports the following layouts :\n 1410      // Input  : B x D x R x C\n 1411      // Filter : OD x ID x R x C\n ....\n 1416      // The first TransformDepth performs\n 1417      // (B x R x C x D) => (B x D x R x C).\n 1418:     // Since the tensor returned from cuDNN is B x D x R x C also,\n 1419      // the second TransformDepth performs\n 1420      // (B x D x R x C) => (B x R x C x D).\n ....\n 1474                         transformed_input.template flat<T>().size());\n 1475  \n 1476:     static int64 ConvolveBackwardFilterScratchSize = GetCudnnWorkspaceLimit(\n 1477:         \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32  // 4GB by default\n 1478          );\n 1479      int device_id = stream->parent()->device_ordinal();\n ....\n 1493      };\n 1494      AlgorithmConfig algorithm_config;\n 1495:     if (cudnn_use_autotune_ &&\n 1496          !autotune_results_.Find(conv_parameters, &algorithm_config)) {\n 1497        std::vector<AlgorithmType> algorithms;\n ....\n 1502          // TODO(zhengxq): profile each algorithm multiple times to better\n 1503          // accuracy.\n 1504:         CudnnScratchAllocator scratch_allocator(\n 1505              ConvolveBackwardFilterScratchSize, context);\n 1506          ProfileResult profile_result;\n 1507:         bool cudnn_launch_status =\n 1508              stream\n 1509                  ->ThenConvolveBackwardFilterWithAlgorithm(\n ....\n 1513                      &profile_result)\n 1514                  .ok();\n 1515:         if (cudnn_launch_status) {\n 1516            if (profile_result.is_valid()) {\n 1517              if (profile_result.elapsed_time_in_ms() <\n ....\n 1539        autotune_results_.Insert(conv_parameters, algorithm_config);\n 1540      }\n 1541:     CudnnScratchAllocator scratch_allocator(ConvolveBackwardFilterScratchSize,\n 1542                                              context);\n 1543:     bool cudnn_launch_status =\n 1544          stream\n 1545              ->ThenConvolveBackwardFilterWithAlgorithm(\n ....\n 1549              .ok();\n 1550  \n 1551:     if (!cudnn_launch_status) {\n 1552        context->SetStatus(errors::Internal(\n 1553:           \"cuDNN Backward Filter function launch failure : input shape(\",\n 1554            input_shape.DebugString(), \") filter shape(\",\n 1555            filter_shape.DebugString(), \")\"));\n ....\n 1567    std::vector<int32> strides_;\n 1568    Padding padding_;\n 1569:   bool use_cudnn_;\n 1570    TensorFormat data_format_;\n 1571    AutoTuneMap<ConvParameters, perftools::gputools::dnn::AlgorithmConfig>\n 1572        autotune_results_;\n 1573:   bool cudnn_use_autotune_;\n 1574  \n 1575    TF_DISALLOW_COPY_AND_ASSIGN(Conv2DSlowBackpropFilterOp);\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\conv_grad_ops_3d.cc:\n  450      TensorShape compatible_input_shape;\n  451      if (rows_odd || cols_odd || planes_odd) {\n  452:       // cuDNN only supports the same amount of padding on both sides.\n  453        compatible_input_shape = {\n  454            batch,\n  ...\n  531                         pre_transformed_in_backprop.template flat<T>().size());\n  532  \n  533:     static int64 ConvolveBackwardDataScratchSize = GetCudnnWorkspaceLimit(\n  534:         \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32);  // 4GB by default\n  535  \n  536:     CudnnScratchAllocator scratch_allocator(ConvolveBackwardDataScratchSize,\n  537                                              context);\n  538:     bool cudnn_launch_status =\n  539          stream\n  540              ->ThenConvolveBackwardDataWithScratch(\n  ...\n  543              .ok();\n  544  \n  545:     if (!cudnn_launch_status) {\n  546        context->SetStatus(errors::Internal(\n  547:           \"cuDNN Backward Data function launch failure : input shape(\",\n  548            input_shape.DebugString(), \") filter shape(\",\n  549            filter_shape.DebugString(), \")\"));\n  ...\n  755                         transformed_input.template flat<T>().size());\n  756  \n  757:     static int64 ConvolveBackwardFilterScratchSize = GetCudnnWorkspaceLimit(\n  758:         \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32);  // 4GB by default\n  759:     CudnnScratchAllocator scratch_allocator(ConvolveBackwardFilterScratchSize,\n  760                                              context);\n  761:     bool cudnn_launch_status =\n  762          stream\n  763              ->ThenConvolveBackwardFilterWithScratch(\n  ...\n  766              .ok();\n  767  \n  768:     if (!cudnn_launch_status) {\n  769        context->SetStatus(errors::Internal(\n  770:           \"cuDNN Backward Filter function launch failure : input shape(\",\n  771            input_shape.DebugString(), \") filter shape(\",\n  772            filter_shape.DebugString(), \")\"));\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\conv_ops.cc:\n   39  #include \"tensorflow/core/util/padding.h\"\n   40  #include \"tensorflow/core/util/tensor_format.h\"\n   41: #include \"tensorflow/core/util/use_cudnn.h\"\n   42  \n   43  #if GOOGLE_CUDA\n   ..\n   95  class LaunchConvOp<CPUDevice, T> {\n   96   public:\n   97:   void launch(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n   98                const Tensor& input, const Tensor& filter, int row_stride,\n   99                int col_stride, const Eigen::PaddingType& padding, Tensor* output,\n  ...\n  114      OP_REQUIRES(context, FormatFromString(data_format, &data_format_),\n  115                  errors::InvalidArgument(\"Invalid data format\"));\n  116:     OP_REQUIRES_OK(context, context->GetAttr(\"use_cudnn_on_gpu\", &use_cudnn_));\n  117:     use_cudnn_ &= CanUseCudnn();\n  118:     cudnn_use_autotune_ = CudnnUseAutotune();\n  119      OP_REQUIRES(context, strides_.size() == 4,\n  120                  errors::InvalidArgument(\"Sliding window strides field must \"\n  ...\n  222        return;\n  223      }\n  224:     launcher_.launch(context, use_cudnn_, cudnn_use_autotune_, input, filter,\n  225                       stride_rows, stride_cols,\n  226                       BrainPadding2EigenPadding(padding_), output, data_format_);\n  ...\n  229   private:\n  230    std::vector<int32> strides_;\n  231:   bool use_cudnn_;\n  232    Padding padding_;\n  233    TensorFormat data_format_;\n  234    LaunchConvOp<Device, T> launcher_;\n  235:   bool cudnn_use_autotune_;\n  236  \n  237    TF_DISALLOW_COPY_AND_ASSIGN(Conv2DOp);\n  ...\n  247  #if GOOGLE_CUDA\n  248  \n  249: int64 GetCudnnWorkspaceLimit(const string& envvar_in_mb,\n  250                               int64 default_value_in_bytes) {\n  251    const char* workspace_limit_in_mb_str = getenv(envvar_in_mb.c_str());\n  ...\n  267  class LaunchConvOp<GPUDevice, T> {\n  268   public:\n  269:   void launch(OpKernelContext* ctx, bool use_cudnn, bool cudnn_use_autotune,\n  270                const Tensor& input_param, const Tensor& filter, int row_stride,\n  271                int col_stride, const Eigen::PaddingType& padding, Tensor* output,\n  ...\n  278      OP_REQUIRES(ctx, stream, errors::Internal(\"No GPU stream available.\"));\n  279  \n  280:     if (!use_cudnn) {\n  281        ctx->SetStatus(\n  282            errors::Unimplemented(\"Conv2D for GPU is not currently supported \"\n  283:                                 \"without cudnn\"));\n  284        return;\n  285      }\n  ...\n  427                         transformed_output.template flat<T>().size());\n  428  \n  429:     static int64 ConvolveScratchSize = GetCudnnWorkspaceLimit(\n  430:         \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32  // 4GB by default\n  431          );\n  432  \n  ...\n  447      };\n  448      AlgorithmConfig algorithm_config;\n  449:     if (cudnn_use_autotune &&\n  450          !autotune_results_.Find(conv_parameters, &algorithm_config)) {\n  451        std::vector<AlgorithmType> algorithms;\n  ...\n  456          // TODO(zhengxq): profile each algorithm multiple times to better\n  457          // accuracy.\n  458:         CudnnScratchAllocator scratch_allocator(ConvolveScratchSize, ctx);\n  459          ProfileResult profile_result;\n  460:         bool cudnn_launch_status =\n  461              stream\n  462                  ->ThenConvolveWithAlgorithm(\n  ...\n  465                      AlgorithmConfig(profile_algorithm), &profile_result)\n  466                  .ok();\n  467:         if (cudnn_launch_status) {\n  468            if (profile_result.is_valid()) {\n  469              if (profile_result.elapsed_time_in_ms() <\n  ...\n  492      }\n  493  \n  494:     CudnnScratchAllocator scratch_allocator(ConvolveScratchSize, ctx);\n  495:     bool cudnn_launch_status =\n  496          stream\n  497              ->ThenConvolveWithAlgorithm(input_desc, input_ptr, filter_desc,\n  ...\n  501              .ok();\n  502  \n  503:     if (!cudnn_launch_status) {\n  504        ctx->SetStatus(errors::Internal(\n  505:           \"cuDNN launch failure : input shape(\", input.shape().DebugString(),\n  506            \") filter shape(\", filter.shape().DebugString(), \")\"));\n  507      }\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\conv_ops_3d.cc:\n  198        const bool planes_odd = (pad_planes % 2 != 0);\n  199  \n  200:       // Necessary because cuDNN only supports symmetric padding.\n  201        // TODO(mjanusz): Consider making this optional? This would save some\n  202        // overhead and would work as long as an op trained this way is only\n  ...\n  233      // input: [b, x, y, z, d]\n  234      // t_input: [b, d, x, y, z]\n  235:     // NCDHW is the only format universally supported by cuDNN.\n  236      functor::NHWCToNCHW<GPUDevice, T, 5>()(\n  237          ctx->eigen_device<GPUDevice>(),\n  ...\n  296                         transformed_output.template flat<T>().size());\n  297  \n  298:     static int64 ConvolveScratchSize = GetCudnnWorkspaceLimit(\n  299:         \"TF_CUDNN_WORKSPACE_LIMIT_IN_MB\", 1LL << 32);  // 4GB by default\n  300:     CudnnScratchAllocator scratch_allocator(ConvolveScratchSize, ctx);\n  301:     bool cudnn_launch_status =\n  302          stream\n  303              ->ThenConvolveWithScratch(input_desc, input_ptr, filter_desc,\n  ...\n  306              .ok();\n  307  \n  308:     if (!cudnn_launch_status) {\n  309        ctx->SetStatus(errors::Internal(\n  310:           \"cuDNN launch failure : input shape(\", input.shape().DebugString(),\n  311            \") filter shape(\", filter.shape().DebugString(), \")\"));\n  312      }\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\conv_ops_gpu.h:\n   35  }\n   36  \n   37: // Get the Cudnn workspace limit from the environment variable, which is in MB.\n   38  // Return the workspace memory limit in bytes. If no value is set, return the\n   39  // default value.\n   40: int64 GetCudnnWorkspaceLimit(const string& envvar_in_mb,\n   41                               int64 default_value_in_bytes);\n   42  \n   43: // A class to provide scratch-space allocator for Stream-Executor Cudnn\n   44  // callback. TensorFlow is responsible for releasing the temporary buffers after\n   45  // the kernel finishes.\n   46: class CudnnScratchAllocator : public perftools::gputools::ScratchAllocator {\n   47   public:\n   48:   virtual ~CudnnScratchAllocator() {}\n   49:   CudnnScratchAllocator(int64 memory_limit, OpKernelContext* context)\n   50        : memory_limit_(memory_limit), total_byte_size_(0), context_(context) {}\n   51    virtual int64 GetMemoryLimitInBytes(\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\conv_ops_gpu_3.cu.cc:\n  316  }\n  317  \n  318: // A GPU helper function that converts TensorFlow filter format to Cudnn filter\n  319  // format.\n  320  template <typename T, int NDIMS>\n  ...\n  338  };\n  339  \n  340: // Converts Cudnn filter format back to TensorFlow filter format.\n  341  template <typename T, int NDIMS>\n  342  struct ReverseTransformFilter<GPUDevice, T, NDIMS> {\n  ...\n  431  \n  432  // A GPU helper functor that converts NHWC TensorFlow data format to\n  433: // NCHW format that is accepted by Cudnn.\n  434  template <typename T, int NDIMS>\n  435  struct NHWCToNCHW<GPUDevice, T, NDIMS> {\n  ...\n  448  };\n  449  \n  450: // A GPU helper functor that converts NCHW Cudnn data format to NHWC TensorFlow\n  451  // Format.\n  452  template <typename T, int NDIMS>\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\cudnn_pooling_gpu.cc:\n   19  #include <array>\n   20  \n   21: #include \"tensorflow/core/kernels/cudnn_pooling_gpu.h\"\n   22  #include \"tensorflow/core/kernels/conv_2d.h\"\n   23  #include \"tensorflow/core/kernels/conv_3d.h\"\n   ..\n   94                      .ok();\n   95    OP_REQUIRES(context, status,\n   96:               errors::Internal(\"cudnn PoolForward launch failed\"));\n   97  \n   98    auto toConstTensor = [](const Tensor& x) -> const Tensor { return x; };\n   ..\n  202            .ok();\n  203    OP_REQUIRES(context, status,\n  204:               errors::Internal(\"cudnn PoolBackward launch failed\"));\n  205  \n  206    auto toConstTensor = [](const Tensor& x) -> const Tensor { return x; };\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\cudnn_pooling_gpu.h:\n   14  ==============================================================================*/\n   15  \n   16: // Helper functions to run 3d pooling on GPU using CuDNN.\n   17  \n   18: #ifndef TENSORFLOW_KERNELS_CUDNN_POOLING_GPU_H_\n   19: #define TENSORFLOW_KERNELS_CUDNN_POOLING_GPU_H_\n   20  \n   21  #include <array>\n   ..\n   67  }  // namespace tensorflow\n   68  \n   69: #endif  // TENSORFLOW_KERNELS_CUDNN_POOLING_GPU_H_\n   70  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\maxpooling_op.cc:\n   36  #include \"tensorflow/core/util/padding.h\"\n   37  #include \"tensorflow/core/util/tensor_format.h\"\n   38: #include \"tensorflow/core/util/use_cudnn.h\"\n   39  \n   40  #if GOOGLE_CUDA\n   ..\n  364                      \"Pooling is not yet supported on the batch dimension.\"));\n  365  \n  366:     use_dnn_ = CanUseCudnn();\n  367    }\n  368  \n  ...\n  390      } else {\n  391        CHECK(data_format_ == FORMAT_NHWC)\n  392:           << \"Non-Cudnn MaxPoolGrad only supports NHWC format\";\n  393        MaxPoolingBackwardCustomKernel<T>(context, ksize_, stride_, padding_,\n  394                                          &tensor_in, out_backprop, output_shape);\n  ...\n  590                  errors::Unimplemented(\n  591                      \"Pooling is not yet supported on the batch dimension.\"));\n  592:     use_dnn_ = CanUseCudnn();\n  593    }\n  594  \n  ...\n  611      } else {\n  612        CHECK(data_format_ == FORMAT_NHWC)\n  613:           << \"Non-Cudnn MaxPool only supports NHWC format\";\n  614        Tensor* output = nullptr;\n  615        OP_REQUIRES_OK(context, context->allocate_output(0, out_shape, &output));\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\pooling_ops_3d.cc:\n   31  \n   32  #if GOOGLE_CUDA\n   33: #include \"tensorflow/core/kernels/cudnn_pooling_gpu.h\"\n   34  #endif\n   35  namespace tensorflow {\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\pooling_ops_common.cc:\n  148    }\n  149  \n  150:   /// For now, cudnn does not support NHWC format, so we need to convert it\n  151:   /// to NCHW before calling cudnn. We need to get rid of this once it is done\n  152    Tensor transformed_input;\n  153    if (data_format == FORMAT_NHWC) {\n  ...\n  174    }\n  175  \n  176:   /// Get ready to call cudnn\n  177    perftools::gputools::dnn::PoolingDescriptor pooling_desc;\n  178    pooling_desc.set_pooling_mode(pooling_mode)\n  ...\n  212                      .ok();\n  213    OP_REQUIRES(context, status,\n  214:               errors::Internal(\"cudnn PoolBackward launch failed\"));\n  215  \n  216    if (data_format == FORMAT_NHWC) {\n  ...\n  247    }\n  248  \n  249:   /// For now, cudnn does not support NHWC format, so we need to convert it\n  250:   /// to NCHW before calling cudnn. We need to get rid of this once it is done\n  251    Tensor transformed_input;\n  252    TensorShape transformed_input_shape;\n  ...\n  294      if (tensor_in) {\n  295        // For AvgPoolGrad, the original input tensor is not necessary. However,\n  296:       // cudnn still requires them to run, although they do not affect the\n  297        // results.\n  298        functor::NHWCToNCHW<GPUDevice, T, 4>()(context->eigen_device<Device>(),\n  ...\n  302      if (tensor_out) {\n  303        // For AvgPoolGrad, the original output tensor is not necessary. However,\n  304:       // cudnn still requires them to run, although they do not affect the\n  305        // results.\n  306        functor::NHWCToNCHW<GPUDevice, T, 4>()(context->eigen_device<Device>(),\n  ...\n  313    }\n  314  \n  315:   /// Get ready to call cudnn\n  316    perftools::gputools::dnn::PoolingDescriptor pooling_desc;\n  317    pooling_desc.set_pooling_mode(pooling_mode)\n  ...\n  360            .ok();\n  361    OP_REQUIRES(context, status,\n  362:               errors::Internal(\"cudnn PoolBackward launch failed\"));\n  363  \n  364    if (data_format == FORMAT_NHWC) {\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\kernels\\pooling_ops_common_gpu.h:\n   35  namespace tensorflow {\n   36  \n   37: // A helper class that launch the cudnn pooling forward operations.\n   38  template <typename T>\n   39  class DnnPoolingOp {\n   ..\n   48  };\n   49  \n   50: // A helper class that launch the cudnn pooling backward operations.\n   51  // The original input and output tensors are optional for AvgPoolGrad, but\n   52  // mandatory for MaxPoolGrad.\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\ops\\compat\\ops_history.v0.pbtxt:\n 6261    }\n 6262    attr {\n 6263:     name: \"use_cudnn_on_gpu\"\n 6264      type: \"bool\"\n 6265      default_value {\n ....\n 6307    }\n 6308    attr {\n 6309:     name: \"use_cudnn_on_gpu\"\n 6310      type: \"bool\"\n 6311      default_value {\n ....\n 6367    }\n 6368    attr {\n 6369:     name: \"use_cudnn_on_gpu\"\n 6370      type: \"bool\"\n 6371      default_value {\n ....\n 6430    }\n 6431    attr {\n 6432:     name: \"use_cudnn_on_gpu\"\n 6433      type: \"bool\"\n 6434      default_value {\n ....\n 6480    }\n 6481    attr {\n 6482:     name: \"use_cudnn_on_gpu\"\n 6483      type: \"bool\"\n 6484      default_value {\n ....\n 6544    }\n 6545    attr {\n 6546:     name: \"use_cudnn_on_gpu\"\n 6547      type: \"bool\"\n 6548      default_value {\n ....\n 6607    }\n 6608    attr {\n 6609:     name: \"use_cudnn_on_gpu\"\n 6610      type: \"bool\"\n 6611      default_value {\n ....\n 6657    }\n 6658    attr {\n 6659:     name: \"use_cudnn_on_gpu\"\n 6660      type: \"bool\"\n 6661      default_value {\n ....\n 6721    }\n 6722    attr {\n 6723:     name: \"use_cudnn_on_gpu\"\n 6724      type: \"bool\"\n 6725      default_value {\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\ops\\nn_grad.cc:\n  125      {\"T: {float, double}\",\n  126       \"strides: list(int)\",\n  127:      \"use_cudnn_on_gpu: bool = true\",\n  128       GetPaddingAttrString(),\n  129       GetConvnetDataFormatAttrString()},\n  ...\n  136                    {\"padding\", \"$padding\"},\n  137                    {\"data_format\", \"$data_format\"},\n  138:                   {\"use_cudnn_on_gpu\", \"$use_cudnn_on_gpu\"}}},\n  139  \n  140        {{\"f_shape\"}, \"Shape\", {\"filter\"}, {{\"T\", \"$T\"}}},\n  ...\n  144                    {\"padding\", \"$padding\"},\n  145                    {\"data_format\", \"$data_format\"},\n  146:                   {\"use_cudnn_on_gpu\", \"$use_cudnn_on_gpu\"}}},\n  147      });\n  148    // clang-format on\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\ops\\nn_ops.cc:\n  227      .Attr(\"T: {half, float, double}\")\n  228      .Attr(\"strides: list(int)\")\n  229:     .Attr(\"use_cudnn_on_gpu: bool = true\")\n  230      .Attr(GetPaddingAttrString())\n  231      .Attr(GetConvnetDataFormatAttrString())\n  ...\n  272      .Attr(\"T: {half, float, double}\")\n  273      .Attr(\"strides: list(int)\")\n  274:     .Attr(\"use_cudnn_on_gpu: bool = true\")\n  275      .Attr(GetPaddingAttrString())\n  276      .Attr(GetConvnetDataFormatAttrString())\n  ...\n  297  )doc\");\n  298  \n  299: // TODO(jeff): Instead of 'use_cudnn_for_gpu', maybe we should have a\n  300  // more general string attribute ('kernel_impl'?) that can be used to\n  301  // select among several possible implementations.\n  ...\n  307      .Attr(\"T: {half, float, double}\")\n  308      .Attr(\"strides: list(int)\")\n  309:     .Attr(\"use_cudnn_on_gpu: bool = true\")\n  310      .Attr(GetPaddingAttrString())\n  311      .Attr(GetConvnetDataFormatAttrString())\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\ops\\ops.pbtxt:\n 3102    }\n 3103    attr {\n 3104:     name: \"use_cudnn_on_gpu\"\n 3105      type: \"bool\"\n 3106      default_value {\n ....\n 3175    }\n 3176    attr {\n 3177:     name: \"use_cudnn_on_gpu\"\n 3178      type: \"bool\"\n 3179      default_value {\n ....\n 3247    }\n 3248    attr {\n 3249:     name: \"use_cudnn_on_gpu\"\n 3250      type: \"bool\"\n 3251      default_value {\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\platform\\default\\platform.bzl:\n    1  CUDA_VERSION = \"\"\n    2  \n    3: CUDNN_VERSION = \"\"\n    4  \n    5  PLATFORM = \"\"\n    .\n    8    return CUDA_VERSION\n    9  \n   10: def cudnn_sdk_version():\n   11:   return CUDNN_VERSION\n   12  \n   13  def cuda_library_path(name, version = cuda_sdk_version()):\n   ..\n   29      return \"lib64/lib{}_static.a\".format(name)\n   30  \n   31: def cudnn_library_path(version = cudnn_sdk_version()):\n   32    if PLATFORM == \"Darwin\":\n   33      if not version:\n   34:       return \"lib/libcudnn.dylib\"\n   35      else:\n   36:       return \"lib/libcudnn.{}.dylib\".format(version)\n   37    else:\n   38      if not version:\n   39:       return \"lib64/libcudnn.so\"\n   40      else:\n   41:       return \"lib64/libcudnn.so.{}\".format(version)\n   42  \n   43  def cupti_library_path(version = cuda_sdk_version()):\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\util\\use_cudnn.cc:\n   14  ==============================================================================*/\n   15  \n   16: #include \"tensorflow/core/util/use_cudnn.h\"\n   17  \n   18  #include <stdlib.h>\n   ..\n   35  }\n   36  \n   37: bool CanUseCudnn() { return ReadBoolFromEnvVar(\"TF_USE_CUDNN\", true); }\n   38  \n   39: bool CudnnUseAutotune() {\n   40:   return ReadBoolFromEnvVar(\"TF_CUDNN_USE_AUTOTUNE\", true);\n   41  }\n   42  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\core\\util\\use_cudnn.h:\n   14  ==============================================================================*/\n   15  \n   16: // The utility to check whether we have Cudnn dependency.\n   17  \n   18: #ifndef TENSORFLOW_UTIL_USE_CUDNN_H_\n   19: #define TENSORFLOW_UTIL_USE_CUDNN_H_\n   20  \n   21  namespace tensorflow {\n   22  \n   23: bool CanUseCudnn();\n   24: bool CudnnUseAutotune();\n   25  \n   26  }  // namespace tensorflow\n   27  \n   28: #endif  // TENSORFLOW_UTIL_USE_CUDNN_H_\n   29  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\examples\\skflow\\multiple_gpu.py:\n   29  \n   30      Note: If you want to run this example with multiple GPUs, Cuda Toolkit 7.0 and \n   31:     CUDNN 6.5 V2 from NVIDIA need to be installed beforehand. \n   32      \"\"\"\n   33      with tf.device('/gpu:1'):\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\examples\\tutorials\\deepdream\\deepdream.ipynb:\n  241         \"        <script>\\n\",\n  242         \"          function load() {\\n\",\n  243:        \"            document.getElementById(&quot;graph0.8534775751&quot;).pbtxt = 'node {\\\\n  name: &quot;input&quot;\\\\n  op: &quot;Placeholder&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;shape&quot;\\\\n    value {\\\\n      shape {\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d0/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 7\\\\n          }\\\\n          dim {\\\\n            size: 7\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 37632 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d0/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d1/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 16384 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d1/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d2/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 442368 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d2/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 49152 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 73728 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 384 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 442368 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 12288 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 64 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 51200 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 128 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 24576 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 128 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 131072 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 131072 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 884736 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 32768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 128 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 307200 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 384 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 65536 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 480\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 368640 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 480\\\\n          }\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 184320 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 384 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 96\\\\n          }\\\\n          dim {\\\\n            size: 204\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 705024 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 204\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 816 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 480\\\\n          }\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 30720 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 64 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 16\\\\n          }\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 76800 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 192 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 480\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 122880 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 508\\\\n          }\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 325120 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 640 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 508\\\\n          }\\\\n          dim {\\\\n            size: 112\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 227584 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 112\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 448 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 112\\\\n          }\\\\n          dim {\\\\n            size: 224\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 903168 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 224\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 896 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 508\\\\n          }\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 48768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 96 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 153600 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 508\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 130048 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 262144 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 262144 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1179648 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1024 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 49152 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 96 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 24\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 153600 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 131072 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 112\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 229376 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 112\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 448 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 144\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 294912 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 144\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 576 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 144\\\\n          }\\\\n          dim {\\\\n            size: 288\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1492992 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 288\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1152 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 65536 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 128 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 204800 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 512\\\\n          }\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 131072 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 64\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 256 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 528\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 540672 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1024 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 528\\\\n          }\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 337920 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 640 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n          dim {\\\\n            size: 320\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1843200 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 320\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1280 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 528\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 67584 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 128 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 32\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 409600 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 528\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 270336 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 851968 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 256\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1024 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 532480 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 640 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 160\\\\n          }\\\\n          dim {\\\\n            size: 320\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1843200 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 320\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1280 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 159744 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 192 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 614400 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 425984 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/1x1_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 384\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1277952 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/1x1_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 384\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1536 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 638976 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 192\\\\n          }\\\\n          dim {\\\\n            size: 384\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 2654208 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 384\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 1536 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 159744 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 192 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 48\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 614400 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool_reduce_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 832\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 425984 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool_reduce_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 508\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 260096 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2048\\\\n          }\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 8388608 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4096 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax0/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4128768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax0/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4032 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck_w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 1\\\\n          }\\\\n          dim {\\\\n            size: 528\\\\n          }\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 270336 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck_b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 128\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 512 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2048\\\\n          }\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 8388608 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4096 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax1/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4128768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax1/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4032 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax2/w&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1024\\\\n          }\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4128768 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax2/b&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 1008\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 4032 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d0/pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;input&quot;\\\\n  input: &quot;conv2d0/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d0/pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;conv2d0/pre_relu/conv&quot;\\\\n  input: &quot;conv2d0/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d0&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;conv2d0/pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;maxpool0&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;conv2d0&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;localresponsenorm0&quot;\\\\n  op: &quot;LRN&quot;\\\\n  input: &quot;maxpool0&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;alpha&quot;\\\\n    value {\\\\n      f: 9.99999974738e-05\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;beta&quot;\\\\n    value {\\\\n      f: 0.5\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;bias&quot;\\\\n    value {\\\\n      f: 2.0\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;depth_radius&quot;\\\\n    value {\\\\n      i: 5\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d1/pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;localresponsenorm0&quot;\\\\n  input: &quot;conv2d1/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;VALID&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d1/pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;conv2d1/pre_relu/conv&quot;\\\\n  input: &quot;conv2d1/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;conv2d1/pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d2/pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;conv2d1&quot;\\\\n  input: &quot;conv2d2/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d2/pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;conv2d2/pre_relu/conv&quot;\\\\n  input: &quot;conv2d2/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;conv2d2&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;conv2d2/pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;localresponsenorm1&quot;\\\\n  op: &quot;LRN&quot;\\\\n  input: &quot;conv2d2&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;alpha&quot;\\\\n    value {\\\\n      f: 9.99999974738e-05\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;beta&quot;\\\\n    value {\\\\n      f: 0.5\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;bias&quot;\\\\n    value {\\\\n      f: 2.0\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;depth_radius&quot;\\\\n    value {\\\\n      i: 5\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;maxpool1&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;localresponsenorm1&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool1&quot;\\\\n  input: &quot;mixed3a/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool1&quot;\\\\n  input: &quot;mixed3a/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a/3x3_bottleneck&quot;\\\\n  input: &quot;mixed3a/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool1&quot;\\\\n  input: &quot;mixed3a/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a/5x5_bottleneck&quot;\\\\n  input: &quot;mixed3a/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;maxpool1&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a/pool&quot;\\\\n  input: &quot;mixed3a/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3a/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed3a/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3a/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3a&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed3a/concat/dim&quot;\\\\n  input: &quot;mixed3a/1x1&quot;\\\\n  input: &quot;mixed3a/3x3&quot;\\\\n  input: &quot;mixed3a/5x5&quot;\\\\n  input: &quot;mixed3a/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a&quot;\\\\n  input: &quot;mixed3b/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a&quot;\\\\n  input: &quot;mixed3b/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3b/3x3_bottleneck&quot;\\\\n  input: &quot;mixed3b/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3a&quot;\\\\n  input: &quot;mixed3b/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3b/5x5_bottleneck&quot;\\\\n  input: &quot;mixed3b/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed3a&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed3b/pool&quot;\\\\n  input: &quot;mixed3b/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed3b/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed3b/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed3b/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed3b&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed3b/concat/dim&quot;\\\\n  input: &quot;mixed3b/1x1&quot;\\\\n  input: &quot;mixed3b/3x3&quot;\\\\n  input: &quot;mixed3b/5x5&quot;\\\\n  input: &quot;mixed3b/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;maxpool4&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed3b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool4&quot;\\\\n  input: &quot;mixed4a/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool4&quot;\\\\n  input: &quot;mixed4a/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a/3x3_bottleneck&quot;\\\\n  input: &quot;mixed4a/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool4&quot;\\\\n  input: &quot;mixed4a/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a/5x5_bottleneck&quot;\\\\n  input: &quot;mixed4a/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;maxpool4&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a/pool&quot;\\\\n  input: &quot;mixed4a/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4a/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed4a/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4a/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4a&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed4a/concat/dim&quot;\\\\n  input: &quot;mixed4a/1x1&quot;\\\\n  input: &quot;mixed4a/3x3&quot;\\\\n  input: &quot;mixed4a/5x5&quot;\\\\n  input: &quot;mixed4a/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a&quot;\\\\n  input: &quot;mixed4b/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a&quot;\\\\n  input: &quot;mixed4b/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b/3x3_bottleneck&quot;\\\\n  input: &quot;mixed4b/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4a&quot;\\\\n  input: &quot;mixed4b/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b/5x5_bottleneck&quot;\\\\n  input: &quot;mixed4b/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed4a&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b/pool&quot;\\\\n  input: &quot;mixed4b/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4b/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed4b/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4b/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4b&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed4b/concat/dim&quot;\\\\n  input: &quot;mixed4b/1x1&quot;\\\\n  input: &quot;mixed4b/3x3&quot;\\\\n  input: &quot;mixed4b/5x5&quot;\\\\n  input: &quot;mixed4b/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b&quot;\\\\n  input: &quot;mixed4c/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b&quot;\\\\n  input: &quot;mixed4c/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c/3x3_bottleneck&quot;\\\\n  input: &quot;mixed4c/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4b&quot;\\\\n  input: &quot;mixed4c/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c/5x5_bottleneck&quot;\\\\n  input: &quot;mixed4c/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed4b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c/pool&quot;\\\\n  input: &quot;mixed4c/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4c/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed4c/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4c/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4c&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed4c/concat/dim&quot;\\\\n  input: &quot;mixed4c/1x1&quot;\\\\n  input: &quot;mixed4c/3x3&quot;\\\\n  input: &quot;mixed4c/5x5&quot;\\\\n  input: &quot;mixed4c/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c&quot;\\\\n  input: &quot;mixed4d/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c&quot;\\\\n  input: &quot;mixed4d/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d/3x3_bottleneck&quot;\\\\n  input: &quot;mixed4d/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4c&quot;\\\\n  input: &quot;mixed4d/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d/5x5_bottleneck&quot;\\\\n  input: &quot;mixed4d/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed4c&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d/pool&quot;\\\\n  input: &quot;mixed4d/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4d/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed4d/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4d/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4d&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed4d/concat/dim&quot;\\\\n  input: &quot;mixed4d/1x1&quot;\\\\n  input: &quot;mixed4d/3x3&quot;\\\\n  input: &quot;mixed4d/5x5&quot;\\\\n  input: &quot;mixed4d/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d&quot;\\\\n  input: &quot;mixed4e/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d&quot;\\\\n  input: &quot;mixed4e/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4e/3x3_bottleneck&quot;\\\\n  input: &quot;mixed4e/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4d&quot;\\\\n  input: &quot;mixed4e/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4e/5x5_bottleneck&quot;\\\\n  input: &quot;mixed4e/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed4d&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed4e/pool&quot;\\\\n  input: &quot;mixed4e/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed4e/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed4e/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed4e/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed4e&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed4e/concat/dim&quot;\\\\n  input: &quot;mixed4e/1x1&quot;\\\\n  input: &quot;mixed4e/3x3&quot;\\\\n  input: &quot;mixed4e/5x5&quot;\\\\n  input: &quot;mixed4e/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;maxpool10&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed4e&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool10&quot;\\\\n  input: &quot;mixed5a/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool10&quot;\\\\n  input: &quot;mixed5a/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a/3x3_bottleneck&quot;\\\\n  input: &quot;mixed5a/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;maxpool10&quot;\\\\n  input: &quot;mixed5a/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a/5x5_bottleneck&quot;\\\\n  input: &quot;mixed5a/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;maxpool10&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a/pool&quot;\\\\n  input: &quot;mixed5a/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5a/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed5a/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5a/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5a&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed5a/concat/dim&quot;\\\\n  input: &quot;mixed5a/1x1&quot;\\\\n  input: &quot;mixed5a/3x3&quot;\\\\n  input: &quot;mixed5a/5x5&quot;\\\\n  input: &quot;mixed5a/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/1x1_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a&quot;\\\\n  input: &quot;mixed5b/1x1_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/1x1_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/1x1_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/1x1_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/1x1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/1x1_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a&quot;\\\\n  input: &quot;mixed5b/3x3_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/3x3_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/3x3_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/3x3_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5b/3x3_bottleneck&quot;\\\\n  input: &quot;mixed5b/3x3_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/3x3_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/3x3_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/3x3&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/3x3_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5a&quot;\\\\n  input: &quot;mixed5b/5x5_bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/5x5_bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/5x5_bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/5x5_bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5b/5x5_bottleneck&quot;\\\\n  input: &quot;mixed5b/5x5_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/5x5_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/5x5_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/5x5&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/5x5_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool&quot;\\\\n  op: &quot;MaxPool&quot;\\\\n  input: &quot;mixed5a&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool_reduce_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;mixed5b/pool&quot;\\\\n  input: &quot;mixed5b/pool_reduce_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool_reduce_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;mixed5b/pool_reduce_pre_relu/conv&quot;\\\\n  input: &quot;mixed5b/pool_reduce_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/pool_reduce&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;mixed5b/pool_reduce_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b/concat/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 3\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;mixed5b&quot;\\\\n  op: &quot;Concat&quot;\\\\n  input: &quot;mixed5b/concat/dim&quot;\\\\n  input: &quot;mixed5b/1x1&quot;\\\\n  input: &quot;mixed5b/3x3&quot;\\\\n  input: &quot;mixed5b/5x5&quot;\\\\n  input: &quot;mixed5b/pool_reduce&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;N&quot;\\\\n    value {\\\\n      i: 4\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;avgpool0&quot;\\\\n  op: &quot;AvgPool&quot;\\\\n  input: &quot;mixed5b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 7\\\\n        i: 7\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;VALID&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/pool&quot;\\\\n  op: &quot;AvgPool&quot;\\\\n  input: &quot;mixed4a&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 5\\\\n        i: 5\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;VALID&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;head0/pool&quot;\\\\n  input: &quot;head0/bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;head0/bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;head0/bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;head0/bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck/reshape/shape&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\000\\\\\\\\010\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head0/bottleneck/reshape&quot;\\\\n  op: &quot;Reshape&quot;\\\\n  input: &quot;head0/bottleneck&quot;\\\\n  input: &quot;head0/bottleneck/reshape/shape&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/pre_relu/matmul&quot;\\\\n  op: &quot;MatMul&quot;\\\\n  input: &quot;head0/bottleneck/reshape&quot;\\\\n  input: &quot;nn0/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_a&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_b&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;nn0/pre_relu/matmul&quot;\\\\n  input: &quot;nn0/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;nn0/pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/reshape/shape&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\000\\\\\\\\004\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn0/reshape&quot;\\\\n  op: &quot;Reshape&quot;\\\\n  input: &quot;nn0&quot;\\\\n  input: &quot;nn0/reshape/shape&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax0/pre_activation/matmul&quot;\\\\n  op: &quot;MatMul&quot;\\\\n  input: &quot;nn0/reshape&quot;\\\\n  input: &quot;softmax0/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_a&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_b&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax0/pre_activation&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;softmax0/pre_activation/matmul&quot;\\\\n  input: &quot;softmax0/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax0&quot;\\\\n  op: &quot;Softmax&quot;\\\\n  input: &quot;softmax0/pre_activation&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/pool&quot;\\\\n  op: &quot;AvgPool&quot;\\\\n  input: &quot;mixed4d&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;ksize&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 5\\\\n        i: 5\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;VALID&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 3\\\\n        i: 3\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck_pre_relu/conv&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;head1/pool&quot;\\\\n  input: &quot;head1/bottleneck_w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck_pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;head1/bottleneck_pre_relu/conv&quot;\\\\n  input: &quot;head1/bottleneck_b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;head1/bottleneck_pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck/reshape/shape&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\000\\\\\\\\010\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;head1/bottleneck/reshape&quot;\\\\n  op: &quot;Reshape&quot;\\\\n  input: &quot;head1/bottleneck&quot;\\\\n  input: &quot;head1/bottleneck/reshape/shape&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/pre_relu/matmul&quot;\\\\n  op: &quot;MatMul&quot;\\\\n  input: &quot;head1/bottleneck/reshape&quot;\\\\n  input: &quot;nn1/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_a&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_b&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/pre_relu&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;nn1/pre_relu/matmul&quot;\\\\n  input: &quot;nn1/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1&quot;\\\\n  op: &quot;Relu&quot;\\\\n  input: &quot;nn1/pre_relu&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/reshape/shape&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\000\\\\\\\\004\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;nn1/reshape&quot;\\\\n  op: &quot;Reshape&quot;\\\\n  input: &quot;nn1&quot;\\\\n  input: &quot;nn1/reshape/shape&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax1/pre_activation/matmul&quot;\\\\n  op: &quot;MatMul&quot;\\\\n  input: &quot;nn1/reshape&quot;\\\\n  input: &quot;softmax1/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_a&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_b&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax1/pre_activation&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;softmax1/pre_activation/matmul&quot;\\\\n  input: &quot;softmax1/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax1&quot;\\\\n  op: &quot;Softmax&quot;\\\\n  input: &quot;softmax1/pre_activation&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;avgpool0/reshape/shape&quot;\\\\n  op: &quot;Const&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 2\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\000\\\\\\\\004\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;avgpool0/reshape&quot;\\\\n  op: &quot;Reshape&quot;\\\\n  input: &quot;avgpool0&quot;\\\\n  input: &quot;avgpool0/reshape/shape&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax2/pre_activation/matmul&quot;\\\\n  op: &quot;MatMul&quot;\\\\n  input: &quot;avgpool0/reshape&quot;\\\\n  input: &quot;softmax2/w&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_a&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;transpose_b&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax2/pre_activation&quot;\\\\n  op: &quot;BiasAdd&quot;\\\\n  input: &quot;softmax2/pre_activation/matmul&quot;\\\\n  input: &quot;softmax2/b&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;softmax2&quot;\\\\n  op: &quot;Softmax&quot;\\\\n  input: &quot;softmax2/pre_activation&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;output&quot;\\\\n  op: &quot;Identity&quot;\\\\n  input: &quot;softmax0&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;output1&quot;\\\\n  op: &quot;Identity&quot;\\\\n  input: &quot;softmax1&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;output2&quot;\\\\n  op: &quot;Identity&quot;\\\\n  input: &quot;softmax2&quot;\\\\n  device: &quot;/cpu:0&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\n';\\n\",\n  244         \"          }\\n\",\n  245         \"        </script>\\n\",\n  ...\n  673         \"        <script>\\n\",\n  674         \"          function load() {\\n\",\n  675:        \"            document.getElementById(&quot;graph0.536811672345&quot;).pbtxt = 'node {\\\\n  name: &quot;lap_in&quot;\\\\n  op: &quot;Placeholder&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;shape&quot;\\\\n    value {\\\\n      shape {\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;ExpandDims/dim&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;ExpandDims&quot;\\\\n  op: &quot;ExpandDims&quot;\\\\n  input: &quot;lap_in&quot;\\\\n  input: &quot;ExpandDims/dim&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/Conv2D/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/Conv2D&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;ExpandDims&quot;\\\\n  input: &quot;split/Conv2D/filter&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;ExpandDims&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;split/Shape&quot;\\\\n  input: &quot;split/conv2d_transpose/filter&quot;\\\\n  input: &quot;split/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split/sub&quot;\\\\n  op: &quot;Sub&quot;\\\\n  input: &quot;ExpandDims&quot;\\\\n  input: &quot;split/conv2d_transpose&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/Conv2D/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/Conv2D&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;split/Conv2D&quot;\\\\n  input: &quot;split_1/Conv2D/filter&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;split/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;split_1/Shape&quot;\\\\n  input: &quot;split_1/conv2d_transpose/filter&quot;\\\\n  input: &quot;split_1/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_1/sub&quot;\\\\n  op: &quot;Sub&quot;\\\\n  input: &quot;split/Conv2D&quot;\\\\n  input: &quot;split_1/conv2d_transpose&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/Conv2D/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/Conv2D&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;split_1/Conv2D&quot;\\\\n  input: &quot;split_2/Conv2D/filter&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;split_1/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;split_2/Shape&quot;\\\\n  input: &quot;split_2/conv2d_transpose/filter&quot;\\\\n  input: &quot;split_2/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_2/sub&quot;\\\\n  op: &quot;Sub&quot;\\\\n  input: &quot;split_1/Conv2D&quot;\\\\n  input: &quot;split_2/conv2d_transpose&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/Conv2D/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/Conv2D&quot;\\\\n  op: &quot;Conv2D&quot;\\\\n  input: &quot;split_2/Conv2D&quot;\\\\n  input: &quot;split_3/Conv2D/filter&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;split_2/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;split_3/Shape&quot;\\\\n  input: &quot;split_3/conv2d_transpose/filter&quot;\\\\n  input: &quot;split_3/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;split_3/sub&quot;\\\\n  op: &quot;Sub&quot;\\\\n  input: &quot;split_2/Conv2D&quot;\\\\n  input: &quot;split_3/conv2d_transpose&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Square&quot;\\\\n  op: &quot;Square&quot;\\\\n  input: &quot;split_3/Conv2D&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Rank&quot;\\\\n  op: &quot;Rank&quot;\\\\n  input: &quot;normalize/Square&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/range/start&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/range/delta&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/range&quot;\\\\n  op: &quot;Range&quot;\\\\n  input: &quot;normalize/range/start&quot;\\\\n  input: &quot;normalize/Rank&quot;\\\\n  input: &quot;normalize/range/delta&quot;\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Mean&quot;\\\\n  op: &quot;Mean&quot;\\\\n  input: &quot;normalize/Square&quot;\\\\n  input: &quot;normalize/range&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;keep_dims&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Sqrt&quot;\\\\n  op: &quot;Sqrt&quot;\\\\n  input: &quot;normalize/Mean&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Maximum/y&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n        }\\\\n        float_val: 1.00000001335e-10\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/Maximum&quot;\\\\n  op: &quot;Maximum&quot;\\\\n  input: &quot;normalize/Sqrt&quot;\\\\n  input: &quot;normalize/Maximum/y&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize/div&quot;\\\\n  op: &quot;Div&quot;\\\\n  input: &quot;split_3/Conv2D&quot;\\\\n  input: &quot;normalize/Maximum&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Square&quot;\\\\n  op: &quot;Square&quot;\\\\n  input: &quot;split_3/sub&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Rank&quot;\\\\n  op: &quot;Rank&quot;\\\\n  input: &quot;normalize_1/Square&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/range/start&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/range/delta&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/range&quot;\\\\n  op: &quot;Range&quot;\\\\n  input: &quot;normalize_1/range/start&quot;\\\\n  input: &quot;normalize_1/Rank&quot;\\\\n  input: &quot;normalize_1/range/delta&quot;\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Mean&quot;\\\\n  op: &quot;Mean&quot;\\\\n  input: &quot;normalize_1/Square&quot;\\\\n  input: &quot;normalize_1/range&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;keep_dims&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Sqrt&quot;\\\\n  op: &quot;Sqrt&quot;\\\\n  input: &quot;normalize_1/Mean&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Maximum/y&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n        }\\\\n        float_val: 1.00000001335e-10\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/Maximum&quot;\\\\n  op: &quot;Maximum&quot;\\\\n  input: &quot;normalize_1/Sqrt&quot;\\\\n  input: &quot;normalize_1/Maximum/y&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_1/div&quot;\\\\n  op: &quot;Div&quot;\\\\n  input: &quot;split_3/sub&quot;\\\\n  input: &quot;normalize_1/Maximum&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Square&quot;\\\\n  op: &quot;Square&quot;\\\\n  input: &quot;split_2/sub&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Rank&quot;\\\\n  op: &quot;Rank&quot;\\\\n  input: &quot;normalize_2/Square&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/range/start&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/range/delta&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/range&quot;\\\\n  op: &quot;Range&quot;\\\\n  input: &quot;normalize_2/range/start&quot;\\\\n  input: &quot;normalize_2/Rank&quot;\\\\n  input: &quot;normalize_2/range/delta&quot;\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Mean&quot;\\\\n  op: &quot;Mean&quot;\\\\n  input: &quot;normalize_2/Square&quot;\\\\n  input: &quot;normalize_2/range&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;keep_dims&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Sqrt&quot;\\\\n  op: &quot;Sqrt&quot;\\\\n  input: &quot;normalize_2/Mean&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Maximum/y&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n        }\\\\n        float_val: 1.00000001335e-10\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/Maximum&quot;\\\\n  op: &quot;Maximum&quot;\\\\n  input: &quot;normalize_2/Sqrt&quot;\\\\n  input: &quot;normalize_2/Maximum/y&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_2/div&quot;\\\\n  op: &quot;Div&quot;\\\\n  input: &quot;split_2/sub&quot;\\\\n  input: &quot;normalize_2/Maximum&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Square&quot;\\\\n  op: &quot;Square&quot;\\\\n  input: &quot;split_1/sub&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Rank&quot;\\\\n  op: &quot;Rank&quot;\\\\n  input: &quot;normalize_3/Square&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/range/start&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/range/delta&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/range&quot;\\\\n  op: &quot;Range&quot;\\\\n  input: &quot;normalize_3/range/start&quot;\\\\n  input: &quot;normalize_3/Rank&quot;\\\\n  input: &quot;normalize_3/range/delta&quot;\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Mean&quot;\\\\n  op: &quot;Mean&quot;\\\\n  input: &quot;normalize_3/Square&quot;\\\\n  input: &quot;normalize_3/range&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;keep_dims&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Sqrt&quot;\\\\n  op: &quot;Sqrt&quot;\\\\n  input: &quot;normalize_3/Mean&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Maximum/y&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n        }\\\\n        float_val: 1.00000001335e-10\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/Maximum&quot;\\\\n  op: &quot;Maximum&quot;\\\\n  input: &quot;normalize_3/Sqrt&quot;\\\\n  input: &quot;normalize_3/Maximum/y&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_3/div&quot;\\\\n  op: &quot;Div&quot;\\\\n  input: &quot;split_1/sub&quot;\\\\n  input: &quot;normalize_3/Maximum&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Square&quot;\\\\n  op: &quot;Square&quot;\\\\n  input: &quot;split/sub&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Rank&quot;\\\\n  op: &quot;Rank&quot;\\\\n  input: &quot;normalize_4/Square&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/range/start&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/range/delta&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n        }\\\\n        int_val: 1\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/range&quot;\\\\n  op: &quot;Range&quot;\\\\n  input: &quot;normalize_4/range/start&quot;\\\\n  input: &quot;normalize_4/Rank&quot;\\\\n  input: &quot;normalize_4/range/delta&quot;\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Mean&quot;\\\\n  op: &quot;Mean&quot;\\\\n  input: &quot;normalize_4/Square&quot;\\\\n  input: &quot;normalize_4/range&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;keep_dims&quot;\\\\n    value {\\\\n      b: false\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Sqrt&quot;\\\\n  op: &quot;Sqrt&quot;\\\\n  input: &quot;normalize_4/Mean&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Maximum/y&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n        }\\\\n        float_val: 1.00000001335e-10\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/Maximum&quot;\\\\n  op: &quot;Maximum&quot;\\\\n  input: &quot;normalize_4/Sqrt&quot;\\\\n  input: &quot;normalize_4/Maximum/y&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;normalize_4/div&quot;\\\\n  op: &quot;Div&quot;\\\\n  input: &quot;split/sub&quot;\\\\n  input: &quot;normalize_4/Maximum&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;normalize_1/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;merge/Shape&quot;\\\\n  input: &quot;merge/conv2d_transpose/filter&quot;\\\\n  input: &quot;normalize/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge/add&quot;\\\\n  op: &quot;Add&quot;\\\\n  input: &quot;merge/conv2d_transpose&quot;\\\\n  input: &quot;normalize_1/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_1/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;normalize_2/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_1/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_1/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;merge_1/Shape&quot;\\\\n  input: &quot;merge_1/conv2d_transpose/filter&quot;\\\\n  input: &quot;merge/add&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_1/add&quot;\\\\n  op: &quot;Add&quot;\\\\n  input: &quot;merge_1/conv2d_transpose&quot;\\\\n  input: &quot;normalize_2/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_2/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;normalize_3/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_2/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_2/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;merge_2/Shape&quot;\\\\n  input: &quot;merge_2/conv2d_transpose/filter&quot;\\\\n  input: &quot;merge_1/add&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_2/add&quot;\\\\n  op: &quot;Add&quot;\\\\n  input: &quot;merge_2/conv2d_transpose&quot;\\\\n  input: &quot;normalize_3/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_3/Shape&quot;\\\\n  op: &quot;Shape&quot;\\\\n  input: &quot;normalize_4/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_3/conv2d_transpose/filter&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_FLOAT\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 5\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n          dim {\\\\n            size: 3\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;<stripped 900 bytes>&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_3/conv2d_transpose&quot;\\\\n  op: &quot;Conv2DBackpropInput&quot;\\\\n  input: &quot;merge_3/Shape&quot;\\\\n  input: &quot;merge_3/conv2d_transpose/filter&quot;\\\\n  input: &quot;merge_2/add&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;padding&quot;\\\\n    value {\\\\n      s: &quot;SAME&quot;\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;strides&quot;\\\\n    value {\\\\n      list {\\\\n        i: 1\\\\n        i: 2\\\\n        i: 2\\\\n        i: 1\\\\n      }\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;use_cudnn_on_gpu&quot;\\\\n    value {\\\\n      b: true\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;merge_3/add&quot;\\\\n  op: &quot;Add&quot;\\\\n  input: &quot;merge_3/conv2d_transpose&quot;\\\\n  input: &quot;normalize_4/div&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;Slice/begin&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 4\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\000&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;Slice/size&quot;\\\\n  op: &quot;Const&quot;\\\\n  attr {\\\\n    key: &quot;dtype&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;value&quot;\\\\n    value {\\\\n      tensor {\\\\n        dtype: DT_INT32\\\\n        tensor_shape {\\\\n          dim {\\\\n            size: 4\\\\n          }\\\\n        }\\\\n        tensor_content: &quot;\\\\\\\\001\\\\\\\\000\\\\\\\\000\\\\\\\\000\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377\\\\\\\\377&quot;\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;Slice&quot;\\\\n  op: &quot;Slice&quot;\\\\n  input: &quot;merge_3/add&quot;\\\\n  input: &quot;Slice/begin&quot;\\\\n  input: &quot;Slice/size&quot;\\\\n  attr {\\\\n    key: &quot;Index&quot;\\\\n    value {\\\\n      type: DT_INT32\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n}\\\\nnode {\\\\n  name: &quot;Squeeze&quot;\\\\n  op: &quot;Squeeze&quot;\\\\n  input: &quot;Slice&quot;\\\\n  attr {\\\\n    key: &quot;T&quot;\\\\n    value {\\\\n      type: DT_FLOAT\\\\n    }\\\\n  }\\\\n  attr {\\\\n    key: &quot;squeeze_dims&quot;\\\\n    value {\\\\n      list {\\\\n        i: 0\\\\n      }\\\\n    }\\\\n  }\\\\n}\\\\n';\\n\",\n  676         \"          }\\n\",\n  677         \"        </script>\\n\",\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\g3doc\\api_docs\\python\\functions_and_classes\\shard8\\tf.nn.conv2d.md:\n    1: ### `tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)` {#conv2d}\n    2  \n    3  Computes a 2-D convolution given 4-D `input` and `filter` tensors.\n    .\n   35  *  <b>`padding`</b>: A `string` from: `\"SAME\", \"VALID\"`.\n   36      The type of padding algorithm to use.\n   37: *  <b>`use_cudnn_on_gpu`</b>: An optional `bool`. Defaults to `True`.\n   38  *  <b>`data_format`</b>: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n   39      Specify the data format of the input and output data. With the\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\g3doc\\api_docs\\python\\nn.md:\n  260  bottom and right sides always get the one additional padded pixel. For example,\n  261  when `pad_along_height` is 5, we pad 2 pixels at the top and 3 pixels at the\n  262: bottom. Note that this is different from existing libraries such as cuDNN and\n  263  Caffe, which explicitly specify the number of padded pixels and always pad the\n  264  same number of pixels on both sides.\n  ...\n  287  - - -\n  288  \n  289: ### `tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)` {#conv2d}\n  290  \n  291  Computes a 2-D convolution given 4-D `input` and `filter` tensors.\n  ...\n  323  *  <b>`padding`</b>: A `string` from: `\"SAME\", \"VALID\"`.\n  324      The type of padding algorithm to use.\n  325: *  <b>`use_cudnn_on_gpu`</b>: An optional `bool`. Defaults to `True`.\n  326  *  <b>`data_format`</b>: An optional `string` from: `\"NHWC\", \"NCHW\"`. Defaults to `\"NHWC\"`.\n  327      Specify the data format of the input and output data. With the\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\g3doc\\get_started\\os_setup.md:\n    9  \n   10  The GPU version (Linux only) works best with Cuda Toolkit 7.5 and\n   11: cuDNN v4.  other versions are supported (Cuda toolkit >= 7.0 and\n   12: cuDNN 6.5(v2), 7.0(v3), v5) only when installing from sources.\n   13  Please see [Cuda installation](#optional-install-cuda-gpus-on-linux)\n   14  for details.\n   ..\n   63  $ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n   64  \n   65: # Ubuntu/Linux 64-bit, GPU enabled, Python 2.7. Requires CUDA toolkit 7.5 and cuDNN v4.\n   66  # For other versions, see \"Install from sources\" below.\n   67  $ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n   ..\n   78  $ sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\n   79  \n   80: # Ubuntu/Linux 64-bit, GPU enabled, Python 3.4. Requires CUDA toolkit 7.5 and cuDNN v4.\n   81  # For other versions, see \"Install from sources\" below.\n   82  $ sudo pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\n   ..\n  138  (tensorflow)$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n  139  \n  140: # Ubuntu/Linux 64-bit, GPU enabled, Python 2.7. Requires CUDA toolkit 7.5 and cuDNN v4.\n  141  # For other versions, see \"Install from sources\" below.\n  142  (tensorflow)$ pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n  ...\n  156  (tensorflow)$ pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\n  157  \n  158: # Ubuntu/Linux 64-bit, GPU enabled, Python 3.4. Requires CUDA toolkit 7.5 and cuDNN v4.\n  159  # For other versions, see \"Install from sources\" below.\n  160  (tensorflow)$ pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\n  ...\n  246  (tensorflow)$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n  247  \n  248: # Ubuntu/Linux 64-bit, GPU enabled, Python 2.7. Requires CUDA toolkit 7.5 and cuDNN v4.\n  249  # For other versions, see \"Install from sources\" below.\n  250  (tensorflow)$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n  ...\n  263  (tensorflow)$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\n  264  \n  265: # Ubuntu/Linux 64-bit, GPU enabled, Python 3.4. Requires CUDA toolkit 7.5 and cuDNN v4.\n  266  # For other versions, see \"Install from sources\" below.\n  267  (tensorflow)$ pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\n  ...\n  353  \n  354  If you installed the GPU version of TensorFlow, you must also install the Cuda\n  355: Toolkit 7.5 and cuDNN v4.  Please see [Cuda installation](#optional-install-cuda-gpus-on-linux).\n  356  \n  357  You also need to set the `LD_LIBRARY_PATH` and `CUDA_HOME` environment\n  ...\n  483  \n  484  In order to build or run TensorFlow with GPU support, both NVIDIA's Cuda Toolkit (>= 7.0) and\n  485: cuDNN (>= v2) need to be installed.\n  486  \n  487  TensorFlow GPU support requires having a GPU card with NVidia Compute Capability >= 3.0.\n  ...\n  505  Install the toolkit into e.g. `/usr/local/cuda`\n  506  \n  507: ##### Download and install cuDNN\n  508  \n  509: https://developer.nvidia.com/cudnn\n  510  \n  511: Download cuDNN v4 (v5 is currently a release candidate and is only supported when\n  512  installing TensorFlow from sources).\n  513  \n  514: Uncompress and copy the cuDNN files into the toolkit directory.  Assuming the\n  515  toolkit is installed in `/usr/local/cuda`, run the following commands (edited\n  516: to reflect the cuDNN version you downloaded):\n  517  \n  518  ``` bash\n  519: tar xvzf cudnn-7.5-linux-x64-v4.tgz\n  520: sudo cp cudnn-7.5-linux-x64-v4/cudnn.h /usr/local/cuda/include\n  521: sudo cp cudnn-7.5-linux-x64-v4/libcudnn* /usr/local/cuda/lib64\n  522: sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\n  523  ```\n  524  \n  ...\n  527  When running the `configure` script from the root of your source tree, select\n  528  the option `Y` when asked to build TensorFlow with GPU support. If you have\n  529: several versions of Cuda or cuDNN installed, you should definitely select\n  530  one explicitly instead of relying on the system default. You should see\n  531  prompts like the following:\n  ...\n  546  README.md for more details. [default is: /usr/local/cuda]: /usr/local/cuda\n  547  \n  548: Please specify the cuDNN version you want to use. [Leave empty to use system\n  549  default]: 4.0.4\n  550  \n  551: Please specify the location where the cuDNN 4.0.4 library is installed. Refer to\n  552: README.md for more details. [default is: /usr/local/cuda]: /usr/local/cudnn-r4-rc/\n  553  \n  554  Please specify a list of comma-separated Cuda compute capabilities you want to\n  ...\n  569  This creates a canonical set of symbolic links to the Cuda libraries on your system.\n  570  Every time you change the Cuda library paths you need to run this step again before\n  571: you invoke the bazel build command. For the cuDNN libraries, use '6.5' for R2, '7.0'\n  572  for R3, and '4.0.4' for R4-RC.\n  573  \n  ...\n  660  \n  661  Finally, you will also want to install the [CUDA Deep Neural\n  662: Network](https://developer.nvidia.com/cudnn) (cuDNN) library which currently\n  663  requires an [Accelerated Computing Developer\n  664  Program](https://developer.nvidia.com/accelerated-computing-developer) account.\n  ...\n  667  \n  668  ```bash\n  669: $ sudo mv include/cudnn.h /Developer/NVIDIA/CUDA-7.5/include/\n  670: $ sudo mv lib/libcudnn* /Developer/NVIDIA/CUDA-7.5/lib\n  671: $ sudo ln -s /Developer/NVIDIA/CUDA-7.5/lib/libcudnn* /usr/local/cuda/lib/\n  672  ```\n  673  \n  ...\n  692  Please specify the Cuda SDK version you want to use, e.g. 7.0. [Leave empty to use system default]: 7.5\n  693  Please specify the location where CUDA 7.5 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\n  694: Please specify the cuDNN version you want to use. [Leave empty to use system default]: 5\n  695: Please specify the location where cuDNN 5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\n  696  Please specify a list of comma-separated Cuda compute capabilities you want to build with.\n  697  You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\n  ...\n  789  \n  790  Make sure you followed the GPU installation [instructions](#optional-install-cuda-gpus-on-linux).\n  791: If you built from source, and you left the Cuda or cuDNN version empty, try specifying them\n  792  explicitly.\n  793  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\python\\kernel_tests\\pooling_ops_test.py:\n  801        return\n  802  \n  803:     # Test the GPU implementation that uses cudnn for now.\n  804      # It does not propagate the diff in cases of NaNs\n  805:     expected_input_backprop_cudnn = [\n  806          0.0, 0.0, 0.0, 0.0,\n  807          0.0, 0.0, 0.0, 0.0,\n  ...\n  809          0.0, 0.0, 0.0, 0.0]\n  810      self._testMaxPoolGradDirect(\n  811:         input_data, output_backprop, expected_input_backprop_cudnn,\n  812          input_sizes=[1, 4, 4, 1], output_sizes=[1, 3, 3, 1],\n  813          window_rows=2, window_cols=2, row_stride=1, col_stride=1,\n  ...\n  835        return\n  836  \n  837:     # Test the GPU implementation that uses cudnn for now.\n  838      # It does not propagate the diff in cases of NaNs\n  839:     expected_input_backprop_cudnn = [\n  840          0.0, 0.0, 0.0, 0.0,\n  841          0.0, 0.0, 0.0, 0.0,\n  ...\n  843          0.0, 0.0, 0.0, 0.0]\n  844      self._testMaxPoolGradDirect(\n  845:         input_data, output_backprop, expected_input_backprop_cudnn,\n  846          input_sizes=[1, 4, 4, 1], output_sizes=[1, 3, 3, 1],\n  847          window_rows=2, window_cols=2, row_stride=1, col_stride=1,\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\python\\ops\\nn.py:\n   79  bottom and right sides always get the one additional padded pixel. For example,\n   80  when `pad_along_height` is 5, we pad 2 pixels at the top and 3 pixels at the\n   81: bottom. Note that this is different from existing libraries such as cuDNN and\n   82  Caffe, which explicitly specify the number of padded pixels and always pad the\n   83  same number of pixels on both sides.\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\python\\ops\\nn_grad.py:\n   43                                          op.inputs[2], op.get_attr(\"strides\"),\n   44                                          op.get_attr(\"padding\"),\n   45:                                         op.get_attr(\"use_cudnn_on_gpu\"),\n   46                                          op.get_attr(\"data_format\")),\n   47            nn_ops.conv2d(grad, op.inputs[1], op.get_attr(\"strides\"),\n   48:                         op.get_attr(\"padding\"), op.get_attr(\"use_cudnn_on_gpu\"),\n   49                          op.get_attr(\"data_format\"))]\n   50  \n   ..\n   57            op.get_attr(\"strides\"),\n   58            op.get_attr(\"padding\"),\n   59:           op.get_attr(\"use_cudnn_on_gpu\"),\n   60            op.get_attr(\"data_format\")),\n   61        None,\n   ..\n   64            op.get_attr(\"strides\"),\n   65            op.get_attr(\"padding\"),\n   66:           op.get_attr(\"use_cudnn_on_gpu\"),\n   67            op.get_attr(\"data_format\"))\n   68    ]\n   ..\n  268    return [nn_ops.conv2d_backprop_input(\n  269        array_ops.shape(op.inputs[0]), op.inputs[1], grad, op.get_attr(\"strides\"),\n  270:       op.get_attr(\"padding\"), op.get_attr(\"use_cudnn_on_gpu\"),\n  271        op.get_attr(\"data_format\")),\n  272            nn_ops.conv2d_backprop_filter(op.inputs[0],\n  ...\n  274                                          op.get_attr(\"strides\"),\n  275                                          op.get_attr(\"padding\"),\n  276:                                         op.get_attr(\"use_cudnn_on_gpu\"),\n  277                                          op.get_attr(\"data_format\"))]\n  278  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\python\\ops\\nn_ops.py:\n 1078  \n 1079  def conv1d(value, filters, stride, padding,\n 1080:            use_cudnn_on_gpu=None, data_format=None,\n 1081             name=None):\n 1082    \"\"\"Computes a 1-D convolution given 3-D input and filter tensors.\n ....\n 1102        the filter is moved right at each step.\n 1103      padding: 'SAME' or 'VALID'\n 1104:     use_cudnn_on_gpu: An optional `bool`.  Defaults to `True`.\n 1105      data_format: An optional `string` from `\"NHWC\", \"NCHW\"`.  Defaults\n 1106        to `\"NHWC\"`, the data is stored in the order of\n ....\n 1118      filters = array_ops.expand_dims(filters, 0)\n 1119      result = gen_nn_ops.conv2d(value, filters, [1, 1, stride, 1], padding,\n 1120:                                use_cudnn_on_gpu=use_cudnn_on_gpu,\n 1121                                 data_format=data_format)\n 1122      return array_ops.squeeze(result, [1])\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\stream_executor\\BUILD:\n   29          \"//tensorflow/core:cuda\",\n   30          \"//third_party/gpus/cuda:cublas\",\n   31:         \"//third_party/gpus/cuda:cudnn\",\n   32          \"//third_party/gpus/cuda:cufft\",\n   33      ],\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:\n   40  #include \"tensorflow/stream_executor/stream_executor_pimpl.h\"\n   41  // clang-format off\n   42: #include \"third_party/gpus/cuda/include/cudnn.h\"\n   43  // clang-format on\n   44  \n   ..\n   67  namespace cuda {\n   68  \n   69: PLUGIN_REGISTRY_DEFINE_PLUGIN_ID(kCuDnnPlugin);\n   70  \n   71: string ToString(cudnnStatus_t status) {\n   72    switch (status) {\n   73:     case CUDNN_STATUS_SUCCESS:\n   74:       return \"CUDNN_STATUS_SUCCESS\";\n   75:     case CUDNN_STATUS_NOT_INITIALIZED:\n   76:       return \"CUDNN_STATUS_NOT_INITIALIZED\";\n   77:     case CUDNN_STATUS_ALLOC_FAILED:\n   78:       return \"CUDNN_STATUS_ALLOC_FAILED\";\n   79:     case CUDNN_STATUS_BAD_PARAM:\n   80:       return \"CUDNN_STATUS_BAD_PARAM\";\n   81:     case CUDNN_STATUS_INTERNAL_ERROR:\n   82:       return \"CUDNN_STATUS_INTERNAL_ERROR\";\n   83:     case CUDNN_STATUS_INVALID_VALUE:\n   84:       return \"CUDNN_STATUS_INVALID_VALUE\";\n   85:     case CUDNN_STATUS_ARCH_MISMATCH:\n   86:       return \"CUDNN_STATUS_ARCH_MISMATCH\";\n   87:     case CUDNN_STATUS_MAPPING_ERROR:\n   88:       return \"CUDNN_STATUS_MAPPING_ERROR\";\n   89:     case CUDNN_STATUS_EXECUTION_FAILED:\n   90:       return \"CUDNN_STATUS_EXECUTION_FAILED\";\n   91:     case CUDNN_STATUS_NOT_SUPPORTED:\n   92:       return \"CUDNN_STATUS_NOT_SUPPORTED\";\n   93:     case CUDNN_STATUS_LICENSE_ERROR:\n   94:       return \"CUDNN_STATUS_LICENSE_ERROR\";\n   95      default:\n   96:       return port::StrCat(\"<unknown cudnn status: \", static_cast<int>(status),\n   97                            \">\");\n   98    }\n   ..\n  101  namespace dynload {\n  102  \n  103: static port::ThreadPool* InitCudnnThreadpool() {\n  104:   port::ThreadPool* cudnn_threadpool_;\n  105    port::ThreadOptions options;\n  106    // TBD(keveman): Conservatively setting the stack size and guard size to 2MB,\n  ...\n  109    options.stack_size = 2 * 1024 * 1024;\n  110    options.guard_size = 2 * 1024 * 1024;\n  111:   cudnn_threadpool_ = new port::ThreadPool(port::Env::Default(), options,\n  112:                                            \"cudnn_threadpool\", 1);\n  113:   CHECK(cudnn_threadpool_);\n  114:   return cudnn_threadpool_;\n  115  }\n  116  \n  117: static mutex cudnn_threadpool_mu(LINKER_INITIALIZED);\n  118  static port::ThreadPool* GetCudaThreadpool() {\n  119:   mutex_lock lock(cudnn_threadpool_mu);\n  120:   static port::ThreadPool* cudnn_threadpool = InitCudnnThreadpool();\n  121:   return cudnn_threadpool;\n  122  }\n  123  \n  124: // Retrieves the CUDNN DSO, dies on failure.\n  125  void* GetDsoHandle() {\n  126:   static auto result = internal::CachedDsoLoader::GetCudnnDsoHandle();\n  127    return result.ValueOrDie();\n  128  }\n  129  \n  130: // Calls cudnnGetVersion in the loaded DSO.\n  131: size_t cudnnGetVersion() {\n  132:   static void* f = dlsym(GetDsoHandle(), \"cudnnGetVersion\");\n  133    if (f == nullptr) {\n  134:     LOG(FATAL) << \"could not find cudnnGetVersion in cudnn DSO; dlerror: \"\n  135                 << dlerror();\n  136    }\n  ...\n  139  }\n  140  \n  141: // Returns whether the currently loaded cuDNN version is R2.\n  142: bool IsCudnnR2() {\n  143:   static auto version = cudnnGetVersion();\n  144    DCHECK_GE(version, 2000);\n  145    return version < 3000;\n  146  }\n  147  \n  148: #define PERFTOOLS_GPUTOOLS_CUDNN_WRAP(__name)                        \\\n  149    struct DynLoadShim__##__name {                                     \\\n  150      static const char* kName;                                        \\\n  ...\n  154        if (f == nullptr) {                                            \\\n  155          LOG(FATAL) << \"could not find \" << kName                     \\\n  156:                    << \" in cudnn DSO; dlerror: \" << dlerror();       \\\n  157        }                                                              \\\n  158        return reinterpret_cast<FuncPointerT>(f);                      \\\n  159      }                                                                \\\n  160      template <typename... Args>                                      \\\n  161:     cudnnStatus_t operator()(CUDAExecutor* parent, Args... args) {   \\\n  162        cuda::ScopedActivateExecutorContext sac{parent};               \\\n  163:       cudnnStatus_t retval = DynLoad()(args...);                     \\\n  164        return retval;                                                 \\\n  165      }                                                                \\\n  ...\n  168  \n  169  // clang-format off\n  170: #define CUDNN_DNN_ROUTINE_EACH(__macro)                   \\\n  171:   __macro(cudnnGetConvolutionNdForwardOutputDim)          \\\n  172:   __macro(cudnnGetConvolutionForwardAlgorithm)            \\\n  173:   __macro(cudnnCreateTensorDescriptor)                    \\\n  174:   __macro(cudnnDestroyTensorDescriptor)                   \\\n  175:   __macro(cudnnCreateFilterDescriptor)                    \\\n  176:   __macro(cudnnSetPoolingNdDescriptor)                    \\\n  177:   __macro(cudnnDestroyFilterDescriptor)                   \\\n  178:   __macro(cudnnCreateConvolutionDescriptor)               \\\n  179:   __macro(cudnnCreatePoolingDescriptor)                   \\\n  180:   __macro(cudnnDestroyPoolingDescriptor)                  \\\n  181:   __macro(cudnnDestroyConvolutionDescriptor)              \\\n  182:   __macro(cudnnCreate)                                    \\\n  183:   __macro(cudnnDestroy)                                   \\\n  184:   __macro(cudnnSetStream)                                 \\\n  185:   __macro(cudnnActivationForward)                         \\\n  186:   __macro(cudnnConvolutionForward)                        \\\n  187:   __macro(cudnnConvolutionBackwardBias)                   \\\n  188:   __macro(cudnnGetConvolutionForwardWorkspaceSize)        \\\n  189:   __macro(cudnnTransformTensor)                           \\\n  190:   __macro(cudnnSetConvolutionNdDescriptor)                \\\n  191:   __macro(cudnnSetTensorNdDescriptor)                     \\\n  192:   __macro(cudnnSetFilterNdDescriptor)                     \\\n  193:   __macro(cudnnPoolingForward)                            \\\n  194:   __macro(cudnnPoolingBackward)\n  195  // clang-format on\n  196  \n  197: CUDNN_DNN_ROUTINE_EACH(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n  198  \n  199  // clang-format off\n  200: #if CUDNN_VERSION >= 4000 && CUDNN_VERSION < 5000\n  201: #define CUDNN_DNN_ROUTINE_EACH_R2(__macro)                \\\n  202:   __macro(cudnnAddTensor_v2)                              \\\n  203:   __macro(cudnnConvolutionBackwardData_v2)                \\\n  204:   __macro(cudnnConvolutionBackwardFilter_v2)\n  205  #else\n  206: #define CUDNN_DNN_ROUTINE_EACH_R2(__macro)                \\\n  207:   __macro(cudnnAddTensor)                                 \\\n  208:   __macro(cudnnConvolutionBackwardData)                   \\\n  209:   __macro(cudnnConvolutionBackwardFilter)\n  210  #endif\n  211  // clang-format on\n  212  \n  213: CUDNN_DNN_ROUTINE_EACH_R2(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n  214  \n  215  // APIs available after R3:\n  216: #if CUDNN_VERSION >= 3000\n  217: #define CUDNN_DNN_ROUTINE_EACH_AFTER_R3(__macro)              \\\n  218:   __macro(cudnnGetConvolutionBackwardFilterWorkspaceSize)     \\\n  219:   __macro(cudnnGetConvolutionBackwardDataAlgorithm)           \\\n  220:   __macro(cudnnGetConvolutionBackwardFilterAlgorithm)         \\\n  221:   __macro(cudnnGetConvolutionBackwardDataWorkspaceSize)\n  222: CUDNN_DNN_ROUTINE_EACH_AFTER_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n  223: #undef CUDNN_DNN_ROUTINE_EACH_AFTER_R3\n  224  #endif\n  225  \n  226  // APIs in R3 but not in R5\n  227  // clang-format off\n  228: #if CUDNN_VERSION >= 3000 && CUDNN_VERSION < 5000\n  229: #define CUDNN_DNN_ROUTINE_EACH_R3(__macro)                    \\\n  230:   __macro(cudnnAddTensor_v3)                                  \\\n  231:   __macro(cudnnConvolutionBackwardData_v3)                    \\\n  232:   __macro(cudnnConvolutionBackwardFilter_v3)\n  233  // clang-format on\n  234  \n  235: CUDNN_DNN_ROUTINE_EACH_R3(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n  236: #undef CUDNN_DNN_ROUTINE_EACH_R3\n  237  #endif\n  238  \n  239  // APIs in R5\n  240  // clang-format off\n  241: #if CUDNN_VERSION >= 5000\n  242: #define CUDNN_DNN_ROUTINE_EACH_R5(__macro)                    \\\n  243:   __macro(cudnnCreateActivationDescriptor)                    \\\n  244:   __macro(cudnnSetActivationDescriptor)                       \\\n  245:   __macro(cudnnGetActivationDescriptor)                       \\\n  246:   __macro(cudnnDestroyActivationDescriptor)\n  247  // clang-format on\n  248  \n  249: CUDNN_DNN_ROUTINE_EACH_R5(PERFTOOLS_GPUTOOLS_CUDNN_WRAP)\n  250: #undef CUDNN_DNN_ROUTINE_EACH_R5\n  251  #endif\n  252  \n  253: #undef CUDNN_DNN_ROUTINE_EACH\n  254  \n  255  }  // namespace dynload\n  ...\n  257  namespace {\n  258  \n  259: cudnnHandle_t ToHandle(void* opaque_handle) {\n  260:   return static_cast<cudnnHandle_t>(opaque_handle);\n  261  }\n  262  \n  263: cudnnConvolutionFwdAlgo_t ToConvForwardAlgo(dnn::AlgorithmType algorithm) {\n  264:   cudnnConvolutionFwdAlgo_t algo = cudnnConvolutionFwdAlgo_t(algorithm);\n  265    switch (algo) {\n  266:     case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM:\n  267:     case CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM:\n  268:     case CUDNN_CONVOLUTION_FWD_ALGO_GEMM:\n  269:     case CUDNN_CONVOLUTION_FWD_ALGO_DIRECT:\n  270:     case CUDNN_CONVOLUTION_FWD_ALGO_FFT:\n  271:     case CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING:\n  272: #if CUDNN_VERSION >= 5000\n  273:     case CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD:\n  274  #endif\n  275        return algo;\n  276      default:\n  277:       LOG(FATAL) << \"Unsupported Cudnn convolution forward algorithm: \"\n  278                   << algorithm;\n  279    }\n  280  }\n  281  \n  282: cudnnConvolutionBwdDataAlgo_t ToConvBackwardDataAlgo(\n  283      dnn::AlgorithmType algorithm) {\n  284:   cudnnConvolutionBwdDataAlgo_t algo = cudnnConvolutionBwdDataAlgo_t(algorithm);\n  285    switch (algo) {\n  286:     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_0:\n  287:     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_1:\n  288:     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT:\n  289:     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING:\n  290: #if CUDNN_VERSION >= 5000\n  291:     case CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD:\n  292  #endif\n  293        return algo;\n  294      default:\n  295        LOG(FATAL)\n  296:           << \"Unsupported Cudnn convolution backward algorithm for data: \"\n  297            << algorithm;\n  298    }\n  299  }\n  300  \n  301: cudnnConvolutionBwdFilterAlgo_t ToConvBackwardFilterAlgo(\n  302      dnn::AlgorithmType algorithm) {\n  303:   cudnnConvolutionBwdFilterAlgo_t algo =\n  304:       cudnnConvolutionBwdFilterAlgo_t(algorithm);\n  305    switch (algo) {\n  306:     case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0:\n  307:     case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1:\n  308:     case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT:\n  309:     case CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3:\n  310        return algo;\n  311      default:\n  312        LOG(FATAL)\n  313:           << \"Unsupported Cudnn convolution backward algorithm for filter: \"\n  314            << algorithm;\n  315    }\n  ...\n  318  }  // namespace\n  319  \n  320: CudnnSupport::CudnnSupport(CUDAExecutor* parent)\n  321      : parent_(parent), dnn_handle_(nullptr) {}\n  322  \n  323: CudnnSupport::~CudnnSupport() {\n  324:   auto status = dynload::cudnnDestroy(parent_, ToHandle(dnn_handle_));\n  325:   if (status != CUDNN_STATUS_SUCCESS) {\n  326:     LOG(ERROR) << \"could not destroy cudnn handle: \" << ToString(status);\n  327    }\n  328  }\n  329  \n  330: port::Status CudnnSupport::Init() {\n  331:   auto status = dynload::cudnnCreate(\n  332:       parent_, reinterpret_cast<cudnnHandle_t*>(&dnn_handle_));\n  333:   if (status == CUDNN_STATUS_SUCCESS) {\n  334:     // Check whether loaded version of CuDNN matches what the source\n  335      // was built with.\n  336:     size_t loaded_version = dynload::cudnnGetVersion();\n  337:     bool library_loaded_matches_source = (loaded_version == CUDNN_VERSION);\n  338      if (!library_loaded_matches_source) {\n  339        const string error =\n  340:           port::StrCat(\"Loaded cudnn library: \", loaded_version,\n  341:                        \" but source was compiled against \", CUDNN_VERSION,\n  342:                        \".  If using a binary install, upgrade your cudnn \"\n  343                         \"library to match.  If building from sources, \"\n  344                         \"make sure the library loaded matches the \"\n  ...\n  351    }\n  352  \n  353:   LOG(ERROR) << \"could not create cudnn handle: \" << ToString(status);\n  354:   if (status == CUDNN_STATUS_NOT_INITIALIZED) {\n  355      // This is the error code that the driver returns when we're not running a\n  356:     // sufficient CUDA driver -- cudnn requires 6.5+ compatibility, which\n  357      // starts with the 340.XX driver series.\n  358      auto result = cuda::Diagnostician::FindKernelDriverVersion();\n  ...\n  367        if (std::get<0>(version) < 340) {\n  368          LOG(ERROR)\n  369:             << \"cudnn library is only supported on 340.XX+ driver versions\";\n  370        }\n  371  #endif\n  ...\n  374  \n  375    return port::Status{port::error::INTERNAL,\n  376:                       port::StrCat(\"cudnn library could not create a handle: \",\n  377                                     ToString(status))};\n  378  }\n  379  \n  380: // Turns a BatchDescriptor structure into a cudnn tensor handle within a scope.\n  381  class ScopedTensorDescriptor {\n  382   public:\n  383    ScopedTensorDescriptor(CUDAExecutor* parent,\n  384                           const BatchDescriptor& batch_descriptor,\n  385:                          cudnnDataType_t elem_type)\n  386        : parent_(parent), handle_(nullptr) {\n  387:     cudnnStatus_t status =\n  388:         dynload::cudnnCreateTensorDescriptor(parent_, &handle_);\n  389:     if (status != CUDNN_STATUS_SUCCESS) {\n  390:       LOG(FATAL) << \"could not create cudnn tensor descriptor: \"\n  391                   << ToString(status);\n  392      }\n  ...\n  403  \n  404      const int nd = batch_descriptor.ndims() + 2;\n  405:     // cuDNN requires the strides and dims to be ordered as BDYX.\n  406      std::vector<int64> strides64 =\n  407          batch_descriptor.full_strides(dnn::DataLayout::kBatchDepthYX);\n  ...\n  409          batch_descriptor.full_dims(dnn::DataLayout::kBatchDepthYX);\n  410  \n  411:     // cuDNN requires arrays of ints.\n  412      std::vector<int> strides(nd);\n  413      std::vector<int> dims(nd);\n  ...\n  416      std::transform(dims64.cbegin(), dims64.cend(), dims.begin(),\n  417                     &CheckedNarrowing<int64, int>);\n  418:     status = dynload::cudnnSetTensorNdDescriptor(\n  419          parent_, handle_, elem_type, nd, dims.data(), strides.data());\n  420  \n  421:     if (status != CUDNN_STATUS_SUCCESS) {\n  422:       LOG(FATAL) << \"could not set cudnn tensor descriptor: \"\n  423                   << ToString(status);\n  424      }\n  ...\n  426  \n  427    ~ScopedTensorDescriptor() {\n  428:     cudnnStatus_t status =\n  429:         dynload::cudnnDestroyTensorDescriptor(parent_, handle_);\n  430:     if (status != CUDNN_STATUS_SUCCESS) {\n  431:       LOG(ERROR) << \"could not destroy cudnn tensor descriptor: \"\n  432                   << ToString(status);\n  433      }\n  434    }\n  435  \n  436:   cudnnTensorDescriptor_t handle() const { return handle_; }\n  437  \n  438   private:\n  439    CUDAExecutor* parent_;            // Parent executor. Not owned.\n  440:   cudnnTensorDescriptor_t handle_;  // Owned.\n  441  \n  442    SE_DISALLOW_COPY_AND_ASSIGN(ScopedTensorDescriptor);\n  443  };\n  444  \n  445: // Turns a FilterDescriptor structure into a cudnn filter handle within a scope.\n  446  class ScopedFilterDescriptor {\n  447   public:\n  ...\n  449                           const FilterDescriptor& filter_descriptor,\n  450                           const BatchDescriptor& batch_descriptor,\n  451:                          cudnnDataType_t elem_type)\n  452        : parent_(parent), handle_(nullptr) {\n  453:     cudnnStatus_t status =\n  454:         dynload::cudnnCreateFilterDescriptor(parent_, &handle_);\n  455:     if (status != CUDNN_STATUS_SUCCESS) {\n  456:       LOG(FATAL) << \"could not create cudnn filter descriptor: \"\n  457                   << ToString(status);\n  458      }\n  459  \n  460: #if CUDNN_VERSION >= 5000\n  461      // TODO(b/23032134): Even if the filter layout is not supported,\n  462:     // cudnnSetFilter4DDescriptor_v4 will return CUDNN_STATUS_SUCCESS because it\n  463:     // does not take layout as an input. Maybe force cuDNN by giving wrong\n  464      // inputs intentionally?\n  465:     cudnnTensorFormat_t format;\n  466      switch (filter_descriptor.layout()) {\n  467        case dnn::FilterLayout::kOutputInputYX:\n  468:         format = CUDNN_TENSOR_NCHW;\n  469          break;\n  470        default:\n  ...\n  481      std::copy(spatial_dims.begin(), spatial_dims.end(), dims.begin() + 2);\n  482  \n  483:     status = dynload::cudnnSetFilterNdDescriptor(parent_, handle_, elem_type,\n  484: #if CUDNN_VERSION >= 5000\n  485                                                   format,\n  486  #endif\n  487                                                   dims.size(), dims.data());\n  488:     if (status != CUDNN_STATUS_SUCCESS) {\n  489:       LOG(FATAL) << \"could not set cudnn filter descriptor: \"\n  490                   << ToString(status);\n  491      }\n  ...\n  493  \n  494    ~ScopedFilterDescriptor() {\n  495:     cudnnStatus_t status =\n  496:         dynload::cudnnDestroyFilterDescriptor(parent_, handle_);\n  497:     if (status != CUDNN_STATUS_SUCCESS) {\n  498:       LOG(ERROR) << \"could not destroy cudnn filter descriptor: \"\n  499                   << ToString(status);\n  500      }\n  501    }\n  502  \n  503:   cudnnFilterDescriptor_t handle() const { return handle_; }\n  504  \n  505   private:\n  ...\n  507    CUDAExecutor* parent_;\n  508  \n  509:   // cudnn filter descriptor this object creates. Owned.\n  510:   cudnnFilterDescriptor_t handle_;\n  511  \n  512    SE_DISALLOW_COPY_AND_ASSIGN(ScopedFilterDescriptor);\n  513  };\n  514  \n  515: // Turns a ConvolutionDescriptor structure into a cudnn convolution handle\n  516  // within a scope.\n  517  class ScopedConvolutionDescriptor {\n  ...\n  519    ScopedConvolutionDescriptor(\n  520        CUDAExecutor* parent, const ConvolutionDescriptor& convolution_descriptor,\n  521:       cudnnDataType_t data_type)\n  522        : parent_(parent), handle_(nullptr) {\n  523:     cudnnStatus_t status =\n  524:         dynload::cudnnCreateConvolutionDescriptor(parent_, &handle_);\n  525:     if (status != CUDNN_STATUS_SUCCESS) {\n  526:       LOG(FATAL) << \"could not create cudnn convolution descriptor: \"\n  527                   << ToString(status);\n  528      }\n  ...\n  530      const auto& padding64 = convolution_descriptor.padding();\n  531  \n  532:     // cuDNN requires arrays of ints.\n  533      std::vector<int> strides(convolution_descriptor.ndims());\n  534      std::vector<int> padding(convolution_descriptor.ndims());\n  ...\n  539      std::vector<int> upscale(convolution_descriptor.ndims(), 1);\n  540  \n  541:     status = dynload::cudnnSetConvolutionNdDescriptor(\n  542          parent_, handle_, convolution_descriptor.ndims(), padding.data(),\n  543          strides.data(), upscale.data(),\n  544:         // NOTE(keveman): cuDNN supports convolution and cross correlation.\n  545          // However, almost all the use cases do cross correlation, so just\n  546          // hard coding it here.\n  547:         CUDNN_CROSS_CORRELATION, data_type);\n  548  \n  549:     if (status != CUDNN_STATUS_SUCCESS) {\n  550:       LOG(FATAL) << \"could not set cudnn convolution descriptor: \"\n  551                   << ToString(status);\n  552      }\n  ...\n  554  \n  555    ~ScopedConvolutionDescriptor() {\n  556:     cudnnStatus_t status =\n  557:         dynload::cudnnDestroyConvolutionDescriptor(parent_, handle_);\n  558:     if (status != CUDNN_STATUS_SUCCESS) {\n  559:       LOG(ERROR) << \"could not destroy cudnn convolution descriptor: \"\n  560                   << ToString(status);\n  561      }\n  562    }\n  563  \n  564:   cudnnConvolutionDescriptor_t handle() const { return handle_; }\n  565  \n  566   private:\n  567    CUDAExecutor* parent_;                 // Parent executor. Not owned.\n  568:   cudnnConvolutionDescriptor_t handle_;  // Owned.\n  569  \n  570    SE_DISALLOW_COPY_AND_ASSIGN(ScopedConvolutionDescriptor);\n  571  };\n  572  \n  573: // Turns a PoolingDescriptor structure into a cudnn pooling descriptor handle\n  574  // within a scope.\n  575  class ScopedPoolingDescriptor {\n  ...\n  578                            const PoolingDescriptor& pooling_descriptor)\n  579        : parent_(parent), handle_(nullptr) {\n  580:     cudnnStatus_t status =\n  581:         dynload::cudnnCreatePoolingDescriptor(parent_, &handle_);\n  582:     if (status != CUDNN_STATUS_SUCCESS) {\n  583:       LOG(FATAL) << \"could not create cudnn pooling descriptor: \"\n  584                   << ToString(status);\n  585      }\n  ...\n  599      std::transform(shape64.cbegin(), shape64.cend(), shape.begin(),\n  600                     &CheckedNarrowing<int64, int>);\n  601:     status = dynload::cudnnSetPoolingNdDescriptor(\n  602          parent_, handle_,\n  603          (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum\n  604:              ? CUDNN_POOLING_MAX\n  605:              : CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING),\n  606: #if CUDNN_VERSION >= 5000\n  607          // Always propagate nans.\n  608:         CUDNN_PROPAGATE_NAN,\n  609  #endif\n  610          nd, shape.data(), padding.data(), strides.data());\n  611:     if (status != CUDNN_STATUS_SUCCESS) {\n  612:       LOG(FATAL) << \"could not set cudnn pooling descriptor: \"\n  613                   << ToString(status);\n  614      }\n  615    }\n  616    ~ScopedPoolingDescriptor() {\n  617:     cudnnStatus_t status =\n  618:         dynload::cudnnDestroyPoolingDescriptor(parent_, handle_);\n  619:     if (status != CUDNN_STATUS_SUCCESS) {\n  620:       LOG(ERROR) << \"could not destroy cudnn pooling descriptor: \"\n  621                   << ToString(status);\n  622      }\n  623    }\n  624  \n  625:   cudnnPoolingDescriptor_t handle() const { return handle_; }\n  626  \n  627   private:\n  628    CUDAExecutor* parent_;             // Parent executor. Not owned.\n  629:   cudnnPoolingDescriptor_t handle_;  // Owned.\n  630  \n  631    SE_DISALLOW_COPY_AND_ASSIGN(ScopedPoolingDescriptor);\n  632  };\n  633  \n  634: #if CUDNN_VERSION >= 5000\n  635: // Turns a ActivationDescriptor structure into a cudnn activation\n  636  // descriptor handle within a scope.\n  637  class ScopedActivationDescriptor {\n  ...\n  641                               double value_max)\n  642        : parent_(parent), handle_(nullptr) {\n  643:     cudnnStatus_t status =\n  644:         dynload::cudnnCreateActivationDescriptor(parent_, &handle_);\n  645:     if (status != CUDNN_STATUS_SUCCESS) {\n  646:       LOG(FATAL) << \"could not create cudnn activation descriptor: \"\n  647                   << ToString(status);\n  648      }\n  649  \n  650      double relu_ceiling = 0.0;\n  651:     cudnnActivationMode_t mode;\n  652      switch (activation_mode) {\n  653        case dnn::ActivationMode::kRelu6:\n  654          relu_ceiling = 6.0;\n  655:         mode = CUDNN_ACTIVATION_CLIPPED_RELU;\n  656          break;\n  657        case dnn::ActivationMode::kReluX:\n  658          relu_ceiling = value_max;\n  659:         mode = CUDNN_ACTIVATION_CLIPPED_RELU;\n  660          break;\n  661        case dnn::ActivationMode::kRelu:\n  662:         mode = CUDNN_ACTIVATION_RELU;\n  663          break;\n  664        case dnn::ActivationMode::kSigmoid:\n  665:         mode = CUDNN_ACTIVATION_SIGMOID;\n  666          break;\n  667        case dnn::ActivationMode::kTanh:\n  668:         mode = CUDNN_ACTIVATION_TANH;\n  669          break;\n  670        default:\n  ...\n  674  \n  675      // Always propagate nans.\n  676:     cudnnNanPropagation_t nan_propagation = CUDNN_PROPAGATE_NAN;\n  677:     status = dynload::cudnnSetActivationDescriptor(\n  678          parent_, handle_,\n  679          mode, nan_propagation, relu_ceiling);\n  680:     if (status != CUDNN_STATUS_SUCCESS) {\n  681:       LOG(FATAL) << \"could not set cudnn activation descriptor: \"\n  682                   << ToString(status);\n  683      }\n  ...\n  685  \n  686    ~ScopedActivationDescriptor() {\n  687:     cudnnStatus_t status =\n  688:         dynload::cudnnDestroyActivationDescriptor(parent_, handle_);\n  689:     if (status != CUDNN_STATUS_SUCCESS) {\n  690:       LOG(ERROR) << \"could not destroy cudnn activation descriptor: \"\n  691                   << ToString(status);\n  692      }\n  693    }\n  694  \n  695:   cudnnActivationDescriptor_t handle() const { return handle_; }\n  696  \n  697   private:\n  698    CUDAExecutor* parent_;                // Parent executor. Not owned.\n  699:   cudnnActivationDescriptor_t handle_;  // Owned.\n  700  \n  701    SE_DISALLOW_COPY_AND_ASSIGN(ScopedActivationDescriptor);\n  ...\n  704  \n  705  template <class T>\n  706: bool CudnnSupport::DoConvolveImpl(\n  707:     Stream* stream, int cudnn_type,  // Actually cudnnDataType_t.\n  708      const BatchDescriptor& batch_descriptor, const DeviceMemory<T>& input_data,\n  709      const FilterDescriptor& filter_descriptor,\n  ...\n  715      dnn::ProfileResult* output_profile_result) {\n  716    ScopedTensorDescriptor input_nd{parent_, batch_descriptor,\n  717:       static_cast<cudnnDataType_t>(cudnn_type)};\n  718    ScopedTensorDescriptor output_nd{parent_, output_descriptor,\n  719:       static_cast<cudnnDataType_t>(cudnn_type)};\n  720    ScopedFilterDescriptor filter{parent_, filter_descriptor, batch_descriptor,\n  721:       static_cast<cudnnDataType_t>(cudnn_type)};\n  722:   // TODO(sesse): Figure out under what circumstances cuDNN would\n  723:   // accept CUDNN_DATA_HALF here; probably related to compute capability\n  724:   // and cuDNN version; at least cuDNN 4 on TITAN X only supports\n  725:   // CUDNN_DATA_FLOAT even for half input.\n  726    ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n  727:       CUDNN_DATA_FLOAT};\n  728  \n  729    mutex_lock lock{dnn_handle_mutex_};\n  730:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n  731                                          AsCUDAStreamValue(stream));\n  732:   if (status != CUDNN_STATUS_SUCCESS) {\n  733:     LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n  734    }\n  735    // Alpha is the scaling factor for input.\n  ...\n  739  \n  740    const bool is_profiling = output_profile_result != nullptr;\n  741:   cudnnConvolutionFwdAlgo_t algo;\n  742    DeviceMemory<uint8> scratch;\n  743  \n  744    if (algorithm_config.algorithm() == dnn::kDefaultAlgorithm) {\n  745:     // With the default algorithm, use Cudnn's heuristics.\n  746      auto get_algorithm = [&](bool specify_limit)\n  747          SHARED_LOCKS_REQUIRED(dnn_handle_mutex_) {\n  748:           cudnnConvolutionFwdPreference_t preference =\n  749:               specify_limit ? CUDNN_CONVOLUTION_FWD_SPECIFY_WORKSPACE_LIMIT\n  750:                             : CUDNN_CONVOLUTION_FWD_NO_WORKSPACE;\n  751  \n  752            auto memory_limit_bytes =\n  ...\n  758            }\n  759  \n  760:           cudnnConvolutionFwdAlgo_t algo_to_use;\n  761:           status = dynload::cudnnGetConvolutionForwardAlgorithm(\n  762                parent_, ToHandle(dnn_handle_), input_nd.handle(),\n  763                filter.handle(), conv.handle(), output_nd.handle(),\n  ...\n  765                /*memoryLimitInBytes=*/memory_limit_bytes,\n  766                /*algo=*/&algo_to_use);\n  767:           CHECK_EQ(status, CUDNN_STATUS_SUCCESS)\n  768                << \"Unable to find a suitable \"\n  769                   \"algorithm for doing forward \"\n  ...\n  776      if (scratch_allocator != nullptr) {\n  777        size_t size_in_bytes;\n  778:       status = dynload::cudnnGetConvolutionForwardWorkspaceSize(\n  779            parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n  780            /*filterDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n  781            /*destDesc=*/output_nd.handle(), /*algo=*/algo,\n  782            /*sizeInBytes=*/&size_in_bytes);\n  783:       if (status == CUDNN_STATUS_SUCCESS && size_in_bytes != 0) {\n  784          auto allocated =\n  785              scratch_allocator->AllocateBytes(stream, size_in_bytes);\n  ...\n  800  \n  801      size_t size_in_bytes;\n  802:     status = dynload::cudnnGetConvolutionForwardWorkspaceSize(\n  803          parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n  804          /*filterDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n  805          /*destDesc=*/output_nd.handle(), /*algo=*/algo,\n  806          /*sizeInBytes=*/&size_in_bytes);\n  807:     if (status != CUDNN_STATUS_SUCCESS) {\n  808        if (is_profiling) {\n  809          // Silently return when we are profiling.\n  ...\n  840      timer.reset(new CUDATimer(parent_));\n  841      timer->Init();\n  842:     // The start and stop of the timer should be as close to the Cudnn call as\n  843      // possible. It is still possible for other threads to issue workload on\n  844      // to this stream. So it could take multiple profiling measurements.\n  845      timer->Start(AsCUDAStream(stream));\n  846    }\n  847:   status = dynload::cudnnConvolutionForward(\n  848        parent_, ToHandle(dnn_handle_),\n  849        /*alpha=*/&alpha, /*srcDesc=*/input_nd.handle(),\n  ...\n  862    }\n  863  \n  864:   if (status != CUDNN_STATUS_SUCCESS) {\n  865      // Silently return when we are profiling.\n  866      if (!is_profiling) {\n  ...\n  874  }\n  875  \n  876: bool CudnnSupport::GetConvolveAlgorithms(\n  877      std::vector<dnn::AlgorithmType>* out_algorithms) {\n  878    out_algorithms->assign({\n  879        // clang-format off\n  880:       CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM,\n  881:       CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM,\n  882:       CUDNN_CONVOLUTION_FWD_ALGO_GEMM,\n  883:       CUDNN_CONVOLUTION_FWD_ALGO_DIRECT,\n  884:       CUDNN_CONVOLUTION_FWD_ALGO_FFT,\n  885:       CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING,\n  886: #if CUDNN_VERSION >= 5000\n  887:       CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD,\n  888  #endif\n  889        // clang-format on\n  ...\n  892  }\n  893  \n  894: bool CudnnSupport::GetConvolveBackwardDataAlgorithms(\n  895      std::vector<dnn::AlgorithmType>* out_algorithms) {\n  896    out_algorithms->assign({\n  897        // clang-format off\n  898:       CUDNN_CONVOLUTION_BWD_DATA_ALGO_0,\n  899:       CUDNN_CONVOLUTION_BWD_DATA_ALGO_1,\n  900:       CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT,\n  901:       CUDNN_CONVOLUTION_BWD_DATA_ALGO_FFT_TILING,\n  902: #if CUDNN_VERSION >= 5000\n  903:       CUDNN_CONVOLUTION_BWD_DATA_ALGO_WINOGRAD,\n  904  #endif\n  905        // clang-format on\n  ...\n  908  }\n  909  \n  910: bool CudnnSupport::GetConvolveBackwardFilterAlgorithms(\n  911      std::vector<dnn::AlgorithmType>* out_algorithms) {\n  912    out_algorithms->assign({\n  913        // clang-format off\n  914:       CUDNN_CONVOLUTION_BWD_FILTER_ALGO_0,\n  915:       CUDNN_CONVOLUTION_BWD_FILTER_ALGO_1,\n  916:       CUDNN_CONVOLUTION_BWD_FILTER_ALGO_FFT,\n  917:       CUDNN_CONVOLUTION_BWD_FILTER_ALGO_3,\n  918        // clang-format on\n  919    });\n  ...\n  921  }\n  922  \n  923: bool CudnnSupport::DoConvolve(\n  924      Stream* stream, const BatchDescriptor& batch_descriptor,\n  925      const DeviceMemory<float>& input_data,\n  ...\n  932      dnn::ProfileResult* output_profile_result) {\n  933    return DoConvolveImpl<float>(\n  934:       stream, CUDNN_DATA_FLOAT, batch_descriptor, input_data, filter_descriptor,\n  935        filter_data, convolution_descriptor, output_descriptor, output_data,\n  936        scratch_allocator, algorithm_config, output_profile_result);\n  937  }\n  938  \n  939: bool CudnnSupport::DoConvolve(\n  940      Stream* stream, const BatchDescriptor& batch_descriptor,\n  941      const DeviceMemory<double>& input_data,\n  ...\n  949  }\n  950  \n  951: bool CudnnSupport::DoConvolve(\n  952      Stream* stream, const BatchDescriptor& batch_descriptor,\n  953      const DeviceMemory<Eigen::half>& input_data,\n  ...\n  960      dnn::ProfileResult* output_profile_result) {\n  961    return DoConvolveImpl<Eigen::half>(\n  962:       stream, CUDNN_DATA_HALF, batch_descriptor, input_data, filter_descriptor,\n  963        filter_data, convolution_descriptor, output_descriptor, output_data,\n  964        scratch_allocator, algorithm_config, output_profile_result);\n  ...\n  966  \n  967  template<class T>\n  968: DeviceMemory<T> CudnnSupport::MaybeTransformLayout(\n  969      Stream* stream,\n  970:     int cudnn_type,  // Actually cudnnDataType_t.\n  971      BatchDescriptor* output_descriptor,\n  972      DeviceMemory<T> backward_output_data,\n  ...\n  983    transformed_output_descriptor.set_layout(dnn::DataLayout::kBatchDepthYX);\n  984    ScopedTensorDescriptor orig_out_back_nd{\n  985:       parent_, *output_descriptor, static_cast<cudnnDataType_t>(cudnn_type)};\n  986    ScopedTensorDescriptor transformed_out_back_nd{\n  987        parent_, transformed_output_descriptor,\n  988:       static_cast<cudnnDataType_t>(cudnn_type)};\n  989  \n  990    float alpha = 1.0f;\n  991    float beta = 0.0f;\n  992:   auto status = dynload::cudnnTransformTensor(\n  993        parent_, ToHandle(dnn_handle_), &alpha, orig_out_back_nd.handle(),\n  994        backward_output_data.opaque(), &beta, transformed_out_back_nd.handle(),\n  995        (*transform_scratch)->mutable_device_memory()->opaque());\n  996  \n  997:   if (status != CUDNN_STATUS_SUCCESS) {\n  998      LOG(FATAL) << \"Failed to transform the data layout.\";\n  999    }\n ....\n 1003  \n 1004  template <class T>\n 1005: bool CudnnSupport::DoConvolveBackwardDataImpl(\n 1006      Stream* stream,\n 1007:     int cudnn_type,  // Actually cudnnDataType_t.\n 1008      const FilterDescriptor& filter_descriptor,\n 1009      const DeviceMemory<T>& filter_data,\n ....\n 1016      dnn::ProfileResult* output_profile_result) {\n 1017    mutex_lock lock{dnn_handle_mutex_};\n 1018:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1019                                          AsCUDAStreamValue(stream));\n 1020:   if (status != CUDNN_STATUS_SUCCESS) {\n 1021:     LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 1022    }\n 1023  \n ....\n 1027    float beta = 0.0;\n 1028  \n 1029:   // TBD(keveman): remove once cuDNN supports kBatchYXDepth for backward pass.\n 1030    BatchDescriptor output_descriptor;\n 1031    output_descriptor.CloneFrom(output_descriptor_in);\n 1032    std::unique_ptr<TemporaryDeviceMemory<T>> transform_scratch;\n 1033    backward_output_data = MaybeTransformLayout(\n 1034:       stream, cudnn_type, &output_descriptor, backward_output_data,\n 1035        &transform_scratch);\n 1036  \n 1037    ScopedTensorDescriptor out_back_nd{parent_, output_descriptor,\n 1038:                                      static_cast<cudnnDataType_t>(cudnn_type)};\n 1039    ScopedTensorDescriptor in_back_nd{parent_, input_descriptor,\n 1040:                                     static_cast<cudnnDataType_t>(cudnn_type)};\n 1041    ScopedFilterDescriptor filter{parent_, filter_descriptor, input_descriptor,\n 1042:                                 static_cast<cudnnDataType_t>(cudnn_type)};\n 1043:   // TODO(sesse): Figure out under what circumstances cuDNN would\n 1044:   // accept CUDNN_DATA_HALF here; probably related to compute capability\n 1045:   // and cuDNN version; at least cuDNN 4 on TITAN X only supports\n 1046:   // CUDNN_DATA_FLOAT even for half input.\n 1047    ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n 1048:                                    CUDNN_DATA_FLOAT};\n 1049  \n 1050: #if CUDNN_VERSION < 5000\n 1051: #if CUDNN_VERSION >= 3000\n 1052:   if (dynload::IsCudnnR2()) {\n 1053  #endif\n 1054: #if CUDNN_VERSION >= 4000\n 1055:     status = dynload::cudnnConvolutionBackwardData_v2(\n 1056  #else\n 1057:   status = dynload::cudnnConvolutionBackwardData(\n 1058  #endif\n 1059          parent_, ToHandle(dnn_handle_), &alpha, filter.handle(),\n ....\n 1061          backward_output_data.opaque(), conv.handle(), &beta,\n 1062          in_back_nd.handle(), backward_input_data->opaque());\n 1063:     if (status != CUDNN_STATUS_SUCCESS) {\n 1064        LOG(FATAL) << \"failed to enqueue convolution on stream: \"\n 1065                   << ToString(status);\n ....\n 1067      }\n 1068      return true;\n 1069: #if CUDNN_VERSION >= 3000\n 1070    }\n 1071  #endif\n 1072  #endif\n 1073  \n 1074: #if CUDNN_VERSION >= 3000\n 1075    const bool is_profiling = output_profile_result != nullptr;\n 1076:   cudnnConvolutionBwdDataAlgo_t algo;\n 1077    DeviceMemory<uint8> scratch;\n 1078  \n 1079    if (algorithm_config.algorithm() == dnn::kDefaultAlgorithm) {\n 1080:     // With the default algorithm, use Cudnn's heuristics.\n 1081      auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n 1082:         dnn_handle_mutex_) -> cudnnConvolutionBwdDataAlgo_t {\n 1083:       cudnnConvolutionBwdDataPreference_t preference =\n 1084:           specify_limit ? CUDNN_CONVOLUTION_BWD_DATA_SPECIFY_WORKSPACE_LIMIT\n 1085:                         : CUDNN_CONVOLUTION_BWD_DATA_NO_WORKSPACE;\n 1086  \n 1087        auto memory_limit_bytes =\n ....\n 1093        }\n 1094  \n 1095:       cudnnConvolutionBwdDataAlgo_t algo_to_use;\n 1096:       cudnnStatus_t status = dynload::cudnnGetConvolutionBackwardDataAlgorithm(\n 1097            parent_, ToHandle(dnn_handle_),\n 1098            /*filterDesc=*/filter.handle(),\n ....\n 1103            /*memoryLimitInBytes=*/memory_limit_bytes,\n 1104            /*algo=*/&algo_to_use);\n 1105:       CHECK_EQ(status, CUDNN_STATUS_SUCCESS) << \"Unable to find a suitable \"\n 1106                                                  \"algorithm for doing backward \"\n 1107                                                  \"filter convolution\";\n ....\n 1113      if (scratch_allocator != nullptr) {\n 1114        size_t size_in_bytes;\n 1115:       status = dynload::cudnnGetConvolutionBackwardDataWorkspaceSize(\n 1116            parent_, ToHandle(dnn_handle_),\n 1117            /*filterDesc=*/filter.handle(),\n ....\n 1121            /*algo=*/algo,\n 1122            /*sizeInBytes=*/&size_in_bytes);\n 1123:       if (status == CUDNN_STATUS_SUCCESS && size_in_bytes != 0) {\n 1124          auto allocated =\n 1125              scratch_allocator->AllocateBytes(stream, size_in_bytes);\n ....\n 1139      algo = ToConvBackwardDataAlgo(algorithm_config.algorithm());\n 1140      size_t size_in_bytes;\n 1141:     status = dynload::cudnnGetConvolutionBackwardDataWorkspaceSize(\n 1142          parent_, ToHandle(dnn_handle_),\n 1143          /*filterDesc=*/filter.handle(),\n ....\n 1147          /*algo=*/algo,\n 1148          /*sizeInBytes=*/&size_in_bytes);\n 1149:     if (status != CUDNN_STATUS_SUCCESS) {\n 1150        if (is_profiling) {\n 1151          // Silently return when we are profiling.\n ....\n 1182      timer.reset(new CUDATimer(parent_));\n 1183      timer->Init();\n 1184:     // The start and stop of the timer should be as close to the Cudnn call as\n 1185      // possible. It is still possible for other threads to issue workload on\n 1186      // to this stream. So it could take multiple profiling measurements.\n ....\n 1188    }\n 1189  \n 1190: #if CUDNN_VERSION >= 5000\n 1191:   status = dynload::cudnnConvolutionBackwardData(\n 1192  #else\n 1193:   status = dynload::cudnnConvolutionBackwardData_v3(\n 1194  #endif\n 1195        parent_, ToHandle(dnn_handle_),\n ....\n 1214      timer->Destroy();\n 1215    }\n 1216:   if (status != CUDNN_STATUS_SUCCESS) {\n 1217      // Silently return when we are profiling.\n 1218      if (!is_profiling) {\n ....\n 1226  }\n 1227  \n 1228: bool CudnnSupport::DoConvolveBackwardData(\n 1229      Stream* stream, const FilterDescriptor& filter_descriptor,\n 1230      const DeviceMemory<float>& filter_data,\n ....\n 1238      dnn::ProfileResult* output_profile_result) {\n 1239    return DoConvolveBackwardDataImpl(\n 1240:       stream, CUDNN_DATA_FLOAT, filter_descriptor, filter_data,\n 1241        output_descriptor_in, backward_output_data, convolution_descriptor,\n 1242        input_descriptor, backward_input_data, scratch_allocator,\n ....\n 1244  }\n 1245  \n 1246: bool CudnnSupport::DoConvolveBackwardData(\n 1247      Stream* stream, const FilterDescriptor& filter_descriptor,\n 1248      const DeviceMemory<Eigen::half>& filter_data,\n ....\n 1256      dnn::ProfileResult* output_profile_result) {\n 1257    return DoConvolveBackwardDataImpl(\n 1258:       stream, CUDNN_DATA_HALF, filter_descriptor, filter_data,\n 1259        output_descriptor_in, backward_output_data, convolution_descriptor,\n 1260        input_descriptor, backward_input_data, scratch_allocator,\n ....\n 1263  \n 1264  template <class T>\n 1265: bool CudnnSupport::DoConvolveBackwardFilterImpl(\n 1266:     Stream* stream, int cudnn_type,  // Actually cudnnDataType_t.\n 1267      const dnn::BatchDescriptor& input_descriptor,\n 1268      const DeviceMemory<T>& input_data,\n ....\n 1275      dnn::ProfileResult* output_profile_result) {\n 1276    mutex_lock lock{dnn_handle_mutex_};\n 1277:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1278                                          AsCUDAStreamValue(stream));\n 1279:   if (status != CUDNN_STATUS_SUCCESS) {\n 1280:     LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 1281    }\n 1282  \n ....\n 1286    float beta = 0.0;\n 1287  \n 1288:   // TBD(keveman): remove once cuDNN supports kBatchYXDepth for backward pass.\n 1289    BatchDescriptor output_descriptor;\n 1290    output_descriptor.CloneFrom(output_descriptor_in);\n 1291    std::unique_ptr<TemporaryDeviceMemory<T>> transform_scratch;\n 1292    backward_output_data = MaybeTransformLayout(\n 1293:       stream, static_cast<cudnnDataType_t>(cudnn_type),\n 1294        &output_descriptor, backward_output_data,\n 1295        &transform_scratch);\n 1296  \n 1297    ScopedTensorDescriptor out_back_nd{parent_, output_descriptor,\n 1298:         static_cast<cudnnDataType_t>(cudnn_type)};\n 1299    ScopedTensorDescriptor input_nd{parent_, input_descriptor,\n 1300:           static_cast<cudnnDataType_t>(cudnn_type)};\n 1301    ScopedFilterDescriptor filter{parent_, filter_descriptor, input_descriptor,\n 1302:         static_cast<cudnnDataType_t>(cudnn_type)};\n 1303:   // TODO(sesse): Figure out under what circumstances cuDNN would\n 1304:   // accept CUDNN_DATA_HALF here; probably related to compute capability\n 1305:   // and cuDNN version; at least cuDNN 4 on TITAN X only supports\n 1306:   // CUDNN_DATA_FLOAT even for half input.\n 1307    ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n 1308:       CUDNN_DATA_FLOAT};\n 1309  \n 1310: #if CUDNN_VERSION < 5000\n 1311: #if CUDNN_VERSION >= 3000\n 1312:   if (dynload::IsCudnnR2()) {\n 1313  #endif\n 1314: #if CUDNN_VERSION >= 4000\n 1315:     status = dynload::cudnnConvolutionBackwardFilter_v2(\n 1316  #else\n 1317:   status = dynload::cudnnConvolutionBackwardFilter(\n 1318  #endif\n 1319          parent_, ToHandle(dnn_handle_), &alpha, input_nd.handle(),\n ....\n 1321          backward_output_data.opaque(), conv.handle(), &beta, filter.handle(),\n 1322          backward_filter_data->opaque());\n 1323:     if (status != CUDNN_STATUS_SUCCESS) {\n 1324        LOG(FATAL) << \"failed to enqueue convolution on stream: \"\n 1325                   << ToString(status);\n ....\n 1327      }\n 1328      return true;\n 1329: #if CUDNN_VERSION >= 3000\n 1330    }\n 1331  #endif\n 1332  #endif\n 1333  \n 1334: #if CUDNN_VERSION >= 3000\n 1335    const bool is_profiling = output_profile_result != nullptr;\n 1336:   cudnnConvolutionBwdFilterAlgo_t algo;\n 1337    DeviceMemory<uint8> scratch;\n 1338  \n 1339    if (algorithm_config.algorithm() == dnn::kDefaultAlgorithm) {\n 1340:     // With the default algorithm, use Cudnn's heuristics.\n 1341  \n 1342      // Lambda that retrieves the algorithm.\n ....\n 1345      auto get_algorithm = [&](bool specify_limit) SHARED_LOCKS_REQUIRED(\n 1346          dnn_handle_mutex_) {\n 1347:       cudnnConvolutionBwdFilterPreference_t preference =\n 1348:           specify_limit ? CUDNN_CONVOLUTION_BWD_FILTER_SPECIFY_WORKSPACE_LIMIT\n 1349:                         : CUDNN_CONVOLUTION_BWD_FILTER_NO_WORKSPACE;\n 1350  \n 1351        auto memory_limit_bytes =\n ....\n 1357        }\n 1358  \n 1359:       cudnnConvolutionBwdFilterAlgo_t algo_to_use;\n 1360:       cudnnStatus_t status =\n 1361:           dynload::cudnnGetConvolutionBackwardFilterAlgorithm(\n 1362                parent_, ToHandle(dnn_handle_),\n 1363                /*srcDesc=*/input_nd.handle(),\n ....\n 1368                /*memoryLimitInBytes=*/memory_limit_bytes,\n 1369                /*algo=*/&algo_to_use);\n 1370:       CHECK_EQ(status, CUDNN_STATUS_SUCCESS) << \"Unable to find a suitable \"\n 1371                                                  \"algorithm for doing backward \"\n 1372                                                  \"filter convolution\";\n ....\n 1378      if (scratch_allocator != nullptr) {\n 1379        size_t size_in_bytes;\n 1380:       status = dynload::cudnnGetConvolutionBackwardFilterWorkspaceSize(\n 1381            parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 1382            /*diffDesc=*/out_back_nd.handle(), /*convDesc=*/conv.handle(),\n 1383            /*gradDesc=*/filter.handle(), /*algo=*/algo,\n 1384            /*sizeInBytes=*/&size_in_bytes);\n 1385:       if (status == CUDNN_STATUS_SUCCESS && size_in_bytes != 0) {\n 1386          auto allocated =\n 1387              scratch_allocator->AllocateBytes(stream, size_in_bytes);\n ....\n 1402  \n 1403      size_t size_in_bytes;\n 1404:     status = dynload::cudnnGetConvolutionBackwardFilterWorkspaceSize(\n 1405          parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 1406          /*diffDesc=*/out_back_nd.handle(), /*convDesc=*/conv.handle(),\n 1407          /*gradDesc=*/filter.handle(), /*algo=*/algo,\n 1408          /*sizeInBytes=*/&size_in_bytes);\n 1409:     if (status != CUDNN_STATUS_SUCCESS) {\n 1410        if (is_profiling) {\n 1411          // Silently return when we are profiling.\n ....\n 1443      timer.reset(new CUDATimer(parent_));\n 1444      timer->Init();\n 1445:     // The start and stop of the timer should be as close to the Cudnn call as\n 1446      // possible. It is still possible for other threads to issue workload on\n 1447      // to this stream. So it could take multiple profiling measurements.\n ....\n 1449    }\n 1450  \n 1451: #if CUDNN_VERSION >= 5000\n 1452:   status = dynload::cudnnConvolutionBackwardFilter(\n 1453  #else\n 1454:   status = dynload::cudnnConvolutionBackwardFilter_v3(\n 1455  #endif\n 1456        parent_, ToHandle(dnn_handle_), /*alpha=*/&alpha,\n ....\n 1474      timer->Destroy();\n 1475    }\n 1476:   if (status != CUDNN_STATUS_SUCCESS) {\n 1477      // Silently return when we are profiling.\n 1478      if (!is_profiling) {\n ....\n 1486  }\n 1487  \n 1488: bool CudnnSupport::DoConvolveBackwardFilter(\n 1489      Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n 1490      const DeviceMemory<float>& input_data,\n ....\n 1498      dnn::ProfileResult* output_profile_result) {\n 1499    return DoConvolveBackwardFilterImpl(\n 1500:       stream, CUDNN_DATA_FLOAT, input_descriptor, input_data,\n 1501        output_descriptor_in, backward_output_data, convolution_descriptor,\n 1502        filter_descriptor, backward_filter_data, scratch_allocator,\n ....\n 1504  }\n 1505  \n 1506: bool CudnnSupport::DoConvolveBackwardFilter(\n 1507      Stream* stream, const dnn::BatchDescriptor& input_descriptor,\n 1508      const DeviceMemory<Eigen::half>& input_data,\n ....\n 1516      dnn::ProfileResult* output_profile_result) {\n 1517    return DoConvolveBackwardFilterImpl(\n 1518:       stream, CUDNN_DATA_HALF, input_descriptor, input_data,\n 1519        output_descriptor_in, backward_output_data, convolution_descriptor,\n 1520        filter_descriptor, backward_filter_data, scratch_allocator,\n ....\n 1523  \n 1524  template <class T>\n 1525: bool CudnnSupport::DoConvolveBackwardBiasImpl(\n 1526:     Stream* stream, int cudnn_type,  // Actually cudnnDataType_t.\n 1527      const dnn::BatchDescriptor& input_descriptor,\n 1528      const DeviceMemory<T>& input_data,\n ....\n 1530      DeviceMemory<T>* backward_bias_data) {\n 1531    mutex_lock lock{dnn_handle_mutex_};\n 1532:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1533                                          AsCUDAStreamValue(stream));\n 1534:   if (status != CUDNN_STATUS_SUCCESS) {\n 1535:     LOG(FATAL) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 1536    }\n 1537  \n 1538    ScopedTensorDescriptor input_nd{parent_, input_descriptor,\n 1539:                                   static_cast<cudnnDataType_t>(cudnn_type)};\n 1540    ScopedTensorDescriptor bias_nd{parent_, bias_descriptor,\n 1541:                                  static_cast<cudnnDataType_t>(cudnn_type)};\n 1542  \n 1543    // Alpha is the scaling factor for input.\n ....\n 1546    float beta = 0.0;\n 1547  \n 1548:   status = dynload::cudnnConvolutionBackwardBias(\n 1549        parent_, ToHandle(dnn_handle_), &alpha, input_nd.handle(),\n 1550        input_data.opaque(), &beta, bias_nd.handle(),\n 1551        backward_bias_data->opaque());\n 1552:   if (status != CUDNN_STATUS_SUCCESS) {\n 1553      LOG(FATAL) << \"failed to enqueue backward convolution on stream: \"\n 1554                 << ToString(status);\n ....\n 1558  }\n 1559  \n 1560: bool CudnnSupport::DoConvolveBackwardBias(\n 1561      Stream* stream, const BatchDescriptor& input_descriptor,\n 1562      const DeviceMemory<double>& input_data,\n 1563      const BatchDescriptor& bias_descriptor,\n 1564      DeviceMemory<double>* backward_bias_data) {\n 1565:   return DoConvolveBackwardBiasImpl(stream, CUDNN_DATA_DOUBLE, input_descriptor,\n 1566                                      input_data, bias_descriptor,\n 1567                                      backward_bias_data);\n 1568  }\n 1569  \n 1570: bool CudnnSupport::DoConvolveBackwardBias(\n 1571      Stream* stream, const BatchDescriptor& input_descriptor,\n 1572      const DeviceMemory<float>& input_data,\n 1573      const BatchDescriptor& bias_descriptor,\n 1574      DeviceMemory<float>* backward_bias_data) {\n 1575:   return DoConvolveBackwardBiasImpl(stream, CUDNN_DATA_FLOAT, input_descriptor,\n 1576                                      input_data, bias_descriptor,\n 1577                                      backward_bias_data);\n 1578  }\n 1579  \n 1580: bool CudnnSupport::DoConvolveBackwardBias(\n 1581      Stream* stream, const BatchDescriptor& input_descriptor,\n 1582      const DeviceMemory<Eigen::half>& input_data,\n 1583      const BatchDescriptor& bias_descriptor,\n 1584      DeviceMemory<Eigen::half>* backward_bias_data) {\n 1585:   return DoConvolveBackwardBiasImpl(stream, CUDNN_DATA_HALF, input_descriptor,\n 1586                                      input_data, bias_descriptor,\n 1587                                      backward_bias_data);\n 1588  }\n 1589  \n 1590: bool CudnnSupport::DoMatMul(Stream* stream,\n 1591                              const DeviceMemory<float>& input_data,\n 1592                              const DeviceMemory<float>& weights,\n ....\n 1722  }\n 1723  \n 1724: bool CudnnSupport::DoBiasAdd(Stream* stream,\n 1725                               const DeviceMemory<float>& input_data,\n 1726                               const DeviceMemory<float>& biases,\n ....\n 1728                               DeviceMemory<float>* output_data) {\n 1729    ScopedTensorDescriptor input_descriptor{parent_, dimensions,\n 1730:                                           CUDNN_DATA_FLOAT};\n 1731  \n 1732    BatchDescriptor bias_dimensions;\n ....\n 1737        .set_layout(dnn::DataLayout::kBatchYXDepth);\n 1738    ScopedTensorDescriptor bias_descriptor{parent_, bias_dimensions,\n 1739:                                          CUDNN_DATA_FLOAT};\n 1740  \n 1741:   // cudnnAddTensor after R3 is in-place, so we need to copy input_data to\n 1742    // output_data before doing the addition, unless the input and\n 1743    // output are at the same address.\n ....\n 1754  \n 1755    mutex_lock lock{dnn_handle_mutex_};\n 1756:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1757                                          AsCUDAStreamValue(stream));\n 1758:   if (status != CUDNN_STATUS_SUCCESS) {\n 1759:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 1760      return false;\n 1761    }\n ....\n 1763    const float alpha = 1.0f;\n 1764    const float beta = 1.0f;\n 1765: #if CUDNN_VERSION >= 3000\n 1766:   if (dynload::IsCudnnR2()) {\n 1767  #endif\n 1768  \n 1769: #if CUDNN_VERSION < 5000\n 1770: #if CUDNN_VERSION >= 4000\n 1771:     status = dynload::cudnnAddTensor_v2(\n 1772  #else\n 1773:     status = dynload::cudnnAddTensor(\n 1774  #endif\n 1775:         parent_, ToHandle(dnn_handle_), CUDNN_ADD_SAME_C, &alpha,\n 1776          bias_descriptor.handle(), biases.opaque(), &beta,\n 1777          input_descriptor.handle(), output_data->opaque());\n 1778: #endif  // CUDNN_VERSION < 5000\n 1779  \n 1780: #if CUDNN_VERSION >= 3000\n 1781    } else {\n 1782: #if CUDNN_VERSION >= 5000\n 1783:     status = dynload::cudnnAddTensor(\n 1784  #else\n 1785:     status = dynload::cudnnAddTensor_v3(\n 1786  #endif\n 1787          parent_, ToHandle(dnn_handle_), &alpha, bias_descriptor.handle(),\n ....\n 1791  #endif\n 1792  \n 1793:   if (status != CUDNN_STATUS_SUCCESS) {\n 1794      LOG(ERROR) << \"stream \" << stream << \" could not enqueue bias addition.\";\n 1795      return false;\n ....\n 1799  }\n 1800  \n 1801: bool CudnnSupport::DoActivate(Stream* stream,\n 1802                                dnn::ActivationMode activation_mode,\n 1803                                const dnn::BatchDescriptor& dimensions,\n ....\n 1805                                DeviceMemory<float>* output_data) {\n 1806    mutex_lock lock{dnn_handle_mutex_};\n 1807:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1808                                          AsCUDAStreamValue(stream));\n 1809:   if (status != CUDNN_STATUS_SUCCESS) {\n 1810:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 1811      return false;\n 1812    }\n 1813  \n 1814: #if CUDNN_VERSION >= 5000\n 1815    ScopedActivationDescriptor activation_desc{parent_, activation_mode,\n 1816                                               dimensions.value_max()};\n 1817  #else\n 1818:   cudnnActivationMode_t mode;\n 1819    switch (activation_mode) {\n 1820      case dnn::ActivationMode::kRelu6:\n 1821        // TODO(leary) should probably do a post-pass to clip at 6?\n 1822        LOG(WARNING) << \"user requested Relu6, but providing Relu instead\";\n 1823:       mode = CUDNN_ACTIVATION_RELU;\n 1824        break;\n 1825      case dnn::ActivationMode::kReluX:\n 1826        // TODO(broune) should probably do a post-pass to clip at X?\n 1827        LOG(WARNING) << \"user requested ReluX, but providing Relu instead\";\n 1828:       mode = CUDNN_ACTIVATION_RELU;\n 1829        break;\n 1830      case dnn::ActivationMode::kRelu:\n 1831:       mode = CUDNN_ACTIVATION_RELU;\n 1832        break;\n 1833      case dnn::ActivationMode::kSigmoid:\n 1834:       mode = CUDNN_ACTIVATION_SIGMOID;\n 1835        break;\n 1836      case dnn::ActivationMode::kTanh:\n 1837:       mode = CUDNN_ACTIVATION_TANH;\n 1838        break;\n 1839      default:\n ....\n 1844  #endif\n 1845  \n 1846:   ScopedTensorDescriptor input_nd{parent_, dimensions, CUDNN_DATA_FLOAT};\n 1847    // Alpha is the input scaling factor.\n 1848    float alpha = 1.0;\n 1849    // Beta is the output scaling factor.\n 1850    float beta = 0.0;\n 1851:   status = dynload::cudnnActivationForward(\n 1852        parent_, ToHandle(dnn_handle_),\n 1853: #if CUDNN_VERSION >= 5000\n 1854        activation_desc.handle(),\n 1855  #else\n ....\n 1858        &alpha, input_nd.handle(), input_data.opaque(), &beta, input_nd.handle(),\n 1859        output_data->opaque());\n 1860:   if (status != CUDNN_STATUS_SUCCESS) {\n 1861      LOG(ERROR) << \"stream \" << stream\n 1862                 << \" could not enqueue activation: \" << ToString(status);\n ....\n 1867  }\n 1868  \n 1869: bool CudnnSupport::DoPoolForward(\n 1870      Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n 1871      const dnn::BatchDescriptor& input_dimensions,\n ....\n 1874      DeviceMemory<float>* output_data) {\n 1875    mutex_lock lock{dnn_handle_mutex_};\n 1876:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1877                                          AsCUDAStreamValue(stream));\n 1878:   if (status != CUDNN_STATUS_SUCCESS) {\n 1879:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 1880      return false;\n 1881    }\n ....\n 1886    float beta = 0.0;\n 1887  \n 1888:   ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_FLOAT};\n 1889    ScopedTensorDescriptor dest_desc{parent_, output_dimensions,\n 1890:                                    CUDNN_DATA_FLOAT};\n 1891    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 1892:   status = dynload::cudnnPoolingForward(\n 1893        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 1894        src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(),\n 1895        output_data->opaque());\n 1896:   if (status != CUDNN_STATUS_SUCCESS) {\n 1897      LOG(ERROR) << \"failed to enqueue forward pooling on stream: \"\n 1898                 << ToString(status);\n ....\n 1902  }\n 1903  \n 1904: bool CudnnSupport::DoPoolForward(\n 1905      Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n 1906      const dnn::BatchDescriptor& input_dimensions,\n ....\n 1909      DeviceMemory<Eigen::half>* output_data) {\n 1910    mutex_lock lock{dnn_handle_mutex_};\n 1911:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1912                                          AsCUDAStreamValue(stream));\n 1913:   if (status != CUDNN_STATUS_SUCCESS) {\n 1914:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 1915      return false;\n 1916    }\n ....\n 1921    float beta = 0.0;\n 1922  \n 1923:   ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_HALF};\n 1924:   ScopedTensorDescriptor dest_desc{parent_, output_dimensions, CUDNN_DATA_HALF};\n 1925    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 1926:   status = dynload::cudnnPoolingForward(\n 1927        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 1928        src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(),\n 1929        output_data->opaque());\n 1930:   if (status != CUDNN_STATUS_SUCCESS) {\n 1931      LOG(ERROR) << \"failed to enqueue forward pooling on stream: \"\n 1932                 << ToString(status);\n ....\n 1936  }\n 1937  \n 1938: bool CudnnSupport::DoPoolBackward(\n 1939      Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n 1940      const dnn::BatchDescriptor& input_dimensions,\n ....\n 1945      DeviceMemory<float>* output_diff_data) {\n 1946    mutex_lock lock{dnn_handle_mutex_};\n 1947:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1948                                          AsCUDAStreamValue(stream));\n 1949:   if (status != CUDNN_STATUS_SUCCESS) {\n 1950:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 1951      return false;\n 1952    }\n ....\n 1957    float beta = 0.0;\n 1958  \n 1959:   ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_FLOAT};\n 1960    ScopedTensorDescriptor dest_desc{parent_, output_dimensions,\n 1961:                                    CUDNN_DATA_FLOAT};\n 1962    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 1963:   status = dynload::cudnnPoolingBackward(\n 1964        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 1965        dest_desc.handle(), output_data.opaque(), dest_desc.handle(),\n 1966        input_diff_data.opaque(), src_desc.handle(), input_data.opaque(), &beta,\n 1967        src_desc.handle(), output_diff_data->opaque());\n 1968:   if (status != CUDNN_STATUS_SUCCESS) {\n 1969      LOG(ERROR) << \"failed to enqueue backward pooling on stream: \"\n 1970                 << ToString(status);\n ....\n 1974  }\n 1975  \n 1976: bool CudnnSupport::DoPoolBackward(\n 1977      Stream* stream, const dnn::PoolingDescriptor& pooling_dimensions,\n 1978      const dnn::BatchDescriptor& input_dimensions,\n ....\n 1983      DeviceMemory<Eigen::half>* output_diff_data) {\n 1984    mutex_lock lock{dnn_handle_mutex_};\n 1985:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1986                                          AsCUDAStreamValue(stream));\n 1987:   if (status != CUDNN_STATUS_SUCCESS) {\n 1988:     LOG(ERROR) << \"failed to set stream for cudnn handle: \" << ToString(status);\n 1989      return false;\n 1990    }\n ....\n 1995    float beta = 0.0;\n 1996  \n 1997:   ScopedTensorDescriptor src_desc{parent_, input_dimensions, CUDNN_DATA_HALF};\n 1998:   ScopedTensorDescriptor dest_desc{parent_, output_dimensions, CUDNN_DATA_HALF};\n 1999    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 2000:   status = dynload::cudnnPoolingBackward(\n 2001        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 2002        dest_desc.handle(), output_data.opaque(), dest_desc.handle(),\n 2003        input_diff_data.opaque(), src_desc.handle(), input_data.opaque(), &beta,\n 2004        src_desc.handle(), output_diff_data->opaque());\n 2005:   if (status != CUDNN_STATUS_SUCCESS) {\n 2006      LOG(ERROR) << \"failed to enqueue backward pooling on stream: \"\n 2007                 << ToString(status);\n ....\n 2011  }\n 2012  \n 2013: bool CudnnSupport::DoNormalize(\n 2014      Stream* stream, const dnn::NormalizeDescriptor& normalize_descriptor,\n 2015      const DeviceMemory<float>& input_data, DeviceMemory<float>* output_data) {\n ....\n 2018  }\n 2019  \n 2020: bool CudnnSupport::DoDepthConcatenate(\n 2021      Stream* stream, port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n 2022      port::ArraySlice<const DeviceMemory<float>*> input_data,\n ....\n 2026    for (const auto& dimensions : input_dimensions) {\n 2027      if (dimensions.layout() != dnn::DataLayout::kBatchDepthYX) {\n 2028:       LOG(ERROR) << \"CudnnSupport::DoDepthConcatenate currently only \"\n 2029                      \"supports the kBatchDepthYX layout.\";\n 2030        return false;\n ....\n 2070  }\n 2071  \n 2072: bool CudnnSupport::DoElementwiseOperate(\n 2073      Stream* stream, dnn::ElementwiseOperation operation,\n 2074      port::ArraySlice<dnn::BatchDescriptor> input_dimensions,\n ....\n 2080  }\n 2081  \n 2082: bool CudnnSupport::DoXYPad(Stream* stream,\n 2083                             const dnn::BatchDescriptor& dimensions,\n 2084                             const DeviceMemory<float>& input_data,\n ....\n 2089  }\n 2090  \n 2091: bool CudnnSupport::DoXYSlice(Stream* stream,\n 2092                               const dnn::BatchDescriptor& dimensions,\n 2093                               const DeviceMemory<float>& input_data,\n ....\n 2099  }\n 2100  \n 2101: bool CudnnSupport::DoMemcpyD2HQuantized(\n 2102      Stream* stream, const DeviceMemory<float>& gpu_unquantized_src,\n 2103      dnn::QuantizedActivationMode mode, void* host_dst, int64 size) {\n 2104:   LOG(ERROR) << \"quantized memcpy not supported by cuDNN\";\n 2105    return false;\n 2106  }\n 2107  \n 2108: bool CudnnSupport::DoMemcpyH2DQuantized(\n 2109      Stream* stream, const void* host_src, int64 size,\n 2110      dnn::QuantizedActivationMode mode,\n 2111      DeviceMemory<float>* gpu_unquantized_dst) {\n 2112:   LOG(ERROR) << \"quantized memcpy not supported by cuDNN\";\n 2113    return false;\n 2114  }\n 2115  \n 2116: bool CudnnSupport::DeriveOutputBatchDescriptor(\n 2117      const BatchDescriptor& batch_descriptor,\n 2118      const FilterDescriptor& filter_descriptor,\n 2119      const dnn::ConvolutionDescriptor& convolution_descriptor,\n 2120      dnn::BatchDescriptor* output_batch_descriptor) {\n 2121:   ScopedTensorDescriptor input_nd{parent_, batch_descriptor, CUDNN_DATA_FLOAT};\n 2122    ScopedFilterDescriptor filter{parent_, filter_descriptor, batch_descriptor,\n 2123:                                 CUDNN_DATA_FLOAT};\n 2124    ScopedConvolutionDescriptor conv{parent_, convolution_descriptor,\n 2125:                                    CUDNN_DATA_FLOAT};\n 2126  \n 2127    int dn = batch_descriptor.ndims() + 2;\n 2128    std::vector<int> dims(dn);  // in BDYX\n 2129:   auto status = dynload::cudnnGetConvolutionNdForwardOutputDim(\n 2130        parent_, conv.handle(), input_nd.handle(), filter.handle(), dn,\n 2131        dims.data());\n 2132:   if (status != CUDNN_STATUS_SUCCESS) {\n 2133      LOG(ERROR) << \"could not get output tensor for convolution: \"\n 2134                 << ToString(status);\n ....\n 2152  namespace gpu = ::perftools::gputools;\n 2153  \n 2154: void initialize_cudnn() {\n 2155    gpu::port::Status status =\n 2156        gpu::PluginRegistry::Instance()\n 2157            ->RegisterFactory<gpu::PluginRegistry::DnnFactory>(\n 2158:               gpu::cuda::kCudaPlatformId, gpu::cuda::kCuDnnPlugin, \"cuDNN\",\n 2159                [](gpu::internal::StreamExecutorInterface*\n 2160                       parent) -> gpu::dnn::DnnSupport* {\n ....\n 2168                  }\n 2169  \n 2170:                 gpu::cuda::CudnnSupport* dnn =\n 2171:                     new gpu::cuda::CudnnSupport(cuda_executor);\n 2172                  if (!dnn->Init().ok()) {\n 2173                    // Note: Init() will log a more specific error.\n ....\n 2179  \n 2180    if (!status.ok()) {\n 2181:     LOG(ERROR) << \"Unable to register cuDNN factory: \"\n 2182                 << status.error_message();\n 2183    }\n 2184  \n 2185:   // Prime the cuDNN DSO. The loader will log more information.\n 2186:   auto statusor = gpu::internal::CachedDsoLoader::GetCudnnDsoHandle();\n 2187    if (!statusor.ok()) {\n 2188:     LOG(INFO) << \"Unable to load cuDNN DSO\";\n 2189    }\n 2190  \n 2191    gpu::PluginRegistry::Instance()->SetDefaultFactory(gpu::cuda::kCudaPlatformId,\n 2192                                                       gpu::PluginKind::kDnn,\n 2193:                                                      gpu::cuda::kCuDnnPlugin);\n 2194  }\n 2195  \n ....\n 2197  }  // namespace perftools\n 2198  \n 2199: REGISTER_MODULE_INITIALIZER(register_cudnn,\n 2200:                             { perftools::gputools::initialize_cudnn(); });\n 2201  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.h:\n   33  class CUDAExecutor;\n   34  \n   35: // Opaque and unique identifier for the cuDNN plugin.\n   36: extern const PluginId kCuDnnPlugin;\n   37  \n   38: // cudnn-library based DNN support. For details on overridden interface\n   39  // functions, see dnn.h.\n   40: class CudnnSupport : public dnn::DnnSupport {\n   41   public:\n   42:   explicit CudnnSupport(CUDAExecutor* parent);\n   43:   ~CudnnSupport() override;\n   44  \n   45    port::Status Init() override;\n   ..\n   93        const dnn::BatchDescriptor& output_descriptor,\n   94        DeviceMemory<float>* output_data) override {\n   95:     LOG(ERROR) << \"separable convolution not supported by CUDNN\";\n   96      return false;\n   97    }\n   ..\n  175                           const dnn::BatchDescriptor& output_dimensions,\n  176                           DeviceMemory<float>* output_data) override {\n  177:     LOG(ERROR) << \"DNN MatMulQuantized not supported by CUDNN\";\n  178      return false;\n  179    }\n  ...\n  185                           const dnn::BatchDescriptor& output_dimensions,\n  186                           DeviceMemory<float>* output_data) override {\n  187:     LOG(ERROR) << \"DNN MatMulQuantized not supported by CUDNN\";\n  188      return false;\n  189    }\n  ...\n  282    CUDAExecutor* parent_;  // Parent executor object. Not owned.\n  283  \n  284:   // cudnn library handle. cudnnHandle_t type is not present in this header to\n  285    // prevent third-party library header inclusions from leaking outside the\n  286    // single cuda_dnn translation unit.\n  287    void* dnn_handle_ GUARDED_BY(dnn_handle_mutex_);\n  288  \n  289:   // NOTE(keveman): Temporary data layout transformation until cuDNN supports\n  290    // kBatchYXDepth for backward pass. This function allocates temporary memory,\n  291    // lays out the source data into the temporary but in the kBatchDepthXY\n  ...\n  300    DeviceMemory<T> MaybeTransformLayout(\n  301        Stream* stream,\n  302:       int cudnn_type,  // Actually cudnnDataType_t.\n  303        dnn::BatchDescriptor* output_descriptor,\n  304        DeviceMemory<T> backward_output_data,\n  ...\n  308    template <class T>\n  309    bool DoConvolveImpl(Stream* stream,\n  310:                       int cudnn_type,  // Actually cudnnDataType_t.\n  311                        const dnn::BatchDescriptor& batch_descriptor,\n  312                        const DeviceMemory<T>& input_data,\n  ...\n  323    bool DoConvolveBackwardDataImpl(\n  324        Stream* stream,\n  325:       int cudnn_type,  // Actually cudnnDataType_t.\n  326        const dnn::FilterDescriptor& filter_descriptor,\n  327        const DeviceMemory<T>& filter_data,\n  ...\n  336    template <class T>\n  337    bool DoConvolveBackwardFilterImpl(\n  338:       Stream* stream, int cudnn_type,  // Actually cudnnDataType_t.\n  339        const dnn::BatchDescriptor& input_descriptor,\n  340        const DeviceMemory<T>& input_data,\n  ...\n  350    template <class T>\n  351    bool DoConvolveBackwardBiasImpl(Stream* stream,\n  352:                                   int cudnn_type,  // Actually cudnnDataType_t.\n  353                                    const dnn::BatchDescriptor& input_descriptor,\n  354                                    const DeviceMemory<T>& input_data,\n  ...\n  356                                    DeviceMemory<T>* backward_bias_data);\n  357  \n  358:   SE_DISALLOW_COPY_AND_ASSIGN(CudnnSupport);\n  359  };\n  360  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\stream_executor\\dnn.h:\n   17  //\n   18  // This is an abstract interface for a platform to optionally support common\n   19: // neural net operations; it accommodates implementations such as the cudnn\n   20  // library operations.\n   21  \n   ..\n   47    kYXBatchDepth,      // Same as dist_belief::DF_BATCH_MAJOR.\n   48    kBatchYXDepth,      // Same as run_brain output, and tensorflow's layout.\n   49:   kBatchDepthYX,      // cuDNN's NCHW layout, data laid out as image, feature,\n   50                        // maps, rows, columns.\n   51  };\n   ..\n  236  // Specify int64 so there's no padding in FilterDescriptor.\n  237  enum class FilterLayout : int64 {\n  238:   kOutputInputYX = 0,  // cuDNN's default filter layout, laid out as:\n  239                         // (major) output feature maps >> input feature maps >>\n  240                         // rows >> columns (minor).\n  ...\n  426    std::vector<int64> filter_strides_;\n  427    int ndims_;\n  428:   // TODO(leary) cudnn provides these fields, but need to characterize what\n  429    // their effect is -- they may be boolean rather than integral.\n  430    // int64 upscale_input_x;\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\stream_executor\\dso_loader.cc:\n   42  // any modifications of the format, please make sure the script still works.\n   43  string GetCudaVersion() { return \"\"; }\n   44: string GetCudnnVersion() { return \"\"; }\n   45  \n   46  /* static */ port::Status DsoLoader::GetCublasDsoHandle(void** dso_handle) {\n   ..\n   50  }\n   51  \n   52: /* static */ port::Status DsoLoader::GetCudnnDsoHandle(void** dso_handle) {\n   53:   // libcudnn is versioned differently than the other libraries and may have a\n   54    // different version number than other CUDA libraries.  See b/22397368 for\n   55    // some details about the complications surrounding this.\n   56    return GetDsoHandle(\n   57:       FindDsoPath(tensorflow::internal::FormatLibraryFileName(\"cudnn\", GetCudnnVersion()),\n   58                                GetCudaLibraryDirPath()),\n   59                        dso_handle);\n   ..\n  229  }\n  230  \n  231: /* static */ port::StatusOr<void*> CachedDsoLoader::GetCudnnDsoHandle() {\n  232    static port::StatusOr<void*> result =\n  233:       FetchHandleResult(DsoLoader::GetCudnnDsoHandle);\n  234    return result;\n  235  }\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\stream_executor\\dso_loader.h:\n   44  \n   45    static port::Status GetCublasDsoHandle(void** dso_handle);\n   46:   static port::Status GetCudnnDsoHandle(void** dso_handle);\n   47    static port::Status GetCufftDsoHandle(void** dso_handle);\n   48    static port::Status GetCurandDsoHandle(void** dso_handle);\n   ..\n  106    // Cached versions of the corresponding DsoLoader methods above.\n  107    static port::StatusOr<void*> GetCublasDsoHandle();\n  108:   static port::StatusOr<void*> GetCudnnDsoHandle();\n  109    static port::StatusOr<void*> GetCufftDsoHandle();\n  110    static port::StatusOr<void*> GetCurandDsoHandle();\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\stream_executor\\multi_platform_manager.h:\n   58  //\n   59  //    //perftools/gputools/executor/cuda:pluton_blas_plugin\n   60: //    //perftools/gputools/executor/cuda:cudnn_plugin\n   61  //    //perftools/gputools/executor/cuda:cublas_plugin\n   62  //    //perftools/gputools/executor/cuda:curand_plugin\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\tools\\ci_build\\Dockerfile.gpu:\n    1: FROM nvidia/cuda:7.5-cudnn4-devel\n    2  \n    3  MAINTAINER Jan Prach <jendap@google.com>\n    .\n   23  # Configure the build for our CUDA configuration.\n   24  ENV CUDA_TOOLKIT_PATH /usr/local/cuda\n   25: ENV CUDNN_INSTALL_PATH /usr/local/cuda\n   26  ENV TF_NEED_CUDA 1\n   27  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\tools\\docker\\Dockerfile.devel-gpu:\n    1: FROM nvidia/cuda:7.5-cudnn4-devel\n    2  \n    3  MAINTAINER Craig Citro <craigcitro@google.com>\n    .\n   85  # Configure the build for our CUDA configuration.\n   86  ENV CUDA_TOOLKIT_PATH /usr/local/cuda\n   87: ENV CUDNN_INSTALL_PATH /usr/local/cuda\n   88  ENV TF_NEED_CUDA 1\n   89  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\tools\\docker\\Dockerfile.gpu:\n    1: FROM nvidia/cuda:7.5-cudnn4-devel\n    2  \n    3  MAINTAINER Craig Citro <craigcitro@google.com>\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\third_party\\gpus\\cuda\\BUILD:\n    4  load(\"platform\", \"cuda_library_path\")\n    5  load(\"platform\", \"cuda_static_library_path\")\n    6: load(\"platform\", \"cudnn_library_path\")\n    7  load(\"platform\", \"cupti_library_path\")\n    8  load(\"platform\", \"readlink_command\")\n    .\n   99  \n  100  cc_library(\n  101:     name = \"cudnn\",\n  102      srcs = [\n  103:         cudnn_library_path()\n  104      ],\n  105      data = [\n  106:         cudnn_library_path()\n  107      ],\n  108      includes = [\"include/\"],\n  ...\n  130          \":cudart\",\n  131          \":cublas\",\n  132:         \":cudnn\",\n  133          \":cufft\",\n  134      ],\n  ...\n  168          \"include/cuda.h\",\n  169          \"include/cublas.h\",\n  170:         \"include/cudnn.h\",\n  171          \"extras/CUPTI/include/cupti.h\",\n  172          cuda_static_library_path(\"cudart\"),\n  173          cuda_library_path(\"cublas\"),\n  174:         cudnn_library_path(),\n  175          cuda_library_path(\"cudart\"),\n  176          cuda_library_path(\"cufft\"),\n  ...\n  189             \"touch $(@D)/include/cuda.h\",\n  190             \"touch $(@D)/include/cublas.h\",\n  191:            \"touch $(@D)/include/cudnn.h\",\n  192             \"touch $(@D)/extras/CUPTI/include/cupti.h\",\n  193             \"touch $(@D)/{}\".format(cuda_static_library_path(\"cudart\")),\n  194             \"touch $(@D)/{}\".format(cuda_library_path(\"cublas\")),\n  195:            \"touch $(@D)/{}\".format(cudnn_library_path()),\n  196             \"touch $(@D)/{}\".format(cuda_library_path(\"cudart\")),\n  197             \"touch $(@D)/{}\".format(cuda_library_path(\"cufft\")),\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\third_party\\gpus\\cuda\\cuda_config.sh:\n   17  \n   18  # A simple script to configure the Cuda tree needed for the TensorFlow GPU\n   19: # build. We need both Cuda toolkit $TF_CUDA_VERSION and Cudnn $TF_CUDNN_VERSION.\n   20  # Useage:\n   21: #    * User edit cuda.config to point both Cuda toolkit and Cudnn libraries to their local path\n   22  #    * run cuda_config.sh to generate symbolic links in the source tree to reflect\n   23  #    * the file organizations needed by TensorFlow.\n   ..\n   55  OUTPUTDIR=${OUTPUTDIR:-../../..}\n   56  CUDA_TOOLKIT_PATH=${CUDA_TOOLKIT_PATH:-/usr/local/cuda}\n   57: CUDNN_INSTALL_BASEDIR=${CUDNN_INSTALL_PATH:-/usr/local/cuda}\n   58  \n   59  if [[ -z \"$TF_CUDA_VERSION\" ]]; then\n   ..\n   63  fi\n   64  \n   65: if [[ -z \"$TF_CUDNN_VERSION\" ]]; then\n   66:   TF_CUDNN_EXT=\"\"\n   67  else\n   68:   TF_CUDNN_EXT=\".$TF_CUDNN_VERSION\"\n   69  fi\n   70  \n   ..\n   76  ##############################################################################\n   77  Cuda $TF_CUDA_VERSION toolkit is missing.\n   78: 1. Download and install the CUDA $TF_CUDA_VERSION toolkit and CUDNN $TF_CUDNN_VERSION library;\n   79  2. Run configure from the root of the source tree, before rerunning bazel;\n   80  Please refer to README.md for more details.\n   ..\n   85  }\n   86  \n   87: # An error message when CUDNN is not found\n   88: function CudnnError {\n   89    echo ERROR: $1\n   90  cat << EOF\n   91  ##############################################################################\n   92  ##############################################################################\n   93: Cudnn $TF_CUDNN_VERSION is missing.\n   94: 1. Download and install the CUDA $TF_CUDA_VERSION toolkit and CUDNN $TF_CUDNN_VERSION library;\n   95  2. Run configure from the root of the source tree, before rerunning bazel;\n   96  Please refer to README.md for more details.\n   ..\n  125    CUDA_RT_LIB_STATIC_PATH=\"lib64/libcudart_static.a\"\n  126    CUDA_BLAS_LIB_PATH=\"lib64/libcublas.so${TF_CUDA_EXT}\"\n  127:   CUDA_DNN_LIB_PATH=\"lib64/libcudnn.so${TF_CUDNN_EXT}\"\n  128:   CUDA_DNN_LIB_ALT_PATH=\"libcudnn.so${TF_CUDNN_EXT}\"\n  129    CUDA_FFT_LIB_PATH=\"lib64/libcufft.so${TF_CUDA_EXT}\"\n  130    CUDA_CUPTI_LIB_PATH=\"extras/CUPTI/lib64/libcupti.so${TF_CUDA_EXT}\"\n  ...\n  136    CUDA_RT_LIB_STATIC_PATH=\"lib/libcudart_static.a\"\n  137    CUDA_BLAS_LIB_PATH=\"lib/libcublas${TF_CUDA_EXT}.dylib\"\n  138:   CUDA_DNN_LIB_PATH=\"lib/libcudnn${TF_CUDNN_EXT}.dylib\"\n  139:   CUDA_DNN_LIB_ALT_PATH=\"libcudnn${TF_CUDNN_EXT}.dylib\"\n  140    CUDA_FFT_LIB_PATH=\"lib/libcufft${TF_CUDA_EXT}.dylib\"\n  141    CUDA_CUPTI_LIB_PATH=\"extras/CUPTI/lib/libcupti${TF_CUDA_EXT}.dylib\"\n  ...\n  146    CheckAndLinkToSrcTree CudaError include/cuda.h\n  147    CheckAndLinkToSrcTree CudaError include/cublas.h\n  148:   CheckAndLinkToSrcTree CudnnError include/cudnn.h\n  149    CheckAndLinkToSrcTree CudaError extras/CUPTI/include/cupti.h\n  150    CheckAndLinkToSrcTree CudaError $CUDA_RT_LIB_STATIC_PATH\n  151    CheckAndLinkToSrcTree CudaError $CUDA_BLAS_LIB_PATH\n  152:   CheckAndLinkToSrcTree CudnnError $CUDA_DNN_LIB_PATH\n  153    CheckAndLinkToSrcTree CudaError $CUDA_RT_LIB_PATH\n  154    CheckAndLinkToSrcTree CudaError $CUDA_FFT_LIB_PATH\n  ...\n  168  fi\n  169  \n  170: if test ! -d ${CUDNN_INSTALL_BASEDIR}; then\n  171:   CudnnError \"cannot find dir: ${CUDNN_INSTALL_BASEDIR}\"\n  172  fi\n  173  \n  174: # Locate cudnn.h\n  175: if test -e ${CUDNN_INSTALL_BASEDIR}/cudnn.h; then\n  176:   CUDNN_HEADER_DIR=${CUDNN_INSTALL_BASEDIR}\n  177: elif test -e ${CUDNN_INSTALL_BASEDIR}/include/cudnn.h; then\n  178:   CUDNN_HEADER_DIR=${CUDNN_INSTALL_BASEDIR}/include\n  179: elif test -e /usr/include/cudnn.h; then\n  180:   CUDNN_HEADER_DIR=/usr/include\n  181  else\n  182:   CudnnError \"cannot find cudnn.h under: ${CUDNN_INSTALL_BASEDIR}\"\n  183  fi\n  184  \n  185: # Locate libcudnn\n  186: if test -e ${CUDNN_INSTALL_BASEDIR}/${CUDA_DNN_LIB_PATH}; then\n  187:   CUDNN_LIB_INSTALL_PATH=${CUDNN_INSTALL_BASEDIR}/${CUDA_DNN_LIB_PATH}\n  188: elif test -e ${CUDNN_INSTALL_BASEDIR}/${CUDA_DNN_LIB_ALT_PATH}; then\n  189:   CUDNN_LIB_INSTALL_PATH=${CUDNN_INSTALL_BASEDIR}/${CUDA_DNN_LIB_ALT_PATH}\n  190  else\n  191:   CudnnError \"cannot find ${CUDA_DNN_LIB_PATH} or ${CUDA_DNN_LIB_ALT_PATH} under: ${CUDNN_INSTALL_BASEDIR}\"\n  192  fi\n  193  \n  ...\n  230  LinkAllFiles ${CUDA_TOOLKIT_PATH}/${CUDA_CUPTI_LIB_DIR} $OUTPUTDIR/third_party/gpus/cuda/${CUDA_CUPTI_LIB_DIR} || exit -1\n  231  \n  232: # Set up symbolic link for cudnn\n  233: ln -sf $CUDNN_HEADER_DIR/cudnn.h $OUTPUTDIR/third_party/gpus/cuda/include/cudnn.h || exit -1\n  234: ln -sf $CUDNN_LIB_INSTALL_PATH $OUTPUTDIR/third_party/gpus/cuda/$CUDA_DNN_LIB_PATH || exit -1\n  235  \n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\third_party\\gpus\\cuda\\platform.bzl:\n    1  CUDA_VERSION = \"\"\n    2: CUDNN_VERSION = \"\"\n    3  PLATFORM = \"\"\n    4  \n    .\n    6    return CUDA_VERSION\n    7  \n    8: def cudnn_sdk_version():\n    9:   return CUDNN_VERSION\n   10  \n   11  def cuda_library_path(name, version = cuda_sdk_version()):\n   ..\n   27      return \"lib64/lib{}_static.a\".format(name)\n   28  \n   29: def cudnn_library_path(version = cudnn_sdk_version()):\n   30    if PLATFORM == \"Darwin\":\n   31      if not version:\n   32:       return \"lib/libcudnn.dylib\"\n   33      else:\n   34:       return \"lib/libcudnn.{}.dylib\".format(version)\n   35    else:\n   36      if not version:\n   37:       return \"lib64/libcudnn.so\"\n   38      else:\n   39:       return \"lib64/libcudnn.so.{}\".format(version)\n   40  \n   41  def cupti_library_path(version = cuda_sdk_version()):\n\n2815 matches across 117 files\n\n\nSearching 8909 files for \":cudnn\"\n\nC:\\dev\\vm\\data-share\\serving\\bazel-bin:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-genfiles:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-out:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-serving:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-testlogs:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\caffe.BUILD:\n  205      ),\n  206      deps = if_cuda([\n  207:         \"@org_tensorflow//third_party/gpus/cuda:cudnn\",\n  208          \"@org_tensorflow//third_party/gpus/cuda:cublas\",\n  209          \":curand\"\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\BUILD:\n   17          \"//tensorflow/contrib/copy_graph:copy_graph_py\",\n   18          \"//tensorflow/contrib/crf:crf_py\",\n   19:         \"//tensorflow/contrib/cudnn_rnn:cudnn_rnn_py\",\n   20          \"//tensorflow/contrib/distributions:distributions_py\",\n   21          \"//tensorflow/contrib/factorization:factorization_py\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\cmake\\tf_stream_executor.cmake:\n   23  #        \"//tensorflow/core:cuda\",\n   24  #        \"//third_party/gpus/cuda:cublas\",\n   25: #        \"//third_party/gpus/cuda:cudnn\",\n   26  #    ],\n   27  #    linkopts = [\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\cudnn_rnn\\BUILD:\n   35  tf_gen_op_wrapper_py(\n   36      name = \"cudnn_rnn_ops\",\n   37:     deps = [\":cudnn_rnn_ops_op_lib\"],\n   38  )\n   39  \n   ..\n   50      visibility = [\"//visibility:public\"],\n   51      deps = [\n   52:         \":cudnn_rnn_ops\",\n   53      ],\n   54  )\n   ..\n   59      srcs = [\"python/kernel_tests/cudnn_rnn_ops_test.py\"],\n   60      additional_deps = [\n   61:         \":cudnn_rnn_py\",\n   62          \"//tensorflow:tensorflow_py\",\n   63          \"//tensorflow/python:framework_test_lib\",\n   ..\n   75      srcs = [\"python/kernel_tests/cudnn_rnn_ops_benchmark.py\"],\n   76      additional_deps = [\n   77:         \":cudnn_rnn_py\",\n   78          \"//tensorflow:tensorflow_py\",\n   79          \"//tensorflow/contrib/rnn:rnn_py\",\n   ..\n   94      ],\n   95      deps = [\n   96:         \":cudnn_rnn_ops_op_lib\",\n   97          \"//tensorflow/core\",\n   98          \"//tensorflow/core:framework\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\BUILD:\n   29          \"//tensorflow/core:cuda\",\n   30          \"@local_config_cuda//cuda:cublas\",\n   31:         \"@local_config_cuda//cuda:cudnn\",\n   32          \"@local_config_cuda//cuda:cufft\",\n   33          \"@local_config_cuda//cuda:curand\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:\n  334  }  // namespace\n  335  \n  336: CudnnSupport::CudnnSupport(CUDAExecutor* parent)\n  337      : parent_(parent), dnn_handle_(nullptr) {}\n  338  \n  339  CudnnSupport::~CudnnSupport() {\n  340:   auto status = dynload::cudnnDestroy(parent_, ToHandle(dnn_handle_));\n  341    if (status != CUDNN_STATUS_SUCCESS) {\n  342      LOG(ERROR) << \"could not destroy cudnn handle: \" << ToString(status);\n  ...\n  345  \n  346  port::Status CudnnSupport::Init() {\n  347:   auto status = dynload::cudnnCreate(\n  348        parent_, reinterpret_cast<cudnnHandle_t*>(&dnn_handle_));\n  349    if (status == CUDNN_STATUS_SUCCESS) {\n  350      // Check whether loaded version of CuDNN matches what the source\n  351      // was built with.\n  352:     size_t loaded_version = dynload::cudnnGetVersion();\n  353      size_t loaded_compat_version = cudnnCompatibilityVersion(loaded_version);\n  354      size_t compiled_compat_version = cudnnCompatibilityVersion(CUDNN_VERSION);\n  ...\n  408        : parent_(parent), handle_(nullptr) {\n  409      cudnnStatus_t status =\n  410:         dynload::cudnnCreateTensorDescriptor(parent_, &handle_);\n  411      if (status != CUDNN_STATUS_SUCCESS) {\n  412        LOG(FATAL) << \"could not create cudnn tensor descriptor: \"\n  ...\n  438      std::transform(dims64.cbegin(), dims64.cend(), dims.begin(),\n  439                     &CheckedNarrowing<int64, int>);\n  440:     status = dynload::cudnnSetTensorNdDescriptor(\n  441          parent_, handle_, elem_type, nd, dims.data(), strides.data());\n  442  \n  ...\n  449    ~ScopedTensorDescriptor() {\n  450      cudnnStatus_t status =\n  451:         dynload::cudnnDestroyTensorDescriptor(parent_, handle_);\n  452      if (status != CUDNN_STATUS_SUCCESS) {\n  453        LOG(ERROR) << \"could not destroy cudnn tensor descriptor: \"\n  ...\n  474        : parent_(parent), handle_(nullptr) {\n  475      cudnnStatus_t status =\n  476:         dynload::cudnnCreateFilterDescriptor(parent_, &handle_);\n  477      if (status != CUDNN_STATUS_SUCCESS) {\n  478        LOG(FATAL) << \"could not create cudnn filter descriptor: \"\n  ...\n  503      std::copy(spatial_dims.begin(), spatial_dims.end(), dims.begin() + 2);\n  504  \n  505:     status = dynload::cudnnSetFilterNdDescriptor(parent_, handle_, elem_type,\n  506  #if CUDNN_VERSION >= 5000\n  507                                                   format,\n  ...\n  516    ~ScopedFilterDescriptor() {\n  517      cudnnStatus_t status =\n  518:         dynload::cudnnDestroyFilterDescriptor(parent_, handle_);\n  519      if (status != CUDNN_STATUS_SUCCESS) {\n  520        LOG(ERROR) << \"could not destroy cudnn filter descriptor: \"\n  ...\n  544        : parent_(parent), handle_(nullptr) {\n  545      cudnnStatus_t status =\n  546:         dynload::cudnnCreateConvolutionDescriptor(parent_, &handle_);\n  547      if (status != CUDNN_STATUS_SUCCESS) {\n  548        LOG(FATAL) << \"could not create cudnn convolution descriptor: \"\n  ...\n  561      std::vector<int> upscale(convolution_descriptor.ndims(), 1);\n  562  \n  563:     status = dynload::cudnnSetConvolutionNdDescriptor(\n  564          parent_, handle_, convolution_descriptor.ndims(), padding.data(),\n  565          strides.data(), upscale.data(),\n  ...\n  577    ~ScopedConvolutionDescriptor() {\n  578      cudnnStatus_t status =\n  579:         dynload::cudnnDestroyConvolutionDescriptor(parent_, handle_);\n  580      if (status != CUDNN_STATUS_SUCCESS) {\n  581        LOG(ERROR) << \"could not destroy cudnn convolution descriptor: \"\n  ...\n  601        : parent_(parent), handle_(nullptr) {\n  602      cudnnStatus_t status =\n  603:         dynload::cudnnCreatePoolingDescriptor(parent_, &handle_);\n  604      if (status != CUDNN_STATUS_SUCCESS) {\n  605        LOG(FATAL) << \"could not create cudnn pooling descriptor: \"\n  ...\n  621      std::transform(shape64.cbegin(), shape64.cend(), shape.begin(),\n  622                     &CheckedNarrowing<int64, int>);\n  623:     status = dynload::cudnnSetPoolingNdDescriptor(\n  624          parent_, handle_,\n  625          (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum\n  ...\n  638    ~ScopedPoolingDescriptor() {\n  639      cudnnStatus_t status =\n  640:         dynload::cudnnDestroyPoolingDescriptor(parent_, handle_);\n  641      if (status != CUDNN_STATUS_SUCCESS) {\n  642        LOG(ERROR) << \"could not destroy cudnn pooling descriptor: \"\n  ...\n  660                              const NormalizeDescriptor& normalize_descriptor)\n  661        : parent_(parent), handle_(nullptr) {\n  662:     cudnnStatus_t status = dynload::cudnnCreateLRNDescriptor(parent_, &handle_);\n  663      if (status != CUDNN_STATUS_SUCCESS) {\n  664        LOG(FATAL) << \"could not create cudnn LRN descriptor: \"\n  ...\n  686      double lrnBeta = normalize_descriptor.beta();\n  687      double lrnK = normalize_descriptor.bias();\n  688:     status = dynload::cudnnSetLRNDescriptor(parent_, handle_, lrnN, lrnAlpha,\n  689                                              lrnBeta, lrnK);\n  690      if (status != CUDNN_STATUS_SUCCESS) {\n  ...\n  694  \n  695    ~ScopedNormalizeDescriptor() {\n  696:     cudnnStatus_t status = dynload::cudnnDestroyLRNDescriptor(parent_, handle_);\n  697      if (status != CUDNN_STATUS_SUCCESS) {\n  698        LOG(ERROR) << \"could not destroy cudnn LRN descriptor: \"\n  ...\n  720        : parent_(parent), handle_(nullptr) {\n  721      cudnnStatus_t status =\n  722:         dynload::cudnnCreateActivationDescriptor(parent_, &handle_);\n  723      if (status != CUDNN_STATUS_SUCCESS) {\n  724        LOG(FATAL) << \"could not create cudnn activation descriptor: \"\n  ...\n  753      // Always propagate nans.\n  754      cudnnNanPropagation_t nan_propagation = CUDNN_PROPAGATE_NAN;\n  755:     status = dynload::cudnnSetActivationDescriptor(\n  756          parent_, handle_,\n  757          mode, nan_propagation, relu_ceiling);\n  ...\n  764    ~ScopedActivationDescriptor() {\n  765      cudnnStatus_t status =\n  766:         dynload::cudnnDestroyActivationDescriptor(parent_, handle_);\n  767      if (status != CUDNN_STATUS_SUCCESS) {\n  768        LOG(ERROR) << \"could not destroy cudnn activation descriptor: \"\n  ...\n  879        : parent_(parent), handle_(nullptr) {\n  880      cudnnStatus_t status;\n  881:     status = dynload::cudnnCreateDropoutDescriptor(parent_, &handle_);\n  882      CUDNN_RETURN_IF_FAIL(status, \"Failed to create dropout descriptor\");\n  883  \n  ...\n  889      if (state_allocator) {\n  890        size_t state_sizes_in_bytes = 0;\n  891:       status = dynload::cudnnDropoutGetStatesSize(parent_, cudnn_handle,\n  892                                                    &state_sizes_in_bytes);\n  893        CUDNN_RETURN_IF_FAIL(status, \"Failed to query dropout state sizes\");\n  ...\n  904        }\n  905      }\n  906:     status = dynload::cudnnSetDropoutDescriptor(parent_, handle_, cudnn_handle,\n  907                                                  dropout, state_memory.opaque(),\n  908                                                  state_memory.size(), seed);\n  ...\n  913      if (handle_) {\n  914        cudnnStatus_t status =\n  915:           dynload::cudnnDestroyDropoutDescriptor(parent_, handle_);\n  916        CUDNN_RETURN_IF_FAIL(status, \"Failed to destroy Cudnn dropout handle: \");\n  917      }\n  ...\n  940    ~CudnnRnnParamsDescriptor() {\n  941      cudnnStatus_t status =\n  942:         dynload::cudnnDestroyFilterDescriptor(parent_, handle_);\n  943      CUDNN_RETURN_IF_FAIL(status, \"Failed to destroy RNN filter desciptor\");\n  944    }\n  ...\n  997      // Create the RNN handle\n  998      cudnnStatus_t status =\n  999:         dynload::cudnnCreateRNNDescriptor(parent_, &rnn_desc_);\n 1000      CUDNN_RETURN_IF_FAIL(status, \"Unable to create RNN descriptor\");\n 1001:     status = dynload::cudnnSetRNNDescriptor(\n 1002          parent, rnn_desc_ /*rnnDesc*/, hidden_size /*hiddenSize*/,\n 1003          num_layers /*numLayers*/, dropout_handle() /*dropoutDesc*/,\n ....\n 1017      if (rnn_desc_) {\n 1018        cudnnStatus_t status =\n 1019:           dynload::cudnnDestroyRNNDescriptor(parent_, rnn_desc_);\n 1020        CUDNN_RETURN_IF_FAIL(status, \"Unable to destroy RNN descriptor\");\n 1021      }\n ....\n 1068  };\n 1069  \n 1070: CudnnRnnParamsDescriptor::CudnnRnnParamsDescriptor(\n 1071      CUDAExecutor* parent, cudnnHandle_t cudnn_handle,\n 1072      const CudnnRnnDescriptor& rnn_desc)\n ....\n 1078    {\n 1079      // Query the params size.\n 1080:     auto status = dynload::cudnnCreateTensorDescriptor(parent, &input_desc);\n 1081      CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to create tensor descriptor\");\n 1082      int dims[] = {1, rnn_desc.input_size(), 1};\n 1083      int strides[] = {dims[1] * dims[2], dims[2], 1};\n 1084:     status = dynload::cudnnSetTensorNdDescriptor(\n 1085          parent, input_desc /*tensorDesc*/, rnn_desc.data_type() /*dataType*/,\n 1086          sizeof(dims) / sizeof(dims[0]) /*nbDims*/, dims /*dimA*/,\n ....\n 1089  \n 1090      size_t params_size = 0;\n 1091:     status = dynload::cudnnGetRNNParamsSize(\n 1092          parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n 1093          input_desc /*xDesc*/, &params_size /*sizeInBytes*/,\n ....\n 1099    {\n 1100      // Create the params descriptor.\n 1101:     auto status = dynload::cudnnCreateFilterDescriptor(parent, &handle_);\n 1102      CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to create RNN filter descriptor\");\n 1103      int dims[] = {static_cast<int>(params_size_in_bytes_), 1, 1};\n 1104:     status = dynload::cudnnSetFilterNdDescriptor(\n 1105          parent, handle_ /*filterDesc*/, rnn_desc.data_type() /*dataType*/,\n 1106          CUDNN_TENSOR_NCHW /*format*/, sizeof(dims) / sizeof(dims[0]) /*nbDims*/,\n ....\n 1114      cudnnFilterDescriptor_t region_desc_handle = nullptr;\n 1115      auto status =\n 1116:         dynload::cudnnCreateFilterDescriptor(parent, &region_desc_handle);\n 1117      CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to create filter descriptor\");\n 1118      for (int layer = 0; layer < rnn_desc.num_layers(); layer++) {\n ....\n 1121            void* offset = nullptr;\n 1122            if (type == 0) {\n 1123:             status = dynload::cudnnGetRNNLinLayerMatrixParams(\n 1124                  parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n 1125                  layer /*layer*/, input_desc /*xDesc*/, handle_ /*wDesc*/,\n ....\n 1130                  status, \"Cudnn fails to call cudnnGetRNNLinLayerMatrixParams\");\n 1131            } else {\n 1132:             status = dynload::cudnnGetRNNLinLayerBiasParams(\n 1133                  parent, cudnn_handle /*rnnDesc*/, rnn_desc.handle() /*rnnDesc*/,\n 1134                  layer /*layer*/, input_desc /*xDesc*/, handle_ /*wDesc*/,\n ....\n 1143            cudnnTensorFormat_t tensor_format;\n 1144            int n_dims;\n 1145:           status = dynload::cudnnGetFilterNdDescriptor(\n 1146                parent, region_desc_handle /*filterDesc*/,\n 1147                sizeof(dims) / sizeof(dims[0]) /*nbDimsRequested*/,\n ....\n 1160        }\n 1161      }\n 1162:     status = dynload::cudnnDestroyFilterDescriptor(parent, region_desc_handle);\n 1163      CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to destroy filter descriptor\");\n 1164    }\n ....\n 1166    {\n 1167      // Release the dummy input tensor descriptor.\n 1168:     auto status = dynload::cudnnDestroyTensorDescriptor(parent, input_desc);\n 1169      CUDNN_RETURN_IF_FAIL(status, \"Cudnn fails to destroy tensor descriptor\");\n 1170    }\n ....\n 1206      }\n 1207      cudnnStatus_t status =\n 1208:         dynload::cudnnCreateTensorDescriptor(parent, &handle);\n 1209      CUDNN_RETURN_IF_FAIL(status, \"Failed to create tensor descriptor\");\n 1210      int dims[] = {batch_size, data_size, 1};\n 1211      int strides[] = {dims[1] * dims[2], dims[2], 1};\n 1212:     status = dynload::cudnnSetTensorNdDescriptor(\n 1213          parent, handle /*tensorDesc*/, data_type /*dataType*/,\n 1214          sizeof(dims) / sizeof(dims[0]) /*nbDims*/, dims /*dimA*/,\n ....\n 1222      // Only the first one needs to be destroyed. All others are the same.\n 1223      cudnnStatus_t status =\n 1224:         dynload::cudnnDestroyTensorDescriptor(parent_, handles_[0]);\n 1225      CUDNN_RETURN_IF_FAIL(status, \"Failed to destroy sequence tensor desciptor\");\n 1226    }\n ....\n 1260          data_type_(data_type) {\n 1261      cudnnStatus_t status =\n 1262:         dynload::cudnnCreateTensorDescriptor(parent, &handle_);\n 1263      CUDNN_RETURN_IF_FAIL(status, \"Failed to create tensor descriptor\");\n 1264      int dims[] = {num_layers, batch_size, data_size};\n 1265      int strides[] = {dims[1] * dims[2], dims[2], 1};\n 1266:     status = dynload::cudnnSetTensorNdDescriptor(\n 1267          parent, handle_ /*tensorDesc*/, data_type /*dataType*/,\n 1268          sizeof(dims) / sizeof(dims[0]) /*nbDims*/, dims /*dimA*/,\n ....\n 1274      if (!handle_) {\n 1275        cudnnStatus_t status =\n 1276:           dynload::cudnnDestroyTensorDescriptor(parent_, handle_);\n 1277        CUDNN_RETURN_IF_FAIL(status, \"Unable to destroy RNN state tensor\");\n 1278      }\n ....\n 1374                             const CudnnRnnSequenceTensorDescriptor& input_desc) {\n 1375    size_t params_size_in_bytes = 0;\n 1376:   cudnnStatus_t status = dynload::cudnnGetRNNParamsSize(\n 1377        parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n 1378        input_desc.handles()[0] /*xDesc*/, &params_size_in_bytes /*sizeInBytes*/,\n ....\n 1394    // Query the workspace size.\n 1395    size_t workspace_size_in_bytes = 0;\n 1396:   cudnnStatus_t status = dynload::cudnnGetRNNWorkspaceSize(\n 1397        parent, cudnn_handle /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n 1398        input_desc.seq_length() /*seqLength*/, input_desc.handles() /*xDesc*/,\n ....\n 1469    if (is_training) {\n 1470      size_t reserve_space_size_in_bytes = 0;\n 1471:     cudnnStatus_t status = dynload::cudnnGetRNNTrainingReserveSize(\n 1472          parent_, ToHandle(dnn_handle_) /*handle*/,\n 1473          rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n ....\n 1492    // make the forward call\n 1493    if (!is_training) {\n 1494:     cudnnStatus_t status = dynload::cudnnRNNForwardInference(\n 1495          parent_, ToHandle(dnn_handle_) /*handle*/,\n 1496          rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n ....\n 1510      }\n 1511    } else {\n 1512:     cudnnStatus_t status = dynload::cudnnRNNForwardTraining(\n 1513          parent_, ToHandle(dnn_handle_) /*handle*/,\n 1514          rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n ....\n 1587  \n 1588    // make the backward data call\n 1589:   cudnnStatus_t status = dynload::cudnnRNNBackwardData(\n 1590        parent_, ToHandle(dnn_handle_) /*handle*/, rnn_desc.handle() /*rnnDesc*/,\n 1591        model_dims.seq_length /*seqLength*/, output_desc.handles() /*yDesc*/,\n ....\n 1615      stream->ThenMemZero(params_backprop_data, params_backprop_data->size());\n 1616      // make the backward weight call\n 1617:     status = dynload::cudnnRNNBackwardWeights(\n 1618          parent_, ToHandle(dnn_handle_) /*handle*/,\n 1619          rnn_desc.handle() /*rnnDesc*/, model_dims.seq_length /*seqLength*/,\n ....\n 1832  \n 1833    mutex_lock lock{dnn_handle_mutex_};\n 1834:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1835                                          AsCUDAStreamValue(stream));\n 1836    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 1863  \n 1864            cudnnConvolutionFwdAlgo_t algo_to_use;\n 1865:           status = dynload::cudnnGetConvolutionForwardAlgorithm(\n 1866                parent_, ToHandle(dnn_handle_), input_nd.handle(),\n 1867                filter.handle(), conv.handle(), output_nd.handle(),\n ....\n 1880      if (scratch_allocator != nullptr) {\n 1881        size_t size_in_bytes;\n 1882:       status = dynload::cudnnGetConvolutionForwardWorkspaceSize(\n 1883            parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 1884            /*filterDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n ....\n 1904  \n 1905      size_t size_in_bytes;\n 1906:     status = dynload::cudnnGetConvolutionForwardWorkspaceSize(\n 1907          parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 1908          /*filterDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n ....\n 1954      }\n 1955    }\n 1956:   status = dynload::cudnnConvolutionForward(\n 1957        parent_, ToHandle(dnn_handle_),\n 1958        /*alpha=*/&alpha, /*srcDesc=*/input_nd.handle(),\n ....\n 2065      std::function<void()> inv_var_to_var) {\n 2066    mutex_lock lock{dnn_handle_mutex_};\n 2067:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2068                                          AsCUDAStreamValue(stream));\n 2069    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 2083      stream->ThenMemZero(batch_mean, batch_mean->size());\n 2084      stream->ThenMemZero(batch_var, batch_var->size());\n 2085:     status = dynload::cudnnBatchNormalizationForwardTraining(\n 2086          parent_, ToHandle(dnn_handle_), mode, &one, &zero,\n 2087          x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y->opaque(),\n ....\n 2100      const void* maybe_inv_var = estimated_variance.opaque();\n 2101  #endif\n 2102:     status = dynload::cudnnBatchNormalizationForwardInference(\n 2103          parent_, ToHandle(dnn_handle_), mode, &one, &zero,\n 2104          x_descriptor.handle(), x.opaque(), x_descriptor.handle(), y->opaque(),\n ....\n 2137      DeviceMemory<T>* offset_backprop) {\n 2138    mutex_lock lock{dnn_handle_mutex_};\n 2139:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2140                                          AsCUDAStreamValue(stream));\n 2141    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 2152    float zero = 0.0;\n 2153  \n 2154:   status = dynload::cudnnBatchNormalizationBackward(\n 2155        parent_, ToHandle(dnn_handle_), mode, &one, &zero, &one, &zero,\n 2156        x_descriptor.handle(), x.opaque(), x_descriptor.handle(),\n ....\n 2236    float alpha = 1.0f;\n 2237    float beta = 0.0f;\n 2238:   auto status = dynload::cudnnTransformTensor(\n 2239        parent_, ToHandle(dnn_handle_), &alpha, orig_out_back_nd.handle(),\n 2240        backward_output_data.opaque(), &beta, transformed_out_back_nd.handle(),\n ....\n 2262      dnn::ProfileResult* output_profile_result) {\n 2263    mutex_lock lock{dnn_handle_mutex_};\n 2264:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2265                                          AsCUDAStreamValue(stream));\n 2266    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 2315  \n 2316        cudnnConvolutionBwdDataAlgo_t algo_to_use;\n 2317:       cudnnStatus_t status = dynload::cudnnGetConvolutionBackwardDataAlgorithm(\n 2318            parent_, ToHandle(dnn_handle_),\n 2319            /*filterDesc=*/filter.handle(),\n ....\n 2334      if (scratch_allocator != nullptr) {\n 2335        size_t size_in_bytes;\n 2336:       status = dynload::cudnnGetConvolutionBackwardDataWorkspaceSize(\n 2337            parent_, ToHandle(dnn_handle_),\n 2338            /*filterDesc=*/filter.handle(),\n ....\n 2360      algo = ToConvBackwardDataAlgo(algorithm_config.algorithm());\n 2361      size_t size_in_bytes;\n 2362:     status = dynload::cudnnGetConvolutionBackwardDataWorkspaceSize(\n 2363          parent_, ToHandle(dnn_handle_),\n 2364          /*filterDesc=*/filter.handle(),\n ....\n 2410  \n 2411  #if CUDNN_VERSION >= 5000\n 2412:   status = dynload::cudnnConvolutionBackwardData(\n 2413  #else\n 2414:   status = dynload::cudnnConvolutionBackwardData_v3(\n 2415  #endif\n 2416        parent_, ToHandle(dnn_handle_),\n ....\n 2495      dnn::ProfileResult* output_profile_result) {\n 2496    mutex_lock lock{dnn_handle_mutex_};\n 2497:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2498                                          AsCUDAStreamValue(stream));\n 2499    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 2554        cudnnConvolutionBwdFilterAlgo_t algo_to_use;\n 2555        cudnnStatus_t status =\n 2556:           dynload::cudnnGetConvolutionBackwardFilterAlgorithm(\n 2557                parent_, ToHandle(dnn_handle_),\n 2558                /*srcDesc=*/input_nd.handle(),\n ....\n 2573      if (scratch_allocator != nullptr) {\n 2574        size_t size_in_bytes;\n 2575:       status = dynload::cudnnGetConvolutionBackwardFilterWorkspaceSize(\n 2576            parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 2577            /*diffDesc=*/out_back_nd.handle(), /*convDesc=*/conv.handle(),\n ....\n 2597  \n 2598      size_t size_in_bytes;\n 2599:     status = dynload::cudnnGetConvolutionBackwardFilterWorkspaceSize(\n 2600          parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 2601          /*diffDesc=*/out_back_nd.handle(), /*convDesc=*/conv.handle(),\n ....\n 2645  \n 2646  #if CUDNN_VERSION >= 5000\n 2647:   status = dynload::cudnnConvolutionBackwardFilter(\n 2648  #else\n 2649:   status = dynload::cudnnConvolutionBackwardFilter_v3(\n 2650  #endif\n 2651        parent_, ToHandle(dnn_handle_), /*alpha=*/&alpha,\n ....\n 2724      DeviceMemory<T>* backward_bias_data) {\n 2725    mutex_lock lock{dnn_handle_mutex_};\n 2726:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2727                                          AsCUDAStreamValue(stream));\n 2728    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 2740    float beta = 0.0;\n 2741  \n 2742:   status = dynload::cudnnConvolutionBackwardBias(\n 2743        parent_, ToHandle(dnn_handle_), &alpha, input_nd.handle(),\n 2744        input_data.opaque(), &beta, bias_nd.handle(),\n ....\n 2948  \n 2949    mutex_lock lock{dnn_handle_mutex_};\n 2950:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2951                                          AsCUDAStreamValue(stream));\n 2952    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 2959  \n 2960  #if CUDNN_VERSION >= 5000\n 2961:   status = dynload::cudnnAddTensor(\n 2962  #else\n 2963:   status = dynload::cudnnAddTensor_v3(\n 2964  #endif\n 2965        parent_, ToHandle(dnn_handle_), &alpha, bias_descriptor.handle(),\n ....\n 2981                                DeviceMemory<float>* output_data) {\n 2982    mutex_lock lock{dnn_handle_mutex_};\n 2983:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 2984                                          AsCUDAStreamValue(stream));\n 2985    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 3025    // Beta is the output scaling factor.\n 3026    float beta = 0.0;\n 3027:   status = dynload::cudnnActivationForward(\n 3028        parent_, ToHandle(dnn_handle_),\n 3029  #if CUDNN_VERSION >= 5000\n ....\n 3050      DeviceMemory<float>* output_data) {\n 3051    mutex_lock lock{dnn_handle_mutex_};\n 3052:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3053                                          AsCUDAStreamValue(stream));\n 3054    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 3066                                     CUDNN_DATA_FLOAT};\n 3067    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 3068:   status = dynload::cudnnPoolingForward(\n 3069        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 3070        src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(),\n ....\n 3085      DeviceMemory<Eigen::half>* output_data) {\n 3086    mutex_lock lock{dnn_handle_mutex_};\n 3087:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3088                                          AsCUDAStreamValue(stream));\n 3089    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 3100    ScopedTensorDescriptor dest_desc{parent_, output_dimensions, CUDNN_DATA_HALF};\n 3101    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 3102:   status = dynload::cudnnPoolingForward(\n 3103        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 3104        src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(),\n ....\n 3121      DeviceMemory<float>* output_diff_data) {\n 3122    mutex_lock lock{dnn_handle_mutex_};\n 3123:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3124                                          AsCUDAStreamValue(stream));\n 3125    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 3137                                     CUDNN_DATA_FLOAT};\n 3138    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 3139:   status = dynload::cudnnPoolingBackward(\n 3140        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 3141        dest_desc.handle(), output_data.opaque(), dest_desc.handle(),\n ....\n 3159      DeviceMemory<Eigen::half>* output_diff_data) {\n 3160    mutex_lock lock{dnn_handle_mutex_};\n 3161:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3162                                          AsCUDAStreamValue(stream));\n 3163    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 3174    ScopedTensorDescriptor dest_desc{parent_, output_dimensions, CUDNN_DATA_HALF};\n 3175    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 3176:   status = dynload::cudnnPoolingBackward(\n 3177        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 3178        dest_desc.handle(), output_data.opaque(), dest_desc.handle(),\n ....\n 3209    // Launch the normalization.\n 3210    mutex_lock lock{dnn_handle_mutex_};\n 3211:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3212                                          AsCUDAStreamValue(stream));\n 3213    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 3224    float beta = 0.0f;\n 3225  \n 3226:   status = dynload::cudnnLRNCrossChannelForward(\n 3227        parent_, ToHandle(dnn_handle_), normalize.handle(),\n 3228        CUDNN_LRN_CROSS_CHANNEL_DIM1, &alpha, dims.handle(), input_data.opaque(),\n ....\n 3252  \n 3253    mutex_lock lock{dnn_handle_mutex_};\n 3254:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 3255                                          AsCUDAStreamValue(stream));\n 3256    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 3265    float beta = 0.0f;\n 3266  \n 3267:   status = dynload::cudnnLRNCrossChannelBackward(\n 3268        parent_, ToHandle(dnn_handle_), normalize.handle(),\n 3269        CUDNN_LRN_CROSS_CHANNEL_DIM1, &alpha, dims.handle(),\n ....\n 3387    int dn = batch_descriptor.ndims() + 2;\n 3388    std::vector<int> dims(dn);  // in BDYX\n 3389:   auto status = dynload::cudnnGetConvolutionNdForwardOutputDim(\n 3390        parent_, conv.handle(), input_nd.handle(), filter.handle(), dn,\n 3391        dims.data());\n ....\n 3428                  }\n 3429  \n 3430:                 gpu::cuda::CudnnSupport* dnn =\n 3431:                     new gpu::cuda::CudnnSupport(cuda_executor);\n 3432                  if (!dnn->Init().ok()) {\n 3433                    // Note: Init() will log a more specific error.\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\stream_executor\\multi_platform_manager.h:\n   58  //\n   59  //    //perftools/gputools/executor/cuda:pluton_blas_plugin\n   60: //    //perftools/gputools/executor/cuda:cudnn_plugin\n   61  //    //perftools/gputools/executor/cuda:cublas_plugin\n   62  //    //perftools/gputools/executor/cuda:curand_plugin\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\gpus\\cuda\\BUILD.tpl:\n  137          \":cudart\",\n  138          \":cublas\",\n  139:         \":cudnn\",\n  140          \":cufft\",\n  141          \":curand\",\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\py\\numpy\\numpy_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_lib:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\contrib\\cmake\\tf_stream_executor.cmake:\n   23  #        \"//tensorflow/core:cuda\",\n   24  #        \"//third_party/gpus/cuda:cublas\",\n   25: #        \"//third_party/gpus/cuda:cudnn\",\n   26  #    ],\n   27  #    linkopts = [\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\stream_executor\\BUILD:\n   29          \"//tensorflow/core:cuda\",\n   30          \"//third_party/gpus/cuda:cublas\",\n   31:         \"//third_party/gpus/cuda:cudnn\",\n   32          \"//third_party/gpus/cuda:cufft\",\n   33      ],\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\stream_executor\\cuda\\cuda_dnn.cc:\n  318  }  // namespace\n  319  \n  320: CudnnSupport::CudnnSupport(CUDAExecutor* parent)\n  321      : parent_(parent), dnn_handle_(nullptr) {}\n  322  \n  323  CudnnSupport::~CudnnSupport() {\n  324:   auto status = dynload::cudnnDestroy(parent_, ToHandle(dnn_handle_));\n  325    if (status != CUDNN_STATUS_SUCCESS) {\n  326      LOG(ERROR) << \"could not destroy cudnn handle: \" << ToString(status);\n  ...\n  329  \n  330  port::Status CudnnSupport::Init() {\n  331:   auto status = dynload::cudnnCreate(\n  332        parent_, reinterpret_cast<cudnnHandle_t*>(&dnn_handle_));\n  333    if (status == CUDNN_STATUS_SUCCESS) {\n  334      // Check whether loaded version of CuDNN matches what the source\n  335      // was built with.\n  336:     size_t loaded_version = dynload::cudnnGetVersion();\n  337      bool library_loaded_matches_source = (loaded_version == CUDNN_VERSION);\n  338      if (!library_loaded_matches_source) {\n  ...\n  386        : parent_(parent), handle_(nullptr) {\n  387      cudnnStatus_t status =\n  388:         dynload::cudnnCreateTensorDescriptor(parent_, &handle_);\n  389      if (status != CUDNN_STATUS_SUCCESS) {\n  390        LOG(FATAL) << \"could not create cudnn tensor descriptor: \"\n  ...\n  416      std::transform(dims64.cbegin(), dims64.cend(), dims.begin(),\n  417                     &CheckedNarrowing<int64, int>);\n  418:     status = dynload::cudnnSetTensorNdDescriptor(\n  419          parent_, handle_, elem_type, nd, dims.data(), strides.data());\n  420  \n  ...\n  427    ~ScopedTensorDescriptor() {\n  428      cudnnStatus_t status =\n  429:         dynload::cudnnDestroyTensorDescriptor(parent_, handle_);\n  430      if (status != CUDNN_STATUS_SUCCESS) {\n  431        LOG(ERROR) << \"could not destroy cudnn tensor descriptor: \"\n  ...\n  452        : parent_(parent), handle_(nullptr) {\n  453      cudnnStatus_t status =\n  454:         dynload::cudnnCreateFilterDescriptor(parent_, &handle_);\n  455      if (status != CUDNN_STATUS_SUCCESS) {\n  456        LOG(FATAL) << \"could not create cudnn filter descriptor: \"\n  ...\n  481      std::copy(spatial_dims.begin(), spatial_dims.end(), dims.begin() + 2);\n  482  \n  483:     status = dynload::cudnnSetFilterNdDescriptor(parent_, handle_, elem_type,\n  484  #if CUDNN_VERSION >= 5000\n  485                                                   format,\n  ...\n  494    ~ScopedFilterDescriptor() {\n  495      cudnnStatus_t status =\n  496:         dynload::cudnnDestroyFilterDescriptor(parent_, handle_);\n  497      if (status != CUDNN_STATUS_SUCCESS) {\n  498        LOG(ERROR) << \"could not destroy cudnn filter descriptor: \"\n  ...\n  522        : parent_(parent), handle_(nullptr) {\n  523      cudnnStatus_t status =\n  524:         dynload::cudnnCreateConvolutionDescriptor(parent_, &handle_);\n  525      if (status != CUDNN_STATUS_SUCCESS) {\n  526        LOG(FATAL) << \"could not create cudnn convolution descriptor: \"\n  ...\n  539      std::vector<int> upscale(convolution_descriptor.ndims(), 1);\n  540  \n  541:     status = dynload::cudnnSetConvolutionNdDescriptor(\n  542          parent_, handle_, convolution_descriptor.ndims(), padding.data(),\n  543          strides.data(), upscale.data(),\n  ...\n  555    ~ScopedConvolutionDescriptor() {\n  556      cudnnStatus_t status =\n  557:         dynload::cudnnDestroyConvolutionDescriptor(parent_, handle_);\n  558      if (status != CUDNN_STATUS_SUCCESS) {\n  559        LOG(ERROR) << \"could not destroy cudnn convolution descriptor: \"\n  ...\n  579        : parent_(parent), handle_(nullptr) {\n  580      cudnnStatus_t status =\n  581:         dynload::cudnnCreatePoolingDescriptor(parent_, &handle_);\n  582      if (status != CUDNN_STATUS_SUCCESS) {\n  583        LOG(FATAL) << \"could not create cudnn pooling descriptor: \"\n  ...\n  599      std::transform(shape64.cbegin(), shape64.cend(), shape.begin(),\n  600                     &CheckedNarrowing<int64, int>);\n  601:     status = dynload::cudnnSetPoolingNdDescriptor(\n  602          parent_, handle_,\n  603          (pooling_descriptor.mode() == dnn::PoolingMode::kMaximum\n  ...\n  616    ~ScopedPoolingDescriptor() {\n  617      cudnnStatus_t status =\n  618:         dynload::cudnnDestroyPoolingDescriptor(parent_, handle_);\n  619      if (status != CUDNN_STATUS_SUCCESS) {\n  620        LOG(ERROR) << \"could not destroy cudnn pooling descriptor: \"\n  ...\n  642        : parent_(parent), handle_(nullptr) {\n  643      cudnnStatus_t status =\n  644:         dynload::cudnnCreateActivationDescriptor(parent_, &handle_);\n  645      if (status != CUDNN_STATUS_SUCCESS) {\n  646        LOG(FATAL) << \"could not create cudnn activation descriptor: \"\n  ...\n  675      // Always propagate nans.\n  676      cudnnNanPropagation_t nan_propagation = CUDNN_PROPAGATE_NAN;\n  677:     status = dynload::cudnnSetActivationDescriptor(\n  678          parent_, handle_,\n  679          mode, nan_propagation, relu_ceiling);\n  ...\n  686    ~ScopedActivationDescriptor() {\n  687      cudnnStatus_t status =\n  688:         dynload::cudnnDestroyActivationDescriptor(parent_, handle_);\n  689      if (status != CUDNN_STATUS_SUCCESS) {\n  690        LOG(ERROR) << \"could not destroy cudnn activation descriptor: \"\n  ...\n  728  \n  729    mutex_lock lock{dnn_handle_mutex_};\n  730:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n  731                                          AsCUDAStreamValue(stream));\n  732    if (status != CUDNN_STATUS_SUCCESS) {\n  ...\n  759  \n  760            cudnnConvolutionFwdAlgo_t algo_to_use;\n  761:           status = dynload::cudnnGetConvolutionForwardAlgorithm(\n  762                parent_, ToHandle(dnn_handle_), input_nd.handle(),\n  763                filter.handle(), conv.handle(), output_nd.handle(),\n  ...\n  776      if (scratch_allocator != nullptr) {\n  777        size_t size_in_bytes;\n  778:       status = dynload::cudnnGetConvolutionForwardWorkspaceSize(\n  779            parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n  780            /*filterDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n  ...\n  800  \n  801      size_t size_in_bytes;\n  802:     status = dynload::cudnnGetConvolutionForwardWorkspaceSize(\n  803          parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n  804          /*filterDesc=*/filter.handle(), /*convDesc=*/conv.handle(),\n  ...\n  845      timer->Start(AsCUDAStream(stream));\n  846    }\n  847:   status = dynload::cudnnConvolutionForward(\n  848        parent_, ToHandle(dnn_handle_),\n  849        /*alpha=*/&alpha, /*srcDesc=*/input_nd.handle(),\n  ...\n  990    float alpha = 1.0f;\n  991    float beta = 0.0f;\n  992:   auto status = dynload::cudnnTransformTensor(\n  993        parent_, ToHandle(dnn_handle_), &alpha, orig_out_back_nd.handle(),\n  994        backward_output_data.opaque(), &beta, transformed_out_back_nd.handle(),\n  ...\n 1016      dnn::ProfileResult* output_profile_result) {\n 1017    mutex_lock lock{dnn_handle_mutex_};\n 1018:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1019                                          AsCUDAStreamValue(stream));\n 1020    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 1053  #endif\n 1054  #if CUDNN_VERSION >= 4000\n 1055:     status = dynload::cudnnConvolutionBackwardData_v2(\n 1056  #else\n 1057:   status = dynload::cudnnConvolutionBackwardData(\n 1058  #endif\n 1059          parent_, ToHandle(dnn_handle_), &alpha, filter.handle(),\n ....\n 1094  \n 1095        cudnnConvolutionBwdDataAlgo_t algo_to_use;\n 1096:       cudnnStatus_t status = dynload::cudnnGetConvolutionBackwardDataAlgorithm(\n 1097            parent_, ToHandle(dnn_handle_),\n 1098            /*filterDesc=*/filter.handle(),\n ....\n 1113      if (scratch_allocator != nullptr) {\n 1114        size_t size_in_bytes;\n 1115:       status = dynload::cudnnGetConvolutionBackwardDataWorkspaceSize(\n 1116            parent_, ToHandle(dnn_handle_),\n 1117            /*filterDesc=*/filter.handle(),\n ....\n 1139      algo = ToConvBackwardDataAlgo(algorithm_config.algorithm());\n 1140      size_t size_in_bytes;\n 1141:     status = dynload::cudnnGetConvolutionBackwardDataWorkspaceSize(\n 1142          parent_, ToHandle(dnn_handle_),\n 1143          /*filterDesc=*/filter.handle(),\n ....\n 1189  \n 1190  #if CUDNN_VERSION >= 5000\n 1191:   status = dynload::cudnnConvolutionBackwardData(\n 1192  #else\n 1193:   status = dynload::cudnnConvolutionBackwardData_v3(\n 1194  #endif\n 1195        parent_, ToHandle(dnn_handle_),\n ....\n 1275      dnn::ProfileResult* output_profile_result) {\n 1276    mutex_lock lock{dnn_handle_mutex_};\n 1277:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1278                                          AsCUDAStreamValue(stream));\n 1279    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 1313  #endif\n 1314  #if CUDNN_VERSION >= 4000\n 1315:     status = dynload::cudnnConvolutionBackwardFilter_v2(\n 1316  #else\n 1317:   status = dynload::cudnnConvolutionBackwardFilter(\n 1318  #endif\n 1319          parent_, ToHandle(dnn_handle_), &alpha, input_nd.handle(),\n ....\n 1359        cudnnConvolutionBwdFilterAlgo_t algo_to_use;\n 1360        cudnnStatus_t status =\n 1361:           dynload::cudnnGetConvolutionBackwardFilterAlgorithm(\n 1362                parent_, ToHandle(dnn_handle_),\n 1363                /*srcDesc=*/input_nd.handle(),\n ....\n 1378      if (scratch_allocator != nullptr) {\n 1379        size_t size_in_bytes;\n 1380:       status = dynload::cudnnGetConvolutionBackwardFilterWorkspaceSize(\n 1381            parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 1382            /*diffDesc=*/out_back_nd.handle(), /*convDesc=*/conv.handle(),\n ....\n 1402  \n 1403      size_t size_in_bytes;\n 1404:     status = dynload::cudnnGetConvolutionBackwardFilterWorkspaceSize(\n 1405          parent_, ToHandle(dnn_handle_), /*srcDesc=*/input_nd.handle(),\n 1406          /*diffDesc=*/out_back_nd.handle(), /*convDesc=*/conv.handle(),\n ....\n 1450  \n 1451  #if CUDNN_VERSION >= 5000\n 1452:   status = dynload::cudnnConvolutionBackwardFilter(\n 1453  #else\n 1454:   status = dynload::cudnnConvolutionBackwardFilter_v3(\n 1455  #endif\n 1456        parent_, ToHandle(dnn_handle_), /*alpha=*/&alpha,\n ....\n 1530      DeviceMemory<T>* backward_bias_data) {\n 1531    mutex_lock lock{dnn_handle_mutex_};\n 1532:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1533                                          AsCUDAStreamValue(stream));\n 1534    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 1546    float beta = 0.0;\n 1547  \n 1548:   status = dynload::cudnnConvolutionBackwardBias(\n 1549        parent_, ToHandle(dnn_handle_), &alpha, input_nd.handle(),\n 1550        input_data.opaque(), &beta, bias_nd.handle(),\n ....\n 1754  \n 1755    mutex_lock lock{dnn_handle_mutex_};\n 1756:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1757                                          AsCUDAStreamValue(stream));\n 1758    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 1769  #if CUDNN_VERSION < 5000\n 1770  #if CUDNN_VERSION >= 4000\n 1771:     status = dynload::cudnnAddTensor_v2(\n 1772  #else\n 1773:     status = dynload::cudnnAddTensor(\n 1774  #endif\n 1775          parent_, ToHandle(dnn_handle_), CUDNN_ADD_SAME_C, &alpha,\n ....\n 1781    } else {\n 1782  #if CUDNN_VERSION >= 5000\n 1783:     status = dynload::cudnnAddTensor(\n 1784  #else\n 1785:     status = dynload::cudnnAddTensor_v3(\n 1786  #endif\n 1787          parent_, ToHandle(dnn_handle_), &alpha, bias_descriptor.handle(),\n ....\n 1805                                DeviceMemory<float>* output_data) {\n 1806    mutex_lock lock{dnn_handle_mutex_};\n 1807:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1808                                          AsCUDAStreamValue(stream));\n 1809    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 1849    // Beta is the output scaling factor.\n 1850    float beta = 0.0;\n 1851:   status = dynload::cudnnActivationForward(\n 1852        parent_, ToHandle(dnn_handle_),\n 1853  #if CUDNN_VERSION >= 5000\n ....\n 1874      DeviceMemory<float>* output_data) {\n 1875    mutex_lock lock{dnn_handle_mutex_};\n 1876:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1877                                          AsCUDAStreamValue(stream));\n 1878    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 1890                                     CUDNN_DATA_FLOAT};\n 1891    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 1892:   status = dynload::cudnnPoolingForward(\n 1893        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 1894        src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(),\n ....\n 1909      DeviceMemory<Eigen::half>* output_data) {\n 1910    mutex_lock lock{dnn_handle_mutex_};\n 1911:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1912                                          AsCUDAStreamValue(stream));\n 1913    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 1924    ScopedTensorDescriptor dest_desc{parent_, output_dimensions, CUDNN_DATA_HALF};\n 1925    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 1926:   status = dynload::cudnnPoolingForward(\n 1927        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 1928        src_desc.handle(), input_data.opaque(), &beta, dest_desc.handle(),\n ....\n 1945      DeviceMemory<float>* output_diff_data) {\n 1946    mutex_lock lock{dnn_handle_mutex_};\n 1947:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1948                                          AsCUDAStreamValue(stream));\n 1949    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 1961                                     CUDNN_DATA_FLOAT};\n 1962    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 1963:   status = dynload::cudnnPoolingBackward(\n 1964        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 1965        dest_desc.handle(), output_data.opaque(), dest_desc.handle(),\n ....\n 1983      DeviceMemory<Eigen::half>* output_diff_data) {\n 1984    mutex_lock lock{dnn_handle_mutex_};\n 1985:   auto status = dynload::cudnnSetStream(parent_, ToHandle(dnn_handle_),\n 1986                                          AsCUDAStreamValue(stream));\n 1987    if (status != CUDNN_STATUS_SUCCESS) {\n ....\n 1998    ScopedTensorDescriptor dest_desc{parent_, output_dimensions, CUDNN_DATA_HALF};\n 1999    ScopedPoolingDescriptor pooling_desc{parent_, pooling_dimensions};\n 2000:   status = dynload::cudnnPoolingBackward(\n 2001        parent_, ToHandle(dnn_handle_), pooling_desc.handle(), &alpha,\n 2002        dest_desc.handle(), output_data.opaque(), dest_desc.handle(),\n ....\n 2127    int dn = batch_descriptor.ndims() + 2;\n 2128    std::vector<int> dims(dn);  // in BDYX\n 2129:   auto status = dynload::cudnnGetConvolutionNdForwardOutputDim(\n 2130        parent_, conv.handle(), input_nd.handle(), filter.handle(), dn,\n 2131        dims.data());\n ....\n 2168                  }\n 2169  \n 2170:                 gpu::cuda::CudnnSupport* dnn =\n 2171:                     new gpu::cuda::CudnnSupport(cuda_executor);\n 2172                  if (!dnn->Init().ok()) {\n 2173                    // Note: Init() will log a more specific error.\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\stream_executor\\multi_platform_manager.h:\n   58  //\n   59  //    //perftools/gputools/executor/cuda:pluton_blas_plugin\n   60: //    //perftools/gputools/executor/cuda:cudnn_plugin\n   61  //    //perftools/gputools/executor/cuda:cublas_plugin\n   62  //    //perftools/gputools/executor/cuda:curand_plugin\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\third_party\\gpus\\cuda\\BUILD:\n  130          \":cudart\",\n  131          \":cublas\",\n  132:         \":cudnn\",\n  133          \":cufft\",\n  134      ],\n\n176 matches across 13 files\n\n\nSearching 8909 files for \"session_bundle/signature.h\"\n\nC:\\dev\\vm\\data-share\\serving\\bazel-bin:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-genfiles:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-out:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-serving:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\bazel-testlogs:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\session_bundle\\README.md:\n  120  #### Recovering signatures\n  121  \n  122: These can be recovered at serving time using utilities in [`signature.h`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/session_bundle/signature.h)\n  123  \n  124  ~~~c++\n  ...\n  176  be less stable and/or auto-generated by TensorFlow.\n  177  \n  178: In [`signature.h`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/session_bundle/signature.h)\n  179  note that the generic signature methods `BindGenericInputs` and\n  180  `BindGenericNames` are doing simple string to string mapping as a convenience.\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\session_bundle\\session_bundle.h:\n   22  \n   23  #include \"tensorflow/contrib/session_bundle/manifest.pb.h\"\n   24: #include \"tensorflow/contrib/session_bundle/signature.h\"\n   25  #include \"tensorflow/core/lib/core/status.h\"\n   26  #include \"tensorflow/core/lib/core/stringpiece.h\"\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\session_bundle\\session_bundle_test.cc:\n   21  \n   22  #include \"google/protobuf/any.pb.h\"\n   23: #include \"tensorflow/contrib/session_bundle/signature.h\"\n   24  #include \"tensorflow/contrib/session_bundle/test_util.h\"\n   25  #include \"tensorflow/core/framework/tensor.h\"\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\session_bundle\\signature.cc:\n   14  ==============================================================================*/\n   15  \n   16: #include \"tensorflow/contrib/session_bundle/signature.h\"\n   17  \n   18  #include <string>\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\tensorflow\\contrib\\session_bundle\\signature_test.cc:\n   14  ==============================================================================*/\n   15  \n   16: #include \"tensorflow/contrib/session_bundle/signature.h\"\n   17  \n   18  #include <memory>\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\third_party\\py\\numpy\\numpy_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_include:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow\\util\\python\\python_lib:\n    ERROR: Unable to open file\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow_serving\\example\\obj_detector.cc:\n   51  #include \"tensorflow_serving/core/servable_handle.h\"\n   52  #include \"tensorflow_serving/core/servable_id.h\"\n   53: #include \"tensorflow_serving/session_bundle/signature.h\"\n   54  \n   55  // service api\n\nC:\\dev\\vm\\data-share\\serving\\tensorflow_serving\\servables\\tensorflow\\predict_impl.cc:\n   23  \n   24  #include \"tensorflow/contrib/session_bundle/session_bundle.h\"\n   25: #include \"tensorflow/contrib/session_bundle/signature.h\"\n   26  #include \"tensorflow/core/lib/core/errors.h\"\n   27  #include \"tensorflow/core/protobuf/named_tensor.pb.h\"\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\contrib\\session_bundle\\README.md:\n   95  \n   96  These can be recovered at serving time using utilities in [`signature.h`]\n   97: (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/session_bundle/signature.h)\n   98  \n   99  ~~~c++\n  ...\n  154  be less stable and/or auto-generated by TensorFlow.\n  155  \n  156: In [`signature.h`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/session_bundle/signature.h),\n  157  note that the generic signature methods BindGenericInputs and BindGenericNames\n  158  are doing simple string to string mapping as a convenience. These methods map\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\contrib\\session_bundle\\session_bundle.h:\n   22  \n   23  #include \"tensorflow/contrib/session_bundle/manifest.pb.h\"\n   24: #include \"tensorflow/contrib/session_bundle/signature.h\"\n   25  #include \"tensorflow/core/lib/core/status.h\"\n   26  #include \"tensorflow/core/lib/core/stringpiece.h\"\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\contrib\\session_bundle\\signature.cc:\n   14  ==============================================================================*/\n   15  \n   16: #include \"tensorflow/contrib/session_bundle/signature.h\"\n   17  \n   18  #include <string>\n\nC:\\dev\\vm\\data-share\\serving\\tf_models\\syntaxnet\\tensorflow\\tensorflow\\contrib\\session_bundle\\signature_test.cc:\n   14  ==============================================================================*/\n   15  \n   16: #include \"tensorflow/contrib/session_bundle/signature.h\"\n   17  \n   18  #include <memory>\n\n13 matches across 11 files\n",
			"settings":
			{
				"buffer_size": 837308,
				"line_ending": "Windows",
				"name": "Find Results",
				"scratch": true
			}
		},
		{
			"file": "tools/bazel.rc",
			"settings":
			{
				"buffer_size": 525,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/third_party/gpus/cuda_configure.bzl",
			"settings":
			{
				"buffer_size": 19840,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/core/manager.h",
			"settings":
			{
				"buffer_size": 5853,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/core/servable_data.h",
			"settings":
			{
				"buffer_size": 3578,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_session_bundle_config.proto",
			"settings":
			{
				"buffer_size": 2201,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_source_adapter.proto",
			"settings":
			{
				"buffer_size": 252,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/sources/storage_path/static_storage_path_source.cc",
			"settings":
			{
				"buffer_size": 1573,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/core/source_adapter.h",
			"settings":
			{
				"buffer_size": 9272,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/third_party/gpus/cuda/build_defs.bzl.tpl",
			"settings":
			{
				"buffer_size": 431,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "bazel-bin/tensorflow_serving/example/obj_detector_client --server=localhost:9000",
			"settings":
			{
				"buffer_size": 80,
				"line_ending": "Windows",
				"name": "bazel-bin/tensorflow_serving/example/obj_detector_"
			}
		},
		{
			"file": "/C/dev/public/libfrengine/build_py/bindings/py/setup.py",
			"settings":
			{
				"buffer_size": 1003,
				"line_ending": "Windows"
			}
		},
		{
			"file": "tensorflow/third_party/gpus/cuda/BUILD.tpl",
			"settings":
			{
				"buffer_size": 3176,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/g3doc/custom_servable.md",
			"settings":
			{
				"buffer_size": 5701,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/core/target.h",
			"settings":
			{
				"buffer_size": 6522,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/workspace.bzl",
			"settings":
			{
				"buffer_size": 7406,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/batching/batching_session.cc",
			"settings":
			{
				"buffer_size": 15812,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/batching/basic_batch_scheduler.h",
			"settings":
			{
				"buffer_size": 12376,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "/* Copyright 2016 Google Inc. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow_serving/session_bundle/signature.h\"\n\n#include <string>\n#include <utility>\n#include <vector>\n\n#include \"google/protobuf/any.pb.h\"\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/core/status.h\"\n#include \"tensorflow/core/lib/strings/strcat.h\"\n#include \"tensorflow/core/platform/protobuf.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/protobuf/meta_graph.pb.h\"\n#include \"tensorflow/core/protobuf/saver.pb.h\"\n#include \"tensorflow/core/public/session.h\"\n#include \"tensorflow_serving/session_bundle/manifest.pb.h\"\n\nnamespace tensorflow {\nnamespace serving {\nnamespace {\n\n// Returns OK if the input and output batch sizes match.\nStatus BatchSizesMatch(const Tensor& input, const Tensor& output) {\n  // Ensure the number of outputs match the number of inputs.\n  if (input.dim_size(0) != output.dim_size(0)) {\n    return errors::Internal(\n        strings::StrCat(\"Input batch size did not match output batch size: \",\n                        input.dim_size(0), \" vs. \", output.dim_size(0)));\n  }\n  return Status::OK();\n}\n}  // namespace\n\nStatus GetSignatures(const tensorflow::MetaGraphDef& meta_graph_def,\n                     Signatures* signatures) {\n  const auto& collection_def = meta_graph_def.collection_def();\n  const auto it = collection_def.find(kSignaturesKey);\n  if (it == collection_def.end() || it->second.any_list().value_size() != 1) {\n    return errors::FailedPrecondition(\n        strings::StrCat(\"Expected exactly one signatures proto in : \",\n                        meta_graph_def.DebugString()));\n  }\n  const auto& any = it->second.any_list().value(0);\n  if (!any.Is<Signatures>()) {\n    return errors::FailedPrecondition(\n        \"Expected signature Any type_url for: \",\n        signatures->descriptor()->full_name(), \". Got: \",\n        string(any.type_url().data(), any.type_url().size()), \".\");\n  }\n  if (!any.UnpackTo(signatures)) {\n    return errors::FailedPrecondition(\"Failed to unpack: \", any.DebugString());\n  }\n  return Status::OK();\n}\n\nStatus SetSignatures(const Signatures& signatures,\n                     tensorflow::MetaGraphDef* meta_graph_def) {\n  auto& collection_def = *(meta_graph_def->mutable_collection_def());\n  auto* any_list = collection_def[kSignaturesKey].mutable_any_list();\n  any_list->mutable_value()->Clear();\n  any_list->mutable_value()->Add()->PackFrom(signatures);\n  return Status::OK();\n}\n\nStatus GetClassificationSignature(\n    const tensorflow::MetaGraphDef& meta_graph_def,\n    ClassificationSignature* signature) {\n  Signatures signatures;\n  TF_RETURN_IF_ERROR(GetSignatures(meta_graph_def, &signatures));\n  if (!signatures.has_default_signature()) {\n    return errors::FailedPrecondition(strings::StrCat(\n        \"Expected a default signature in: \", signatures.DebugString()));\n  }\n  if (!signatures.default_signature().has_classification_signature()) {\n    return errors::FailedPrecondition(\n        strings::StrCat(\"Expected a classification signature in: \",\n                        signatures.default_signature().DebugString()));\n  }\n  *signature = signatures.default_signature().classification_signature();\n  return Status::OK();\n}\n\nStatus GetNamedClassificationSignature(\n    const string& name, const tensorflow::MetaGraphDef& meta_graph_def,\n    ClassificationSignature* signature) {\n  Signatures signatures;\n  TF_RETURN_IF_ERROR(GetSignatures(meta_graph_def, &signatures));\n  const auto& it = signatures.named_signatures().find(name);\n  if (it == signatures.named_signatures().end()) {\n    return errors::NotFound(strings::StrCat(\"Missing signature named \\\"\", name,\n                                            \"\\\" in: \",\n                                            signatures.DebugString()));\n  }\n  if (!it->second.has_classification_signature()) {\n    return errors::FailedPrecondition(\n        strings::StrCat(\"Expected a classification signature for name \\\"\", name,\n                        \"\\\" in: \", it->second.DebugString()));\n  }\n  *signature = it->second.classification_signature();\n  return Status::OK();\n}\n\nStatus RunClassification(const ClassificationSignature& signature,\n                         const Tensor& input, Session* session, Tensor* classes,\n                         Tensor* scores) {\n  std::vector<string> output_tensor_names;\n  if (classes) {\n    output_tensor_names.push_back(signature.classes().tensor_name());\n  }\n  if (scores) {\n    output_tensor_names.push_back(signature.scores().tensor_name());\n  }\n  // Run the graph with our inputs and outputs.\n  std::vector<Tensor> outputs;\n  const Status run_status =\n      session->Run({{signature.input().tensor_name(), input}},\n                   output_tensor_names, {}, &outputs);\n  if (!run_status.ok()) {\n    return run_status;\n  }\n  // Ensure the output is shaped how we expect.\n  // There should be one string Tensor of shape,\n  //   [batch_size, num_recommendations].\n  if (outputs.size() != output_tensor_names.size()) {\n    return errors::Internal(\n        strings::StrCat(\"Expected \", output_tensor_names.size(),\n                        \" output tensor(s).  Got: \", outputs.size()));\n  }\n  if (classes) {\n    *classes = outputs[0];\n    TF_RETURN_IF_ERROR(BatchSizesMatch(input, *classes));\n  }\n  if (scores) {\n    *scores = outputs[classes ? 1 : 0];\n    TF_RETURN_IF_ERROR(BatchSizesMatch(input, *scores));\n  }\n  return Status::OK();\n}\n\nStatus GetRegressionSignature(const tensorflow::MetaGraphDef& meta_graph_def,\n                              RegressionSignature* signature) {\n  Signatures signatures;\n  TF_RETURN_IF_ERROR(GetSignatures(meta_graph_def, &signatures));\n  if (!signatures.has_default_signature()) {\n    return errors::FailedPrecondition(strings::StrCat(\n        \"Expected a default signature in: \", signatures.DebugString()));\n  }\n  if (!signatures.default_signature().has_regression_signature()) {\n    return errors::FailedPrecondition(\n        strings::StrCat(\"Expected a regression signature in: \",\n                        signatures.default_signature().DebugString()));\n  }\n  *signature = signatures.default_signature().regression_signature();\n  return Status::OK();\n}\n\nStatus RunRegression(const RegressionSignature& signature,\n                     const Tensor& regression_input, Session* session,\n                     Tensor* regression_output) {\n  std::vector<string> output_tensor_names;\n  if (regression_output) {\n    output_tensor_names.push_back(signature.output().tensor_name());\n  }\n  // Run the graph with our inputs and outputs.\n  std::vector<Tensor> outputs;\n  const Status run_status =\n      session->Run({{signature.input().tensor_name(), regression_input}},\n                   output_tensor_names, {}, &outputs);\n  if (!run_status.ok()) {\n    return run_status;\n  }\n  // Ensure the regression score output is shaped how we expect.\n  // There should be one float Tensor of shape,\n  //   [batch_size, num_recommendations].\n  if (outputs.size() != output_tensor_names.size()) {\n    return errors::Internal(\n        strings::StrCat(\"Expected \", output_tensor_names.size(),\n                        \" output tensor(s).  Got: \", outputs.size()));\n  }\n  if (regression_output) {\n    *regression_output = outputs[0];\n    TF_RETURN_IF_ERROR(BatchSizesMatch(regression_input, *regression_output));\n  }\n  return Status::OK();\n}\n\nStatus GetGenericSignature(const string& name,\n                           const tensorflow::MetaGraphDef& meta_graph_def,\n                           GenericSignature* signature) {\n  Signatures signatures;\n  TF_RETURN_IF_ERROR(GetSignatures(meta_graph_def, &signatures));\n  const auto& it = signatures.named_signatures().find(name);\n  if (it == signatures.named_signatures().end()) {\n    return errors::InvalidArgument(\n        strings::StrCat(\"Missing generic signature named \\\"\", name, \"\\\" in \",\n                        signatures.DebugString()));\n  }\n  if (!it->second.has_generic_signature()) {\n    return errors::InvalidArgument(strings::StrCat(\n        \"Expected a generic signature: \", it->second.DebugString()));\n  }\n  *signature = it->second.generic_signature();\n  return Status::OK();\n}\n\nStatus GetDefaultSignature(const tensorflow::MetaGraphDef& meta_graph_def,\n                           Signature* default_signature) {\n  Signatures signatures;\n  TF_RETURN_IF_ERROR(GetSignatures(meta_graph_def, &signatures));\n  *default_signature = signatures.default_signature();\n  return Status::OK();\n}\n\nStatus GetNamedSignature(const string& name,\n                         const tensorflow::MetaGraphDef& meta_graph_def,\n                         Signature* signature) {\n  Signatures signatures;\n  TF_RETURN_IF_ERROR(GetSignatures(meta_graph_def, &signatures));\n  const auto& it = signatures.named_signatures().find(name);\n  if (it == signatures.named_signatures().end()) {\n    return errors::NotFound(strings::StrCat(\"Missing signature named \\\"\", name,\n                                            \"\\\" in: \",\n                                            signatures.DebugString()));\n  }\n  *signature = it->second;\n  return Status::OK();\n}\n\nStatus BindGenericInputs(const GenericSignature& signature,\n                         const std::vector<std::pair<string, Tensor>>& inputs,\n                         std::vector<std::pair<string, Tensor>>* bound_inputs) {\n  const protobuf::Map<string, serving::TensorBinding>& bindings =\n      signature.map();\n\n  for (const auto& entry : inputs) {\n    const auto mapped = bindings.find(entry.first);\n    if (mapped == bindings.end()) {\n      return errors::NotFound(\n          strings::StrCat(\"Could not find generic binding for: \", entry.first));\n    }\n    bound_inputs->push_back({mapped->second.tensor_name(), entry.second});\n  }\n  return Status::OK();\n}\n\nStatus BindGenericNames(const GenericSignature& signature,\n                        const std::vector<string>& input_names,\n                        std::vector<string>* bound_names) {\n  const protobuf::Map<string, serving::TensorBinding>& bindings =\n      signature.map();\n\n  for (const string& entry : input_names) {\n    const auto mapped = bindings.find(entry);\n    if (mapped == bindings.end()) {\n      return errors::NotFound(\n          strings::StrCat(\"Could not find generic binding for: \", entry));\n    }\n    bound_names->push_back(mapped->second.tensor_name());\n  }\n  return Status::OK();\n}\n\n}  // namespace serving\n}  // namespace tensorflow\n",
			"file": "tensorflow_serving/session_bundle/signature.cc",
			"file_size": 11016,
			"file_write_time": 131135854168871394,
			"settings":
			{
				"buffer_size": 11016,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/core/framework/tensor.cc",
			"settings":
			{
				"buffer_size": 29776,
				"line_ending": "Unix"
			}
		},
		{
			"file": "/C/dev/vm/data-share/caffe-ssd/src/caffe/layers/detection_output_layer.cpp",
			"settings":
			{
				"buffer_size": 18226,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/core/framework/tensor_shape.cc",
			"settings":
			{
				"buffer_size": 11483,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/core/framework/tensor_shape.proto",
			"settings":
			{
				"buffer_size": 1556,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Tests for session_bundle.py.\"\"\"\n\nimport os.path\n# This is a placeholder for a Google-internal import.\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.session_bundle import manifest_pb2\nfrom tensorflow.python.platform import flags\nfrom tensorflow_serving.session_bundle import constants\nfrom tensorflow_serving.session_bundle import session_bundle\n\nFLAGS = flags.FLAGS\n\n\ndef ContribTestSrcDirPath(relative_path):\n  \"\"\"Creates an absolute test srcdir path given a path relative to\n     tensorflow/contrib.\n\n  Args:\n    relative_path: a path relative to tensorflow/contrib\n      e.g. \"session_bundle/example\".\n\n  Returns:\n    An absolute path to the linked in runfiles given the relative path.\n  \"\"\"\n  return os.path.join(os.environ['TEST_SRCDIR'],\n                      \"tf_serving/external/org_tensorflow/tensorflow/contrib\", relative_path)\n\n\nclass SessionBundleLoadTest(tf.test.TestCase):\n\n  def testBasic(self):\n    base_path = ContribTestSrcDirPath(\n        \"session_bundle/example/half_plus_two/00000123\")\n    tf.reset_default_graph()\n    sess, meta_graph_def = session_bundle.LoadSessionBundleFromPath(\n        base_path, target=\"\", config=tf.ConfigProto(device_count={\"CPU\": 2}))\n\n    self.assertTrue(sess)\n    asset_path = os.path.join(base_path, constants.ASSETS_DIRECTORY)\n    with sess.as_default():\n      path1, path2 = sess.run([\"filename1:0\", \"filename2:0\"])\n      self.assertEqual(os.path.join(asset_path, \"hello1.txt\"), path1)\n      self.assertEqual(os.path.join(asset_path, \"hello2.txt\"), path2)\n\n      collection_def = meta_graph_def.collection_def\n\n      signatures_any = collection_def[constants.SIGNATURES_KEY].any_list.value\n      self.assertEquals(len(signatures_any), 1)\n\n      signatures = manifest_pb2.Signatures()\n      signatures_any[0].Unpack(signatures)\n      default_signature = signatures.default_signature\n      input_name = default_signature.regression_signature.input.tensor_name\n      output_name = default_signature.regression_signature.output.tensor_name\n      y = sess.run([output_name], {input_name: np.array([[0], [1], [2], [3]])})\n      # The operation is y = 0.5 * x + 2\n      self.assertEqual(y[0][0], 2)\n      self.assertEqual(y[0][1], 2.5)\n      self.assertEqual(y[0][2], 3)\n      self.assertEqual(y[0][3], 3.5)\n\n  def testBadPath(self):\n    base_path = ContribTestSrcDirPath(\"/no/such/a/dir\")\n    tf.reset_default_graph()\n    with self.assertRaises(RuntimeError) as cm:\n      _, _ = session_bundle.LoadSessionBundleFromPath(\n          base_path, target=\"local\",\n          config=tf.ConfigProto(device_count={\"CPU\": 2}))\n    self.assertTrue(\"Expected meta graph file missing\" in cm.exception.message)\n\nif __name__ == \"__main__\":\n  tf.test.main()\n",
			"file": "tensorflow_serving/session_bundle/session_bundle_test.py",
			"file_size": 3390,
			"file_write_time": 131135854168861393,
			"settings":
			{
				"buffer_size": 3390,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/core/platform/env.h",
			"settings":
			{
				"buffer_size": 16462,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n\"\"\"Blob helper functions.\"\"\"\n\nimport numpy as np\nimport cv2\nfrom six.moves import range\n\n\ndef im_list_to_blob(ims):\n    \"\"\"Convert a list of images into a network input.\n\n    Assumes images are already prepared (means subtracted, BGR order, ...).\n    \"\"\"\n    max_shape = np.array([im.shape for im in ims]).max(axis=0)\n    num_images = len(ims)\n    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                    dtype=np.float32)\n    for i in range(num_images):\n        im = ims[i]\n        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n    # Move channels (axis 3) to axis 1\n    # Axis order will become: (batch elem, channel, height, width)\n    channel_swap = (0, 3, 1, 2)\n    blob = blob.transpose(channel_swap)\n    return blob\n\n\ndef prep_im_for_blob(im, pixel_means, target_size, max_size):\n    \"\"\"Mean subtract and scale an image for use in a blob.\"\"\"\n    im = im.astype(np.float32, copy=False)\n    im -= pixel_means\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n    im_scale = float(target_size) / float(im_size_min)\n    # Prevent the biggest axis from being more than MAX_SIZE\n    if np.round(im_scale * im_size_max) > max_size:\n        im_scale = float(max_size) / float(im_size_max)\n    im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                    interpolation=cv2.INTER_LINEAR)\n\n    return im, im_scale\n",
			"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/utils/blob.py",
			"file_size": 1654,
			"file_write_time": 131146832770000000,
			"settings":
			{
				"buffer_size": 1654,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n\"\"\"Compute minibatch blobs for training a Fast R-CNN network.\"\"\"\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\nfrom fast_rcnn.config import cfg\nfrom utils.blob import prep_im_for_blob, im_list_to_blob\nfrom six.moves import range\n\n\ndef get_minibatch(roidb, num_classes):\n    \"\"\"Given a roidb, construct a minibatch sampled from it.\"\"\"\n    num_images = len(roidb)\n    # Sample random scales to use for each image in this batch\n    random_scale_inds = npr.randint(\n        0, high=len(cfg.TRAIN.SCALES),\n        size=num_images\n    )\n\n    assert (cfg.TRAIN.BATCH_SIZE % num_images == 0), 'num_images ({}) must divide BATCH_SIZE ({})'.format(\n            num_images, cfg.TRAIN.BATCH_SIZE\n        )\n\n    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n    fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)\n\n    # Get the input image blob, formatted for caffe\n    im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)\n\n    blobs = {'data': im_blob}\n\n    if cfg.TRAIN.HAS_RPN:\n        assert len(im_scales) == 1, \"Single batch only\"\n        assert len(roidb) == 1, \"Single batch only\"\n\n        # gt boxes: (x1, y1, x2, y2, cls)\n        gt_inds = np.where(roidb[0]['gt_classes'] != 0)[0]\n        gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n        gt_boxes[:, 0:4] = roidb[0]['boxes'][gt_inds, :] * im_scales[0]\n        gt_boxes[:, 4] = roidb[0]['gt_classes'][gt_inds]\n        blobs['gt_boxes'] = gt_boxes\n        blobs['im_info'] = np.array(\n            [[im_blob.shape[2], im_blob.shape[3], im_scales[0]]],\n            dtype=np.float32)\n    else:\n        # not using RPN\n        # Now, build the region of interest and label blobs\n        rois_blob = np.zeros((0, 5), dtype=np.float32)\n        labels_blob = np.zeros((0), dtype=np.float32)\n        bbox_targets_blob = np.zeros((0, 4 * num_classes), dtype=np.float32)\n        bbox_inside_blob = np.zeros(bbox_targets_blob.shape, dtype=np.float32)\n\n        # all_overlaps = []\n        for im_i in range(num_images):\n            labels, overlaps, im_rois, bbox_targets, bbox_inside_weights \\\n                = _sample_rois(roidb[im_i], fg_rois_per_image, rois_per_image,\n                               num_classes)\n\n            # Add to RoIs blob\n            rois = _project_im_rois(im_rois, im_scales[im_i])\n            batch_ind = im_i * np.ones((rois.shape[0], 1))\n            rois_blob_this_image = np.hstack((batch_ind, rois))\n            rois_blob = np.vstack((rois_blob, rois_blob_this_image))\n\n            # Add to labels, bbox targets, and bbox loss blobs\n            labels_blob = np.hstack((labels_blob, labels))\n            bbox_targets_blob = np.vstack((bbox_targets_blob, bbox_targets))\n            bbox_inside_blob = np.vstack((bbox_inside_blob, bbox_inside_weights))\n            # all_overlaps = np.hstack((all_overlaps, overlaps))\n\n        # For debug visualizations\n        # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps)\n\n        blobs['rois'] = rois_blob\n        blobs['labels'] = labels_blob\n\n        if cfg.TRAIN.BBOX_REG:\n            blobs['bbox_targets'] = bbox_targets_blob\n            blobs['bbox_inside_weights'] = bbox_inside_blob\n            blobs['bbox_outside_weights'] = np.array(bbox_inside_blob > 0).astype(np.float32)\n\n    return blobs\n\n\ndef _sample_rois(roidb, fg_rois_per_image, rois_per_image, num_classes):\n    \"\"\"\n    Generate a random sample of RoIs comprising foreground and background\n    examples.\n    \"\"\"\n    # label = class RoI has max overlap with\n    labels = roidb['max_classes']\n    overlaps = roidb['max_overlaps']\n    rois = roidb['boxes']\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(overlaps >= cfg.TRAIN.FG_THRESH)[0]\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(\n            fg_inds, size=fg_rois_per_this_image, replace=False\n        )\n\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                       (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = np.minimum(bg_rois_per_this_image,\n                                        bg_inds.size)\n    # Sample foreground regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(\n            bg_inds, size=bg_rois_per_this_image, replace=False\n        )\n\n    # The indices that we're selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n    # Clamp labels for the background RoIs to 0\n    labels[fg_rois_per_this_image:] = 0\n    overlaps = overlaps[keep_inds]\n    rois = rois[keep_inds]\n\n    bbox_targets, bbox_inside_weights = _get_bbox_regression_labels(\n        roidb['bbox_targets'][keep_inds, :], num_classes\n    )\n\n    return labels, overlaps, rois, bbox_targets, bbox_inside_weights\n\n\ndef _get_image_blob(roidb, scale_inds):\n    \"\"\"\n    Builds an input blob from the images in the roidb at the specified\n    scales.\n    \"\"\"\n    num_images = len(roidb)\n    processed_ims = []\n    im_scales = []\n    for i in range(num_images):\n        im = cv2.imread(roidb[i]['image'])\n        if roidb[i]['flipped']:\n            im = im[:, ::-1, :]\n        target_size = cfg.TRAIN.SCALES[scale_inds[i]]\n        im, im_scale = prep_im_for_blob(im, cfg.PIXEL_MEANS, target_size,\n                                        cfg.TRAIN.MAX_SIZE)\n        im_scales.append(im_scale)\n        processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n\n\ndef _project_im_rois(im_rois, im_scale_factor):\n    \"\"\"\n    Project image RoIs into the rescaled training image.\n    \"\"\"\n    rois = im_rois * im_scale_factor\n    return rois\n\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    \"\"\"Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        bbox_target_data (ndarray): N x 4K blob of regression targets\n        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n    \"\"\"\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS\n\n    return bbox_targets, bbox_inside_weights\n\n\ndef _vis_minibatch(im_blob, rois_blob, labels_blob, overlaps):\n    \"\"\"\n    Visualize a mini-batch for debugging.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    for i in range(rois_blob.shape[0]):\n        rois = rois_blob[i, :]\n        im_ind = rois[0]\n        roi = rois[1:]\n        im = im_blob[im_ind, :, :, :].transpose((1, 2, 0)).copy()\n        im += cfg.PIXEL_MEANS\n        im = im[:, :, (2, 1, 0)]\n        im = im.astype(np.uint8)\n        cls = labels_blob[i]\n        plt.imshow(im)\n        print('class: ', cls, ' overlap: ', overlaps[i])\n        plt.gca().add_patch(\n            plt.Rectangle((roi[0], roi[1]), roi[2] - roi[0],\n                          roi[3] - roi[1], fill=False,\n                          edgecolor='r', linewidth=3)\n            )\n        plt.show()\n",
			"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/roi_data_layer/minibatch.py",
			"file_size": 8225,
			"file_write_time": 131146832770000000,
			"settings":
			{
				"buffer_size": 8225,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/core/framework/types.cc",
			"settings":
			{
				"buffer_size": 8129,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/core/common_runtime/threadpool_device.cc",
			"settings":
			{
				"buffer_size": 2789,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/common_runtime/device.h\"\n\n#include \"tensorflow/core/framework/op_segment.h\"\n#include \"tensorflow/core/lib/core/errors.h\"\n#include \"tensorflow/core/lib/random/random.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/types.h\"\n\nnamespace tensorflow {\n\nDevice::Device(Env* env, const DeviceAttributes& device_attributes,\n               Allocator* device_allocator)\n    : DeviceBase(env), device_attributes_(device_attributes) {\n  CHECK(DeviceNameUtils::ParseFullName(name(), &parsed_name_))\n      << \"Invalid device name: \" << name();\n  rmgr_ = new ResourceMgr(parsed_name_.job);\n}\n\nDevice::~Device() { delete rmgr_; }\n\n// static\nDeviceAttributes Device::BuildDeviceAttributes(\n    const string& name, DeviceType device, Bytes memory_limit,\n    BusAdjacency bus_adjacency, const string& physical_device_desc) {\n  DeviceAttributes da;\n  da.set_name(name);\n  do {\n    da.set_incarnation(random::New64());\n  } while (da.incarnation() == 0);  // This proto field must not be zero\n  da.set_device_type(device.type());\n  da.set_memory_limit(memory_limit.value());\n  da.set_bus_adjacency(bus_adjacency);\n  da.set_physical_device_desc(physical_device_desc);\n  return da;\n}\n\n}  // namespace tensorflow\n",
			"file": "tensorflow/tensorflow/core/common_runtime/device.cc",
			"file_size": 1919,
			"file_write_time": 131188499262024260,
			"settings":
			{
				"buffer_size": 1919,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/contrib/session_bundle/manifest.proto",
			"settings":
			{
				"buffer_size": 2426,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "im_info shape (1, 3)\ndata shape (1, 3, 224, 373)\nim_info shape (1, 3)\ndata shape (1, 3, 224, 373)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nDemo for data/demo/000456.jpg\nim_info shape (1, 3)\ndata shape (1, 3, 224, 299)\nDetection took 1.361s for 300 object proposals\n bus\n  > score: 0.989\n  > bbox: [  84.61488342   34.5355072   437.61376953  305.57861328]\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nDemo for data/demo/000542.jpg\nim_info shape (1, 3)\ndata shape (1, 3, 224, 299)\nDetection took 1.391s for 300 object proposals\n cat\n  > score: 0.993\n  > bbox: [   0.    0.  499.  374.]\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nDemo for data/demo/001150.jpg\nim_info shape (1, 3)\ndata shape (1, 3, 224, 299)\nDetection took 1.368s for 292 object proposals\n dog\n  > score: 0.980\n  > bbox: [ 161.70223999  104.64179993  393.76809692  355.92376709]\n\n person\n  > score: 0.980\n  > bbox: [   3.62555695    0.          254.68875122  273.04345703]\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nDemo for data/demo/001763.jpg\nim_info shape (1, 3)\ndata shape (1, 3, 224, 299)\nDetection took 1.349s for 300 object proposals\n cat\n  > score: 0.997\n  > bbox: [ 254.9026947    92.76559448  491.66998291  369.78219604]\n\n dog\n  > score: 0.999\n  > bbox: [   3.45184326   23.93199158  319.92108154  374.        ]\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nDemo for data/demo/004545.jpg\nim_info shape (1, 3)\ndata shape (1, 3, 224, 299)\nDetection took 1.406s for 300 object proposals\n car\n  > score: 0.997\n  > bbox: [  18.17498779   97.83976746  141.99404907  189.58720398]\n\n horse\n  > score: 0.990\n  > bbox: [ 200.16259766   73.72839355  394.82385254  353.94198608]\n\n person\n  > score: 0.981\n  > bbox: [ 263.42608643   19.57826996  365.13867188  241.08154297]\n\n\n#########################\n\nim_info shape (1, 3)\ndata shape (1, 3, 600, 1000)\nim_info shape (1, 3)\ndata shape (1, 3, 600, 1000)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nDemo for data/demo/000456.jpg\nim_info shape (1, 3)\ndata shape (1, 3, 600, 800)\nDetection took 6.191s for 300 object proposals\n bus\n  > score: 0.998\n  > bbox: [  80.17433167   46.18685913  422.16174316  298.32351685]\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nDemo for data/demo/000542.jpg\nim_info shape (1, 3)\ndata shape (1, 3, 600, 800)\nDetection took 5.804s for 161 object proposals\n cat\n  > score: 0.999\n  > bbox: [   0.            0.          499.          348.80938721]\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nDemo for data/demo/001150.jpg\nim_info shape (1, 3)\ndata shape (1, 3, 600, 800)\nDetection took 5.793s for 194 object proposals\n dog\n  > score: 0.996\n  > bbox: [ 162.17889404   93.5065918   413.6138916   368.32788086]\n\n person\n  > score: 0.985\n  > bbox: [  23.62877655    0.          251.90863037  201.32220459]\n\n person\n  > score: 0.890\n  > bbox: [ 101.51681519    0.          462.82901001  329.86102295]\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nDemo for data/demo/001763.jpg\nim_info shape (1, 3)\ndata shape (1, 3, 600, 800)\nDetection took 5.821s for 196 object proposals\n cat\n  > score: 0.998\n  > bbox: [ 261.81704712   96.73332214  499.          374.        ]\n\n dog\n  > score: 0.999\n  > bbox: [   7.59580994   26.43634033  318.84649658  350.97781372]\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nDemo for data/demo/004545.jpg\nim_info shape (1, 3)\ndata shape (1, 3, 600, 800)\nDetection took 5.975s for 300 object proposals\n car\n  > score: 1.000\n  > bbox: [   0.          104.04174805  142.41122437  194.11087036]\n\n dog\n  > score: 0.957\n  > bbox: [ 134.68927002  197.69909668  202.42739868  351.86767578]\n\n horse\n  > score: 0.999\n  > bbox: [ 222.1492157    57.5912323   410.86566162  356.38916016]\n\n person\n  > score: 0.999\n  > bbox: [ 259.57110596    8.92913055  349.45922852  231.13201904]\n\n person\n  > score: 0.988\n  > bbox: [ 426.96099854  122.46759033  452.40289307  175.02606201]\n\n",
			"settings":
			{
				"buffer_size": 3747,
				"line_ending": "Windows",
				"name": "im_info shape (1, 3)"
			}
		},
		{
			"file": "tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc",
			"settings":
			{
				"buffer_size": 10268,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_simple_servers_test.cc",
			"settings":
			{
				"buffer_size": 2374,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "# Macros for building CUDA code.\n\ndef if_cuda(if_true, if_false = []):\n    \"\"\"Shorthand for select()'ing on whether we're building with CUDA.\n\n    Returns a select statement which evaluates to if_true if we're building\n    with CUDA enabled.  Otherwise, the select statement evaluates to if_false.\n\n    \"\"\"\n    return select({\n        \"//third_party/gpus/cuda:using_nvcc\": if_true,\n        \"//third_party/gpus/cuda:using_gcudacc\": if_true,\n        \"//conditions:default\": if_false\n    })\n",
			"file": "tensorflow/third_party/gpus/cuda/build_defs.bzl",
			"file_size": 488,
			"file_write_time": 131099595732778827,
			"settings":
			{
				"buffer_size": 488,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "/* Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/platform/test.h\"\n\n#include <cstdlib>\n#include <unordered_set>\n\n#include <netinet/in.h>\n#include <signal.h>\n#include <sys/socket.h>\n#include <sys/types.h>\n#include <unistd.h>\n\n#include \"tensorflow/core/lib/strings/strcat.h\"\n#include \"tensorflow/core/platform/logging.h\"\n\nnamespace tensorflow {\nnamespace testing {\n\nnamespace {\nclass PosixSubProcess : public SubProcess {\n public:\n  PosixSubProcess(const std::vector<string>& argv) : argv_(argv), pid_(0) {}\n\n  ~PosixSubProcess() override {}\n\n  bool Start() override {\n    if (pid_ != 0) {\n      LOG(ERROR) << \"Tried to start process multiple times.\";\n      return false;\n    }\n    pid_ = fork();\n    if (pid_ == 0) {\n      // We are in the child process.\n      const char* path = argv_[0].c_str();\n      const char** argv = new const char*[argv_.size() + 1];\n      int i = 0;\n      for (const string& arg : argv_) {\n        argv[i++] = arg.c_str();\n      }\n      argv[argv_.size()] = nullptr;\n      execv(path, (char* const*)argv);\n      // Never executes.\n      return true;\n    } else if (pid_ < 0) {\n      LOG(ERROR) << \"Failed to fork process.\";\n      return false;\n    } else {\n      // We are in the parent process and fork() was successful.\n      // TODO(mrry): Consider collecting stderr from the child.\n      return true;\n    }\n  }\n\n  bool Kill(int signal) override {\n    if (pid_ == 0) {\n      LOG(ERROR) << \"Tried to kill process before starting it.\";\n      return false;\n    }\n    return kill(pid_, signal) == 0;\n  }\n\n private:\n  const std::vector<string> argv_;\n  pid_t pid_;\n  TF_DISALLOW_COPY_AND_ASSIGN(PosixSubProcess);\n};\n}  // namespace\n\nstd::unique_ptr<SubProcess> CreateSubProcess(const std::vector<string>& argv) {\n  return std::unique_ptr<SubProcess>(new PosixSubProcess(argv));\n}\n\nnamespace {\nbool IsPortAvailable(int* port, bool is_tcp) {\n  const int protocol = is_tcp ? IPPROTO_TCP : 0;\n  const int fd = socket(AF_INET, is_tcp ? SOCK_STREAM : SOCK_DGRAM, protocol);\n\n  struct sockaddr_in addr;\n  socklen_t addr_len = sizeof(addr);\n  int actual_port;\n\n  CHECK_GE(*port, 0);\n  CHECK_LE(*port, 65535);\n  if (fd < 0) {\n    LOG(ERROR) << \"socket() failed: \" << strerror(errno);\n    return false;\n  }\n\n  // SO_REUSEADDR lets us start up a server immediately after it exists.\n  int one = 1;\n  if (setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, &one, sizeof(one)) < 0) {\n    LOG(ERROR) << \"setsockopt() failed: \" << strerror(errno);\n    close(fd);\n    return false;\n  }\n\n  // Try binding to port.\n  addr.sin_family = AF_INET;\n  addr.sin_addr.s_addr = INADDR_ANY;\n  addr.sin_port = htons((uint16_t)*port);\n  if (bind(fd, (struct sockaddr*)&addr, sizeof(addr)) < 0) {\n    LOG(WARNING) << \"bind(port=\" << *port << \") failed: \" << strerror(errno);\n    close(fd);\n    return false;\n  }\n\n  // Get the bound port number.\n  if (getsockname(fd, (struct sockaddr*)&addr, &addr_len) < 0) {\n    LOG(WARNING) << \"getsockname() failed: \" << strerror(errno);\n    close(fd);\n    return false;\n  }\n  CHECK_LE(addr_len, sizeof(addr));\n  actual_port = ntohs(addr.sin_port);\n  CHECK_GT(actual_port, 0);\n  if (*port == 0) {\n    *port = actual_port;\n  } else {\n    CHECK_EQ(*port, actual_port);\n  }\n  close(fd);\n  return true;\n}\n\nconst int kNumRandomPortsToPick = 100;\nconst int kMaximumTrials = 1000;\n\n}  // namespace\n\nint PickUnusedPortOrDie() {\n  static std::unordered_set<int> chosen_ports;\n\n  // Type of port to first pick in the next iteration.\n  bool is_tcp = true;\n  int trial = 0;\n  while (true) {\n    int port;\n    trial++;\n    CHECK_LE(trial, kMaximumTrials)\n        << \"Failed to pick an unused port for testing.\";\n    if (trial == 1) {\n      port = getpid() % (65536 - 30000) + 30000;\n    } else if (trial <= kNumRandomPortsToPick) {\n      port = rand() % (65536 - 30000) + 30000;\n    } else {\n      port = 0;\n    }\n\n    if (chosen_ports.find(port) != chosen_ports.end()) {\n      continue;\n    }\n    if (!IsPortAvailable(&port, is_tcp)) {\n      continue;\n    }\n\n    CHECK_GT(port, 0);\n    if (!IsPortAvailable(&port, !is_tcp)) {\n      is_tcp = !is_tcp;\n      continue;\n    }\n\n    chosen_ports.insert(port);\n    return port;\n  }\n\n  return 0;\n}\n\nstring TensorFlowSrcRoot() {\n  // 'bazel test' sets TEST_SRCDIR, and also TEST_WORKSPACE if a new\n  // enough version of bazel is used.\n  const char* env = getenv(\"TEST_SRCDIR\");\n  const char* workspace = getenv(\"TEST_WORKSPACE\");\n  if (env && env[0] != '\\0') {\n    if (workspace && workspace[0] != '\\0') {\n      return strings::StrCat(env, \"/\", workspace, \"/tensorflow\");\n    } else {\n      return strings::StrCat(env, \"/tensorflow\");\n    }\n  } else {\n    LOG(WARNING) << \"TEST_SRCDIR environment variable not set: \"\n                 << \"using $PWD/tensorflow as TensorFlowSrcRoot() for tests.\";\n    return \"tensorflow\";\n  }\n}\n\n}  // namespace testing\n}  // namespace tensorflow\n",
			"file": "tensorflow/tensorflow/core/platform/posix/test.cc",
			"file_size": 3107,
			"file_write_time": 131188499288787658,
			"settings":
			{
				"buffer_size": 5472,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "/* Copyright 2016 Google Inc. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/contrib/session_bundle/test_util.h\"\n\n#include \"tensorflow/core/lib/io/path.h\"\n#include \"tensorflow/core/platform/env.h\"\n#include \"tensorflow/core/platform/test.h\"\n\nnamespace tensorflow {\nnamespace serving {\nnamespace test_util {\n\nstring TestSrcDirPath(const string& relative_path) {\n  const string base_path = tensorflow::testing::TensorFlowSrcRoot();\n  const string contrib_path = tensorflow::io::JoinPath(\n      tensorflow::testing::TensorFlowSrcRoot(), \"/contrib\");\n  return tensorflow::io::JoinPath(contrib_path, relative_path);\n}\n\n}  // namespace test_util\n}  // namespace serving\n}  // namespace tensorflow\n",
			"file": "tensorflow/tensorflow/contrib/session_bundle/test_util.cc",
			"file_size": 1304,
			"file_write_time": 131219627756989253,
			"settings":
			{
				"buffer_size": 1292,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tf_models/syntaxnet/syntaxnet/syntaxnet.bzl",
			"settings":
			{
				"buffer_size": 4462,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/third_party/gpus/cuda/BUILD",
			"settings":
			{
				"buffer_size": 0,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#include \"tensorflow/core/public/tensor_c_api.h\"\n\n#include <vector>\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/platform/test.h\"\n\nusing tensorflow::Tensor;\nusing tensorflow::TensorShape;\n\nnamespace tensorflow {\nbool TF_Tensor_DecodeStrings(TF_Tensor* src, Tensor* dst, TF_Status* status);\nTF_Tensor* TF_Tensor_EncodeStrings(const Tensor& src);\n}  // namespace tensorflow\n\nTEST(CApi, Status) {\n  TF_Status* s = TF_NewStatus();\n  EXPECT_EQ(TF_OK, TF_GetCode(s));\n  EXPECT_EQ(tensorflow::string(), TF_Message(s));\n  TF_SetStatus(s, TF_CANCELLED, \"cancel\");\n  EXPECT_EQ(TF_CANCELLED, TF_GetCode(s));\n  EXPECT_EQ(tensorflow::string(\"cancel\"), TF_Message(s));\n  TF_DeleteStatus(s);\n}\n\nstatic void Deallocator(void* data, size_t, void* arg) {\n  tensorflow::cpu_allocator()->DeallocateRaw(data);\n  *reinterpret_cast<bool*>(arg) = true;\n}\n\nTEST(CApi, Tensor) {\n  float* values =\n      reinterpret_cast<float*>(tensorflow::cpu_allocator()->AllocateRaw(\n          EIGEN_MAX_ALIGN_BYTES, 6 * sizeof(float)));\n  tensorflow::int64 dims[] = {2, 3};\n  bool deallocator_called = false;\n  TF_Tensor* t = TF_NewTensor(TF_FLOAT, dims, 2, values, sizeof(values),\n                              &Deallocator, &deallocator_called);\n  EXPECT_FALSE(deallocator_called);\n  EXPECT_EQ(TF_FLOAT, TF_TensorType(t));\n  EXPECT_EQ(2, TF_NumDims(t));\n  EXPECT_EQ(dims[0], TF_Dim(t, 0));\n  EXPECT_EQ(dims[1], TF_Dim(t, 1));\n  EXPECT_EQ(sizeof(values), TF_TensorByteSize(t));\n  EXPECT_EQ(static_cast<void*>(values), TF_TensorData(t));\n  TF_DeleteTensor(t);\n  EXPECT_TRUE(deallocator_called);\n}\n\nstatic void TestEncodeDecode(int line,\n                             const std::vector<tensorflow::string>& data) {\n  const tensorflow::int64 n = data.size();\n  for (std::vector<tensorflow::int64> dims :\n       std::vector<std::vector<tensorflow::int64>>{\n           {n}, {1, n}, {n, 1}, {n / 2, 2}}) {\n    // Create C++ Tensor\n    Tensor src(tensorflow::DT_STRING, TensorShape(dims));\n    for (tensorflow::int64 i = 0; i < src.NumElements(); i++) {\n      src.flat<tensorflow::string>()(i) = data[i];\n    }\n    TF_Tensor* dst = TF_Tensor_EncodeStrings(src);\n\n    // Convert back to a C++ Tensor and ensure we get expected output.\n    TF_Status* status = TF_NewStatus();\n    Tensor output;\n    ASSERT_TRUE(TF_Tensor_DecodeStrings(dst, &output, status)) << line;\n    ASSERT_EQ(TF_OK, TF_GetCode(status)) << line;\n    ASSERT_EQ(src.NumElements(), output.NumElements()) << line;\n    for (tensorflow::int64 i = 0; i < src.NumElements(); i++) {\n      ASSERT_EQ(data[i], output.flat<tensorflow::string>()(i)) << line;\n    }\n\n    TF_DeleteStatus(status);\n    TF_DeleteTensor(dst);\n  }\n}\n\nTEST(CApi, TensorEncodeDecodeStrings) {\n  TestEncodeDecode(__LINE__, {});\n  TestEncodeDecode(__LINE__, {\"hello\"});\n  TestEncodeDecode(__LINE__,\n                   {\"the\", \"quick\", \"brown\", \"fox\", \"jumped\", \"over\"});\n\n  tensorflow::string big(1000, 'a');\n  TestEncodeDecode(__LINE__, {\"small\", big, \"small2\"});\n}\n\nTEST(CApi, SessionOptions) {\n  TF_SessionOptions* opt = TF_NewSessionOptions();\n  TF_DeleteSessionOptions(opt);\n}\n\nTEST(CApi, SessionWithRunMetadata) {\n  TF_Status* s = TF_NewStatus();\n  TF_SessionOptions* opt = TF_NewSessionOptions();\n  TF_Session* session = TF_NewSession(opt, s);\n  TF_DeleteSessionOptions(opt);\n  ASSERT_EQ(TF_OK, TF_GetCode(s)) << TF_Message(s);\n\n  TF_Buffer* run_options = TF_NewBufferFromString(\"\", 0);\n  TF_Buffer* run_metadata = TF_NewBuffer();\n  TF_Run(session, run_options, nullptr, nullptr, 0, nullptr, nullptr, 0,\n         nullptr, 0, run_metadata, s);\n  EXPECT_EQ(TF_INVALID_ARGUMENT, TF_GetCode(s)) << TF_Message(s);\n  EXPECT_EQ(std::string(\"Session was not created with a graph before Run()!\"),\n            std::string(TF_Message(s)));\n  TF_DeleteBuffer(run_metadata);\n  TF_DeleteBuffer(run_options);\n\n  TF_DeleteSession(session, s);\n  ASSERT_EQ(TF_OK, TF_GetCode(s)) << TF_Message(s);\n\n  TF_DeleteStatus(s);\n}\n\n// TODO(jeff,sanjay): Session tests\n// . Extend graph\n// . Run\n",
			"file": "tensorflow/tensorflow/core/client/tensor_c_api_test.cc",
			"file_size": 4655,
			"file_write_time": 131135672533826673,
			"settings":
			{
				"buffer_size": 4655,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n==============================================================================*/\n\n#ifndef TENSORFLOW_FRAMEWORK_TENSOR_TESTUTIL_H_\n#define TENSORFLOW_FRAMEWORK_TENSOR_TESTUTIL_H_\n\n#include \"tensorflow/core/framework/tensor.h\"\n#include \"tensorflow/core/lib/gtl/array_slice.h\"\n#include \"tensorflow/core/platform/logging.h\"\n#include \"tensorflow/core/platform/test.h\"\n\nnamespace tensorflow {\nnamespace test {\n\n// Constructs a scalar tensor with 'val'.\ntemplate <typename T>\nTensor AsScalar(const T& val) {\n  Tensor ret(DataTypeToEnum<T>::value, {});\n  ret.scalar<T>()() = val;\n  return ret;\n}\n\n// Constructs a flat tensor with 'vals'.\ntemplate <typename T>\nTensor AsTensor(gtl::ArraySlice<T> vals) {\n  Tensor ret(DataTypeToEnum<T>::value, {static_cast<int64>(vals.size())});\n  std::copy_n(vals.data(), vals.size(), ret.flat<T>().data());\n  return ret;\n}\n\n// Constructs a tensor of \"shape\" with values \"vals\".\ntemplate <typename T>\nTensor AsTensor(gtl::ArraySlice<T> vals, const TensorShape& shape) {\n  Tensor ret;\n  CHECK(ret.CopyFrom(AsTensor(vals), shape));\n  return ret;\n}\n\n// Fills in '*tensor' with 'vals'. E.g.,\n//   Tensor x(&alloc, DT_FLOAT, TensorShape({2, 2}));\n//   test::FillValues<float>(&x, {11, 21, 21, 22});\ntemplate <typename T>\nvoid FillValues(Tensor* tensor, gtl::ArraySlice<T> vals) {\n  auto flat = tensor->flat<T>();\n  CHECK_EQ(flat.size(), vals.size());\n  if (flat.size() > 0) {\n    std::copy_n(vals.data(), vals.size(), flat.data());\n  }\n}\n\n// Fills in '*tensor' with 'vals', converting the types as needed.\ntemplate <typename T, typename SrcType>\nvoid FillValues(Tensor* tensor, std::initializer_list<SrcType> vals) {\n  auto flat = tensor->flat<T>();\n  CHECK_EQ(flat.size(), vals.size());\n  if (flat.size() > 0) {\n    size_t i = 0;\n    for (auto itr = vals.begin(); itr != vals.end(); ++itr, ++i) {\n      flat(i) = T(*itr);\n    }\n  }\n}\n\n// Fills in '*tensor' with a sequence of value of val, val+1, val+2, ...\n//   Tensor x(&alloc, DT_FLOAT, TensorShape({2, 2}));\n//   test::FillIota<float>(&x, 1.0);\ntemplate <typename T>\nvoid FillIota(Tensor* tensor, const T& val) {\n  auto flat = tensor->flat<T>();\n  std::iota(flat.data(), flat.data() + flat.size(), val);\n}\n\n// Fills in '*tensor' with a sequence of value of fn(0), fn(1), ...\n//   Tensor x(&alloc, DT_FLOAT, TensorShape({2, 2}));\n//   test::FillFn<float>(&x, [](int i)->float { return i*i; });\ntemplate <typename T>\nvoid FillFn(Tensor* tensor, std::function<T(int)> fn) {\n  auto flat = tensor->flat<T>();\n  for (int i = 0; i < flat.size(); ++i) flat(i) = fn(i);\n}\n\n// Expects \"x\" and \"y\" are tensors of the same type, same shape, and\n// identical values.\ntemplate <typename T>\nvoid ExpectTensorEqual(const Tensor& x, const Tensor& y);\n\n// Expects \"x\" and \"y\" are tensors of the same type, same shape, and\n// approximate equal values, each within \"abs_err\".\ntemplate <typename T>\nvoid ExpectTensorNear(const Tensor& x, const Tensor& y, const T& abs_err);\n\n// Expects \"x\" and \"y\" are tensors of the same type (float or double),\n// same shape and element-wise difference between x and y is no more\n// than atol + rtol * abs(x).\nvoid ExpectClose(const Tensor& x, const Tensor& y, double atol = 1e-6,\n                 double rtol = 1e-6);\n\n// Implementation details.\n\nnamespace internal {\n\ntemplate <typename T>\nstruct is_floating_point_type {\n  static const bool value = std::is_same<T, Eigen::half>::value ||\n                            std::is_same<T, float>::value ||\n                            std::is_same<T, double>::value ||\n                            std::is_same<T, std::complex<float> >::value ||\n                            std::is_same<T, std::complex<double> >::value;\n};\n\ntemplate <typename T>\ninline void ExpectEqual(const T& a, const T& b) {\n  EXPECT_EQ(a, b);\n}\n\ntemplate <>\ninline void ExpectEqual<float>(const float& a, const float& b) {\n  EXPECT_FLOAT_EQ(a, b);\n}\n\ntemplate <>\ninline void ExpectEqual<double>(const double& a, const double& b) {\n  EXPECT_DOUBLE_EQ(a, b);\n}\n\ntemplate <>\ninline void ExpectEqual<complex64>(const complex64& a, const complex64& b) {\n  EXPECT_FLOAT_EQ(a.real(), b.real()) << a << \" vs. \" << b;\n  EXPECT_FLOAT_EQ(a.imag(), b.imag()) << a << \" vs. \" << b;\n}\n\ntemplate <>\ninline void ExpectEqual<complex128>(const complex128& a, const complex128& b) {\n  EXPECT_DOUBLE_EQ(a.real(), b.real()) << a << \" vs. \" << b;\n  EXPECT_DOUBLE_EQ(a.imag(), b.imag()) << a << \" vs. \" << b;\n}\n\ninline void AssertSameTypeDims(const Tensor& x, const Tensor& y) {\n  ASSERT_EQ(x.dtype(), y.dtype());\n  ASSERT_TRUE(x.IsSameSize(y))\n      << \"x.shape [\" << x.shape().DebugString() << \"] vs \"\n      << \"y.shape [ \" << y.shape().DebugString() << \"]\";\n}\n\ntemplate <typename T, bool is_fp = is_floating_point_type<T>::value>\nstruct Expector;\n\ntemplate <typename T>\nstruct Expector<T, false> {\n  static void Equal(const T& a, const T& b) { ExpectEqual(a, b); }\n\n  static void Equal(const Tensor& x, const Tensor& y) {\n    ASSERT_EQ(x.dtype(), DataTypeToEnum<T>::v());\n    AssertSameTypeDims(x, y);\n    auto a = x.flat<T>();\n    auto b = y.flat<T>();\n    for (int i = 0; i < a.size(); ++i) {\n      ExpectEqual(a(i), b(i));\n    }\n  }\n};\n\n// Partial specialization for float and double.\ntemplate <typename T>\nstruct Expector<T, true> {\n  static void Equal(const T& a, const T& b) { ExpectEqual(a, b); }\n\n  static void Equal(const Tensor& x, const Tensor& y) {\n    ASSERT_EQ(x.dtype(), DataTypeToEnum<T>::v());\n    AssertSameTypeDims(x, y);\n    auto a = x.flat<T>();\n    auto b = y.flat<T>();\n    for (int i = 0; i < a.size(); ++i) {\n      ExpectEqual(a(i), b(i));\n    }\n  }\n\n  static void Near(const T& a, const T& b, const double abs_err) {\n    if (a != b) {  // Takes care of inf.\n      EXPECT_LE(double(Eigen::numext::abs(a - b)), abs_err) << \"a = \" << a\n                                                            << \" b = \" << b;\n    }\n  }\n\n  static void Near(const Tensor& x, const Tensor& y, const double abs_err) {\n    ASSERT_EQ(x.dtype(), DataTypeToEnum<T>::v());\n    AssertSameTypeDims(x, y);\n    auto a = x.flat<T>();\n    auto b = y.flat<T>();\n    for (int i = 0; i < a.size(); ++i) {\n      Near(a(i), b(i), abs_err);\n    }\n  }\n};\n\n}  // namespace internal\n\ntemplate <typename T>\nvoid ExpectTensorEqual(const Tensor& x, const Tensor& y) {\n  internal::Expector<T>::Equal(x, y);\n}\n\ntemplate <typename T>\nvoid ExpectTensorNear(const Tensor& x, const Tensor& y, const double abs_err) {\n  static_assert(internal::is_floating_point_type<T>::value,\n                \"T is not a floating point types.\");\n  internal::Expector<T>::Near(x, y, abs_err);\n}\n\n}  // namespace test\n}  // namespace tensorflow\n\n#endif  // TENSORFLOW_FRAMEWORK_TENSOR_TESTUTIL_H_\n",
			"file": "tensorflow/tensorflow/core/framework/tensor_testutil.h",
			"file_size": 7230,
			"file_write_time": 131188499267514957,
			"settings":
			{
				"buffer_size": 7230,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/core/framework/tensor_shape.h",
			"settings":
			{
				"buffer_size": 12978,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_source_adapter.cc",
			"settings":
			{
				"buffer_size": 2969,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_session_bundle_factory_test.cc",
			"settings":
			{
				"buffer_size": 7856,
				"line_ending": "Unix"
			}
		},
		{
			"file": "README.md",
			"settings":
			{
				"buffer_size": 9308,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_session_bundle_factory.cc",
			"settings":
			{
				"buffer_size": 7768,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_source_adapter_test.cc",
			"settings":
			{
				"buffer_size": 3833,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "",
			"file": "tensorflow_serving/servables/caffe/test_data/py_layers/00000001/weights.caffemodel",
			"file_size": 0,
			"file_write_time": 131193571244061802,
			"settings":
			{
				"buffer_size": 0,
				"line_ending": "Windows"
			}
		},
		{
			"contents": "name: 'pythonnet'\nforce_backward: true\n\ninput: 'data'\ninput_shape { dim: 10 dim: 9 dim: 8 }\n\nlayer { type: 'Python' name: 'one' bottom: 'data' top: 'one'\n  python_param { module: 'test_python_layer' layer: 'TimesTenLayer' } }\n\nlayer { type: 'Python' name: 'two' bottom: 'one' top: 'two'\n  python_param { module: 'test_python_layer' layer: 'TimesTenLayer' } }\n\nlayer { type: 'Python' name: 'three' bottom: 'two' top: 'three'\n  python_param { module: 'test_python_layer' layer: 'TimesTenLayer' } }\n",
			"file": "tensorflow_serving/servables/caffe/test_data/py_layers/00000001/deploy.prototxt",
			"file_size": 496,
			"file_write_time": 131193571243931786,
			"settings":
			{
				"buffer_size": 496,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "import caffe\n\nclass TimesTenLayer(caffe.Layer):\n    \"\"\"A layer that just multiplies by ten\"\"\"\n\n    def setup(self, bottom, top):\n      pass\n\n    def reshape(self, bottom, top):\n      top[0].reshape(*bottom[0].data.shape)\n\n    def forward(self, bottom, top):\n      top[0].data[...] = 10 * bottom[0].data\n\n    def backward(self, top, propagate_down, bottom):\n      bottom[0].diff[...] = 10 * top[0].diff\n",
			"file": "tensorflow_serving/servables/caffe/test_data/py_layers/00000001/test_python_layer.py",
			"file_size": 402,
			"file_write_time": 131193571244026798,
			"settings":
			{
				"buffer_size": 402,
				"line_ending": "Unix",
				"name": "SimpleL"
			}
		},
		{
			"contents": "{'google': <module 'google' (built-in)>, 'copy_reg': <module 'copy_reg' from '/usr/lib/python2.7/copy_reg.pyc'>, 'sre_compile': <module 'sre_compile' from '/usr/lib/python2.7/sre_compile.pyc'>, '_sre': <module '_sre' (built-in)>, 'encodings': <module 'encodings' from '/usr/lib/python2.7/encodings/__init__.pyc'>, 'site': <module 'site' from '/usr/lib/python2.7/site.pyc'>, '__builtin__': <module '__builtin__' (built-in)>, 'sysconfig': <module 'sysconfig' from '/usr/lib/python2.7/sysconfig.pyc'>, '__main__': <module '__main__' (built-in)>, 'encodings.encodings': None, 'abc': <module 'abc' from '/usr/lib/python2.7/abc.pyc'>, 'posixpath': <module 'posixpath' from '/usr/lib/python2.7/posixpath.pyc'>, '_weakrefset': <module '_weakrefset' from '/usr/lib/python2.7/_weakrefset.pyc'>, 'errno': <module 'errno' (built-in)>, 'encodings.codecs': None, 'sre_constants': <module 'sre_constants' from '/usr/lib/python2.7/sre_constants.pyc'>, 're': <module 're' from '/usr/lib/python2.7/re.pyc'>, '_abcoll': <module '_abcoll' from '/usr/lib/python2.7/_abcoll.pyc'>, 'test_python_layer': <module 'test_python_layer' from '/home/psinapse/.cache/bazel/_bazel_psinapse/5801f6736ccf8880498ee6ef4f02d03b/execroot/serving/bazel-out/local-fastbuild/bin/tensorflow_serving/servables/caffe/caffe_session_bundle_factory_test_py.runfiles/tf_serving/tensorflow_serving/servables/caffe/test_data/py_layers/00000001/test_python_layer.py'>, 'types': <module 'types' from '/usr/lib/python2.7/types.pyc'>, '_codecs': <module '_codecs' (built-in)>, 'encodings.__builtin__': None, '_warnings': <module '_warnings' (built-in)>, 'genericpath': <module 'genericpath' from '/usr/lib/python2.7/genericpath.pyc'>, 'stat': <module 'stat' from '/usr/lib/python2.7/stat.pyc'>, 'zipimport': <module 'zipimport' (built-in)>, '_sysconfigdata': <module '_sysconfigdata' from '/usr/lib/python2.7/_sysconfigdata.pyc'>, 'warnings': <module 'warnings' from '/usr/lib/python2.7/warnings.pyc'>, 'UserDict': <module 'UserDict' from '/usr/lib/python2.7/UserDict.pyc'>, 'encodings.ascii': <module 'encodings.ascii' from '/usr/lib/python2.7/encodings/ascii.pyc'>, 'sys': <module 'sys' (built-in)>, 'pwd': <module 'pwd' (built-in)>, 'codecs': <module 'codecs' from '/usr/lib/python2.7/codecs.pyc'>, '_sysconfigdata_nd': <module '_sysconfigdata_nd' from '/usr/lib/python2.7/plat-x86_64-linux-gnu/_sysconfigdata_nd.pyc'>, 'os.path': <module 'posixpath' from '/usr/lib/python2.7/posixpath.pyc'>, '_locale': <module '_locale' (built-in)>, 'sitecustomize': <module 'sitecustomize' from '/usr/lib/python2.7/sitecustomize.pyc'>, 'signal': <module 'signal' (built-in)>, 'traceback': <module 'traceback' from '/usr/lib/python2.7/traceback.pyc'>, 'linecache': <module 'linecache' from '/usr/lib/python2.7/linecache.pyc'>, 'posix': <module 'posix' (built-in)>, 'encodings.aliases': <module 'encodings.aliases' from '/usr/lib/python2.7/encodings/aliases.pyc'>, 'exceptions': <module 'exceptions' (built-in)>, 'sre_parse': <module 'sre_parse' from '/usr/lib/python2.7/sre_parse.pyc'>, 'os': <module 'os' from '/usr/lib/python2.7/os.pyc'>, '_weakref': <module '_weakref' (built-in)>}",
			"settings":
			{
				"buffer_size": 3118,
				"line_ending": "Windows",
				"name": "{'google': <module 'google' (built-in)>, 'copy_reg"
			}
		},
		{
			"contents": "package(\n    default_visibility = [\"//tensorflow_serving:internal\"],\n)\n\nfilegroup(\n    name = \"py_layers\",\n    srcs = [\n        \"py_layers/00000001/deploy.prototxt\",\n        \"py_layers/00000001/test_python_layer.py\",\n        \"py_layers/00000001/weights.caffemodel\",\n    ],\n)\n\npy_binary(\n    name = \"mnist_caffe_fetch\",\n    srcs = [\"mnist_caffe_fetch.py\"],\n)\n\ngenrule(\n    name = \"mnist_pretrained_caffe\",\n    cmd = '''\n        rm -rf $(@D)/mnist_pretrained_caffe/00000023;\n        $(locations :mnist_caffe_fetch) --version=23 $(@D)/mnist_pretrained_caffe;\n        ''', \n    outs = [\n        \"mnist_pretrained_caffe/00000023/deploy.prototxt\",\n        \"mnist_pretrained_caffe/00000023/weights.caffemodel\",\n        \"mnist_pretrained_caffe/00000023/classlabels.txt\",\n    ],\n    tools = [\n        \":mnist_caffe_fetch\",\n    ],\n)\n\ncc_library(\n    name = \"mnist_sample\",\n    hdrs = [\"mnist_sample.h\"]\n)\n",
			"file": "tensorflow_serving/servables/caffe/test_data/BUILD",
			"file_size": 895,
			"file_write_time": 131193571243701757,
			"settings":
			{
				"buffer_size": 895,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "// Copyright 2016 IBM Corp. All Rights Reserved.\n#pragma once\n\n#include <tuple>\n#include <vector>\n\nnamespace tensorflow {\nnamespace serving {\n\n/* a single test sample from the mnist dataset, 28px x 28px */\ninline const std::vector<float> mnist_sample_28x28() {\n  return {\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0.33 ,0.73 ,0.62 ,0.59 ,0.24 ,0.14 ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0.87 ,1    ,1    ,1    ,1    ,0.95 ,0.78 ,0.78 ,0.78 ,0.78 ,0.78 ,0.78 ,0.78 ,0.78 ,0.67 ,0.2  ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0.26 ,0.45 ,0.28 ,0.45 ,0.64 ,0.89 ,1    ,0.88 ,1    ,1    ,1    ,0.98 ,0.9  ,1    ,1    ,0.55 ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.067,0.26 ,0.055,0.26 ,0.26 ,0.26 ,0.23 ,0.082,0.93 ,1    ,0.42 ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.33 ,0.99 ,0.82 ,0.071,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.086,0.91 ,1    ,0.33 ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.51 ,1    ,0.93 ,0.17 ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.23 ,0.98 ,1    ,0.24 ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.52 ,1    ,0.73 ,0.02 ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.035,0.8  ,0.97 ,0.23 ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.49 ,1    ,0.71 ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.29 ,0.98 ,0.94 ,0.22 ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.075,0.87 ,1    ,0.65 ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.012,0.8  ,1    ,0.86 ,0.14 ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.15 ,1    ,1    ,0.3  ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.12 ,0.88 ,1    ,0.45 ,0.0039,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.52 ,1    ,1    ,0.2  ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.24 ,0.95 ,1    ,1    ,0.2  ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.47 ,1    ,1    ,0.86 ,0.16 ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0.47 ,1    ,0.81 ,0.071,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0,\n      0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0    ,0\n    };\n}\n\n} // namespace serving\n} // namespace tensorflow",
			"file": "tensorflow_serving/servables/caffe/test_data/mnist_sample.h",
			"file_size": 5118,
			"file_write_time": 131193571243801769,
			"settings":
			{
				"buffer_size": 5118,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/simple_thread_sink.h",
			"settings":
			{
				"buffer_size": 1290,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/predict_impl.h",
			"settings":
			{
				"buffer_size": 1250,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/predict_impl.cc",
			"settings":
			{
				"buffer_size": 5557,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/simple_thread_sink.cc",
			"settings":
			{
				"buffer_size": 1013,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/core/framework/tensor.h",
			"settings":
			{
				"buffer_size": 24328,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/core/framework/tensor.proto",
			"settings":
			{
				"buffer_size": 2468,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/core/framework/types.proto",
			"settings":
			{
				"buffer_size": 1834,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow/tensorflow/core/protobuf/meta_graph.proto",
			"settings":
			{
				"buffer_size": 8885,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tf_models/inception/inception/data/build_imagenet_data.py",
			"settings":
			{
				"buffer_size": 26199,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_simple_servers.cc",
			"settings":
			{
				"buffer_size": 4386,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/model_servers/BUILD",
			"settings":
			{
				"buffer_size": 5192,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "# Copyright 2016 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n#!/usr/bin/env python2.7\n\n\"\"\"A client that talks to tensorflow_model_server loaded with mnist model.\n\nThe client downloads test images of mnist data set, queries the service with\nsuch test images to get predictions, and calculates the inference error rate.\n\nTypical usage example:\n\n    mnist_client.py --num_tests=100 --server=localhost:9000\n\"\"\"\n\nimport sys\nimport threading\nfrom timeit import default_timer as timer\n\n# This is a placeholder for a Google-internal import.\n\nfrom grpc.beta import implementations\nimport numpy\nimport tensorflow as tf\nfrom client_util import InferenceStats\n\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2\nfrom tensorflow_serving.example import mnist_input_data\n\n\ntf.app.flags.DEFINE_integer('concurrency', 1,\n                            'maximum number of concurrent inference requests')\ntf.app.flags.DEFINE_integer('num_tests', 100, 'Number of test images')\ntf.app.flags.DEFINE_string('server', '', 'mnist_inference service host:port')\ntf.app.flags.DEFINE_string('work_dir', '/tmp', 'Working directory. ')\nFLAGS = tf.app.flags.FLAGS\n\n\ndef do_inference(hostport, work_dir, concurrency, num_tests):\n  \"\"\"Tests mnist_inference service with concurrent requests.\n\n  Args:\n    hostport: Host:port address of the mnist_inference service.\n    work_dir: The full path of working directory for test data set.\n    concurrency: Maximum number of concurrent requests.\n    num_tests: Number of test images to use.\n\n  Returns:\n    An instance of InferenceStats\n\n  Raises:\n    IOError: An error occurred processing test data set.\n  \"\"\"\n  test_data_set = mnist_input_data.read_data_sets(work_dir).test\n  host, port = hostport.split(':')\n  channel = implementations.insecure_channel(host, int(port))\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  cv = threading.Condition()\n  result = {'active': 0, 'error': 0, 'done': 0}\n  result_timing = numpy.zeros(num_tests, dtype=numpy.float64);\n  def done(reqid, result_future, label):\n    with cv:\n      # Workaround for gRPC issue https://github.com/grpc/grpc/issues/7133\n      try:\n        exception = result_future.exception()\n      except AttributeError:\n        exception = None\n      if exception:\n        result_timing[reqid] = numpy.NaN  # ignore when evaluating time statistics\n        result['error'] += 1\n        print exception\n      else:\n        result_timing[reqid] = timer() - result_timing[reqid]\n        sys.stdout.write('.')\n        sys.stdout.flush()\n        response = numpy.array(result_future.result().outputs['scores'])\n        prediction = numpy.argmax(response)\n        if label != prediction:\n          result['error'] += 1\n      result['done'] += 1\n      result['active'] -= 1\n      cv.notify()\n  start_time = timer()\n  for n in range(num_tests):\n    request = predict_pb2.PredictRequest()\n    request.model_spec.name = 'mnist'\n    image, label = test_data_set.next_batch(1)\n    request.inputs['images'].CopyFrom(\n        tf.contrib.util.make_tensor_proto(image[0], shape=[1, image[0].size]))\n    with cv:\n      while result['active'] == concurrency:\n        cv.wait()\n      result['active'] += 1\n    result_timing[n] = timer()\n    result_future = stub.Predict.future(request, 5.0)  # 5 seconds\n    result_future.add_done_callback(\n        lambda result_future, l=label[0], n=n: done(n, result_future, l))  # pylint: disable=cell-var-from-loop\n  with cv:\n    while result['done'] != num_tests:\n      cv.wait()\n\n    return InferenceStats(num_tests,\n      result['error'] / float(num_tests),\n      result_timing,\n      timer() - start_time)\n\n\ndef main(_):\n  if FLAGS.num_tests > 10000:\n    print 'num_tests should not be greater than 10k'\n    return\n  if not FLAGS.server:\n    print 'please specify server host:port'\n    return\n  stats = do_inference(FLAGS.server, FLAGS.work_dir,\n                       FLAGS.concurrency, FLAGS.num_tests)\n  InferenceStats.print_summary(stats)\n\n\nif __name__ == '__main__':\n  tf.app.run()",
			"settings":
			{
				"buffer_size": 4670,
				"line_ending": "Windows",
				"name": "# Copyright 2016 Google Inc. All Rights Reserved."
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_source_adapter.h",
			"settings":
			{
				"buffer_size": 1948,
				"line_ending": "Unix"
			}
		},
		{
			"file": "tensorflow_serving/servables/caffe/caffe_simple_servers.h",
			"settings":
			{
				"buffer_size": 2360,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "",
	"build_system_choices":
	[
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 100.0,
		"last_filter": "pyth",
		"selected_items":
		[
			[
				"pyth",
				"Set Syntax: Python"
			],
			[
				"trail",
				"Trailing Spaces: Delete Trailing Spaces"
			],
			[
				"markmon",
				"Markmon launch"
			],
			[
				"trailin",
				"Trailing Spaces: Delete Trailing Spaces"
			],
			[
				"tr",
				"Trailing Spaces: Delete Trailing Spaces"
			],
			[
				"t",
				"Trailing Spaces: Delete Trailing Spaces"
			],
			[
				"markmo",
				"Markmon launch"
			],
			[
				"spaces",
				"Indentation: Convert to Spaces"
			],
			[
				"TRAIL",
				"Trailing Spaces: Delete Trailing Spaces"
			],
			[
				"traIL",
				"Trailing Spaces: Delete Trailing Spaces"
			],
			[
				"trails",
				"Trailing Spaces: Delete Trailing Spaces"
			],
			[
				"com",
				"Toggle Comment"
			],
			[
				"tra",
				"Trailing Spaces: Delete Trailing Spaces"
			],
			[
				"py",
				"Set Syntax: Python"
			],
			[
				"del",
				"Trailing Spaces: Delete Trailing Spaces"
			],
			[
				"install",
				"Package Control: Install Package"
			],
			[
				"trailing",
				"Trailing Spaces: Delete Trailing Spaces"
			],
			[
				"c",
				"Set Syntax: C++"
			],
			[
				"next",
				"SublimeLinter: Next Error"
			],
			[
				"lint",
				"SublimeLinter: Show All Errors"
			],
			[
				"lint gu",
				"SublimeLinter: Choose Gutter Theme"
			],
			[
				"java",
				"Set Syntax: JavaScript"
			],
			[
				"pyt",
				"Set Syntax: Python"
			],
			[
				"c++",
				"Set Syntax: C++"
			],
			[
				"j",
				"Set Syntax: JSON"
			],
			[
				"comment",
				"Toggle Comment"
			],
			[
				"cmake",
				"Set Syntax: CMake"
			],
			[
				"trsil",
				"Trailing Spaces: Toggle Trailing Spaces Highlighting"
			],
			[
				"insta",
				"Package Control: Install Package"
			],
			[
				"markm",
				"Markmon launch"
			],
			[
				"files",
				"View: Toggle Open Files in Side Bar"
			],
			[
				"",
				"Markmon launch"
			],
			[
				"make",
				"Set Syntax: Makefile"
			],
			[
				"markdown",
				"Set Syntax: Markdown"
			],
			[
				"launch",
				"Markmon launch"
			],
			[
				"laun",
				"Markmon launch"
			],
			[
				"remove",
				"Package Control: Remove Package"
			],
			[
				"mark",
				"Set Syntax: MultiMarkdown"
			],
			[
				"strap",
				"Strapdown.js Markdown: Preview in a browser"
			],
			[
				"package",
				"Package Control: Install Package"
			],
			[
				"Package Control: insta",
				"Package Control: Install Package"
			],
			[
				"packa",
				"Package Control: Install Package"
			]
		],
		"width": 392.0
	},
	"console":
	{
		"height": 373.0,
		"history":
		[
			"import urllib.request,os,hashlib; h = '2915d1851351e5ee549c20394736b442' + '8bc59f460fa1548d1514676163dafc88'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)"
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"expanded_folders":
	[
		"/C/dev/vm/data-share/serving",
		"/C/dev/vm/data-share/serving/tensorflow",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow",
		"/C/dev/vm/data-share/serving/tensorflow/third_party",
		"/C/dev/vm/data-share/serving/tensorflow/third_party/gpus",
		"/C/dev/vm/data-share/serving/tensorflow/third_party/gpus/cuda",
		"/C/dev/vm/data-share/serving/tensorflow/tools",
		"/C/dev/vm/data-share/serving/tensorflow_serving",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe",
		"/C/dev/vm/data-share/serving/third_party",
		"/C/dev/vm/data-share/serving/tools",
		"/C/dev/vm/data-share/serving/utils"
	],
	"file_history":
	[
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/contrib/quantization/kernels/quantized_concat_op.cc",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/core/framework/tensor_types.h",
		"/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/roi_data_layer/layer.py",
		"/C/dev/vm/data-share/serving/third_party/caffe/cuda.bzl",
		"/C/dev/vm/data-share/serving/tensorflow_serving/example/mnist_inference_2.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/example/mnist_inference.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_session_bundle_factory_test_py.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_serving_session.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/example/mnist_client.py",
		"/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/tools/demo.py",
		"/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/fast_rcnn/bbox_transform.py",
		"/C/dev/vm/data-share/serving/third_party/caffe/python_prelude.h",
		"/C/dev/vm/data-share/serving/third_party/caffe/openblas_prelude.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_session_bundle.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_serving_session.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_session_bundle_config.proto",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_session_bundle_factory.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/tensorflow/session_bundle_config.proto",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_py_util.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_py_util.cc",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/contrib/session_bundle/signature.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_signature.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_session_bundle_factory.h",
		"/C/Users/IBM_ADMIN/Downloads/deeplearning/live_net.cpp",
		"/C/dev/vm/data-share/serving/tensorflow_serving/example/rpc_utils.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_source_adapter.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_signature.cc",
		"/C/dev/vm/data-share/serving/README.md",
		"/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/Makefile",
		"/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/models/pascal_voc/VGG16/faster_rcnn_alt_opt/faster_rcnn_test.pt",
		"/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/models/pascal_voc/VGG16/faster_rcnn_end2end/test.prototxt",
		"/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/rpn/proposal_layer.py",
		"/C/dev/vm/data-share/serving/tensorflow_serving/session_bundle/signature.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/example/obj_detector_ssd.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/example/ssd_fetch.py",
		"/C/dev/vm/data-share/serving/tensorflow_serving/example/rcnn_setup.py",
		"/C/dev/vm/data-share/serving/tensorflow_serving/example/obj_detector_rcnn.cc",
		"/C/dev/vm/data-share/ssd_data/models/VGGNet/VOC0712/SSD_300x300/00000001/deploy.prototxt",
		"/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/README.md",
		"/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/experiments/cfgs/faster_rcnn_end2end.yml",
		"/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/rpn/proposal_target_layer.py",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/example/BUILD",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/example/mnist_caffe_fetch.py",
		"/C/dev/vm/data-share/serving/third_party/caffe/pycaffe.py",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/cc/ops/cc_op_gen.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/test_data/py_layers/00000001/__init__.py",
		"/C/Users/IBM_ADMIN/Downloads/deeplearning/data/synset_words.txt",
		"/C/Users/IBM_ADMIN/Downloads/classlabels.txt",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/contrib/session_bundle/manifest.proto",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_signature_test.cc",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/core/framework/tensor.proto",
		"/C/dev/vm/data-share/serving/classlabels.txt",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/cc/ops/const_op.cc",
		"/C/dev/vm/data-share/serving/third_party/openblas/BUILD",
		"/C/dev/vm/data-share/serving/third_party/openblas/prelude.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/workspace.bzl",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/workspace.bzl",
		"/C/dev/vm/data-share/serving/caffe.BUILD",
		"/C/dev/vm/data-share/serving/RELEASE.md",
		"/C/dev/vm/data-share/serving/third_party/caffe/BUILD",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/core/platform/default/build_config.bzl",
		"/C/dev/vm/data-share/serving/tensorflow_serving/core/manager.h",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/core/lib/core/errors.h",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/contrib/session_bundle/signature.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_session_bundle.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/simple_thread_sink.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/simple_thread_sink.cc",
		"/C/dev/vm/data-share/serving/tensorflow/third_party/gpus/cuda/BUILD",
		"/C/dev/vm/data-share/serving/tensorflow/third_party/gpus/cuda/platform.bzl",
		"/C/dev/vm/data-share/serving/tensorflow/third_party/gpus/cuda/build_defs.bzl",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/core/platform/default/build_config/BUILD",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/core/platform/default/platform.bzl",
		"/C/dev/vm/data-share/serving/tensorflow_serving/example/BUILD",
		"/C/Users/IBM_ADMIN/Downloads/nalini/preprocess (2)/DeployFeat.py",
		"/C/Users/IBM_ADMIN/Downloads/nalini/preprocess (2)/dist/dist.py",
		"/C/Users/IBM_ADMIN/Downloads/nalini/preprocess (2)/LandmarkDetection.py",
		"/C/dev/vm/data-share/serving/tensorflow_serving/batching/batching_session.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/resources/resources.proto",
		"/C/dev/vm/data-share/serving/tensorflow_serving/resources/resource_values.h",
		"/C/Users/IBM_ADMIN/Downloads/nalini/preprocess (2)/process.py",
		"/C/Users/IBM_ADMIN/Downloads/nalini/preprocess (2)/Alignment.py",
		"/C/Users/IBM_ADMIN/Downloads/nalini/preprocess (2)/dist/batch_dist.py",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/BUILD",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/hashmap/hashmap_source_adapter_test.cc",
		"/C/dev/vm/data-share/serving/third_party/caffe.bzl",
		"/C/dev/vm/data-share/serving/third_party/BUILD",
		"/C/dev/vm/data-share/serving/tools/BUILD",
		"/C/dev/vm/data-share/serving/tensorflow_serving/example/mnist_inference_caffe.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/example/mnist_inference_2_caffe.cc",
		"/C/Users/IBM_ADMIN/Downloads/nalini/preprocess (2)/LandmarkProcessSingle.py",
		"/C/Users/IBM_ADMIN/Downloads/nalini/preprocess (2)/file_list.txt",
		"/C/Users/IBM_ADMIN/Downloads/nalini/preprocess (2)/process_extract.py",
		"/C/dev/vm/data-share/serving/tools/bazel.rc",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/contrib/cmake/tf_stream_executor.cmake",
		"/C/dev/vm/data-share/serving/tensorflow_serving/test_util/test_stats.py",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_session_bundle_factory_test.cc",
		"/C/Users/IBM_AD~1/AppData/Local/Temp/7zE093A1752/deploy_LFW.prototxt",
		"/C/Users/IBM_ADMIN/Downloads/nalini/preprocess (2)/deploy_LFW.prototxt",
		"/C/dev/vm/data-share/serving/WORKSPACE",
		"/C/dev/vm/data-share/serving/tensorflow/third_party/gpus/cuda/cuda_config.sh",
		"/C/dev/vm/data-share/serving/tensorflow_serving/serving.bzl",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/tensorflow/BUILD",
		"/C/Users/IBM_ADMIN/Downloads/aaa/mnist_pretrained_caffe (1)/deploy.prototxt",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/example/mnist_sample.h",
		"/C/dev/vm/data-share/caffe/examples/mnist/lenet.prototxt",
		"/C/dev/vm/data-share/caffe/examples/mnist/train_lenet.sh",
		"/C/dev/vm/data-share/caffe/src/caffe/util/db_lmdb.cpp",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_simple_servers_test.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/batching/basic_batch_scheduler.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/core/servable_handle.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_source_adapter.proto",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/caffe/caffe_source_adapter_test.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/core/servable_data.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/test_util/test_util.cc",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/core/platform/test.h",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/core/platform/env.cc",
		"/C/dev/vm/data-share/serving/tensorflow_serving/core/test_util/source_adapter_test_util.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/core/aspired_versions_manager_builder.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/servables/tensorflow/simple_servers.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/g3doc/serving_advanced.md",
		"/C/dev/vm/data-share/serving/AUTHORS",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/tensorflow.bzl",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/stream_executor/dso_loader.cc",
		"/C/dev/vm/data-share/caffe/src/caffe/layers/data_layer.cpp",
		"/C/dev/vm/data-share/caffe/src/caffe/layers/input_layer.cpp",
		"/C/dev/vm/data-share/serving/tensorflow_serving/batching/batching_session.cc",
		"/C/dev/vm/data-share/serving/tensorflow/tensorflow/core/public/session.h",
		"/C/dev/vm/data-share/serving/tensorflow_serving/batching/batch_scheduler_retrier.h"
	],
	"find":
	{
		"height": 57.0
	},
	"find_in_files":
	{
		"height": 157.0,
		"where_history":
		[
			"C:\\dev\\vm\\data-share\\serving",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow_serving",
			"C:\\dev\\vm\\data-share\\serving\\",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow_serving",
			"C:\\dev\\vm\\data-share\\serving",
			"C:\\dev\\vm\\data-share\\caffe-ssd",
			"C:\\dev\\vm\\data-share\\serving",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow_serving",
			"C:\\dev\\vm\\data-share\\rcnn_data\\py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af\\lib",
			"C:\\dev\\vm\\data-share\\serving",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow_serving",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow",
			"C:\\dev\\vm\\data-share\\serving\\tf_models\\inception\\",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow_serving",
			"C:\\Users\\IBM_ADMIN\\Downloads\\nalini\\preprocess (2)",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow_serving",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow_serving\\servables\\caffe",
			"C:\\dev\\vm\\data-share\\serving\\serving\\servables\\caffe",
			"C:\\dev\\vm\\data-share\\serving\\servables\\caffe",
			"C:\\dev\\vm\\data-share\\serving\\",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow",
			"C:\\dev\\vm\\data-share\\serving",
			"C:\\dev\\vm\\data-share\\caffe\\src",
			"C:\\dev\\vm\\data-share\\caffe",
			"C:\\dev\\vm\\data-share\\serving",
			"C:\\dev\\vm\\data-share\\caffe\\src",
			"C:\\dev\\vm\\data-share\\caffe",
			"C:\\dev\\vm\\data-share\\serving",
			"C:\\dev\\vm\\data-share\\caffe\\src",
			"C:\\dev\\vm\\data-share\\serving",
			"C:\\dev\\vm\\data-share\\caffe\\src",
			"C:\\dev\\vm\\data-share\\caffe\\include",
			"C:\\dev\\vm\\data-share\\caffe\\includes",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow_serving",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow",
			"C:\\dev\\vm\\data-share\\serving\\tensorflow_serving",
			"C:\\dev\\vm\\data-share\\caffe\\src",
			"C:\\dev\\vm\\data-share\\serving\\",
			"C:\\dev\\vm\\data-share\\caffe\\build_ubuntu",
			"C:\\dev\\vm\\data-share\\caffe",
			"C:\\dev\\vm\\data-share\\caffe\\include",
			"C:\\dev\\vm\\data-share\\caffe\\src",
			"C:\\dev\\vm\\data-share\\caffe\\build_ubuntu\\install",
			"C:\\dev\\vm\\data-share\\caffe\\build_ubuntu\\install\\include",
			"C:\\sdks\\nvidia\\CUDA\\v7.5\\toolkit\\include",
			"C:\\dev\\public\\hemi\\examples\\vs2013_projects\\simple",
			"C:\\dev\\public\\ecuda\\include\\ecuda"
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
			"session_bundle/signature.h",
			"cuda_library_path",
			"@org_tensorflow//third_party/gpus/cuda",
			"cuda",
			":cudnn",
			"cudnn",
			"cuda_configure.bzl",
			"local_config_cuda",
			"/crosstool",
			":crosstool",
			"third_party/gpus/crosstool:crosstool",
			"@org_tensorflow//third_party/gpus/crosstool:crosstool",
			"make_tensor_proto",
			"protobuf_incl",
			"@caffe_tools",
			"test_num",
			"input tensor alias not found in signature",
			"num_test",
			"num_tests",
			"test_num",
			"if_cuda",
			"ef if_cuda",
			"using_nvcc",
			"using_gcudacc",
			"if_cuda",
			"Detatch()",
			"Detach()",
			"ServableTraits<Caffe>",
			"constexpr char*",
			"InitMain",
			"CaffeGlobalInit",
			"S = TensorFlow",
			"kModelType()",
			"kModelType",
			"runtime",
			"BatchingParameters",
			"Serving::",
			"use_caffe",
			"::kModelType",
			"Tensorflow",
			"TensorFlow",
			"tensorflow",
			"TensorFlow",
			"SessionBundleSourceAdapter",
			"kCaffeModelType",
			"kTensorFlowModelType",
			"Traits::",
			"SessionBundleSourceAdapter",
			"CaffeSourceAdapterConfig",
			"SessionBundleSourceAdapter",
			"TensorflowPredictImpl",
			"struct sessionbundle",
			"~",
			"TestPyDirPath",
			"_py",
			"struct SessionBundle",
			"class sessionbundle",
			"simple_servers",
			"client_util",
			"BindGenericInputs",
			"BindGenericNames",
			"manifest_proto",
			"GetNamedSignature",
			"RunClassification",
			"output_tensor_names",
			"TensorflowPredictImpl",
			"GetServableHandle",
			"Servable type doesn't match the asked for type.",
			"handle",
			"GetServableHandle",
			"Servable type doesn't match the asked for type.",
			"BuildSingleModelConfig",
			"ReloadConfig",
			"ServableData(",
			"CaffeSessionBundleConfig",
			"CaffeSourceAdapterConfig",
			"ServableData(",
			"ServableData",
			"File-system polling update",
			"kTensorFlowModelType",
			"CreateSourceAdapter",
			"kCaffeModelType",
			"ConnectSourceToTarget",
			"CreateSourceAdapter",
			"ModelServerSourceAdapter",
			"UnarySourceAdapter",
			"class SourceAdapter",
			"def if_cuda",
			"cuda:platform.bzl",
			"SessionBundleSourceAdapter",
			"ConnectSourceToTarget",
			"session",
			"ServerCoreTestAccess",
			"ModelServerSourceAdapter",
			"SessionBundleFactory",
			"::Create",
			"CreateSingleTFModelManagerFromBasePath",
			"USE_CAFFE",
			"__workspace_dir__",
			"cuda_configure.bzl",
			"kNumChannels",
			"GetConfiguration",
			"get_configuration",
			"GetConfiguration",
			"Detect",
			"detect",
			"Detect",
			"detect",
			"DetectConfiguration",
			"ConfigurationRequest",
			"handshake",
			"im_scale_to_fit",
			"Connect",
			"connect",
			"Detect",
			"TMyReq",
			"::AsyncService",
			"AsyncService",
			"::AsyncService",
			"DetectResponse",
			"Serve",
			"::Service",
			"AsyncService",
			"DetectorStub",
			"DetectService",
			"AsyncService",
			"DetectService",
			"Detect"
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 59,
			"sheets":
			[
				{
					"buffer": 0,
					"file": "WORKSPACE",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 423,
						"regions":
						{
						},
						"selection":
						[
							[
								284,
								284
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				},
				{
					"buffer": 1,
					"file": "caffe.BUILD",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6541,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2010.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 2,
					"file": "/C/Users/IBM_ADMIN/Downloads/mnist_pretrained_caffe/deploy.prototxt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1746,
						"regions":
						{
						},
						"selection":
						[
							[
								1712,
								1712
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 40.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 3,
					"file": "third_party/caffe/BUILD",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 793,
						"regions":
						{
						},
						"selection":
						[
							[
								358,
								358
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"type": "text"
				},
				{
					"buffer": 4,
					"file": "third_party/caffe/__init__.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1314,
						"regions":
						{
						},
						"selection":
						[
							[
								1314,
								1314
							]
						],
						"settings":
						{
							"spell_check": false,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 5,
					"type": "text"
				},
				{
					"buffer": 5,
					"file": "third_party/caffe/config.bzl",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 382,
						"regions":
						{
						},
						"selection":
						[
							[
								382,
								382
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"syntax": "Packages/Python/Python.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 6,
					"type": "text"
				},
				{
					"buffer": 6,
					"file": "tensorflow_serving/workspace.bzl",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1586,
						"regions":
						{
						},
						"selection":
						[
							[
								623,
								623
							]
						],
						"settings":
						{
							"spell_check": false,
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 7,
					"type": "text"
				},
				{
					"buffer": 7,
					"file": "tensorflow_serving/servables/tensorflow/session_bundle_source_adapter.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2627,
						"regions":
						{
						},
						"selection":
						[
							[
								1499,
								1499
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 8,
					"type": "text"
				},
				{
					"buffer": 8,
					"file": "tensorflow_serving/servables/tensorflow/simple_servers.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 4048,
						"regions":
						{
						},
						"selection":
						[
							[
								3015,
								3041
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 486.0,
						"zoom_level": 1.0
					},
					"stack_index": 9,
					"type": "text"
				},
				{
					"buffer": 9,
					"file": "tensorflow/tensorflow/contrib/session_bundle/signature_test.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 23164,
						"regions":
						{
						},
						"selection":
						[
							[
								7552,
								7552
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 3583.0,
						"zoom_level": 1.0
					},
					"stack_index": 10,
					"type": "text"
				},
				{
					"buffer": 10,
					"file": "tensorflow_serving/servables/caffe/caffe_session_bundle.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1264,
						"regions":
						{
						},
						"selection":
						[
							[
								933,
								950
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"spell_check": false,
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 11,
					"type": "text"
				},
				{
					"buffer": 11,
					"file": "tensorflow_serving/servables/caffe/BUILD",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 7980,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2696.0,
						"zoom_level": 1.0
					},
					"stack_index": 12,
					"type": "text"
				},
				{
					"buffer": 12,
					"file": "tensorflow_serving/servables/caffe/caffe_signature.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1452,
						"regions":
						{
						},
						"selection":
						[
							[
								1452,
								1452
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 13,
					"type": "text"
				},
				{
					"buffer": 13,
					"file": "tensorflow_serving/servables/caffe/caffe_signature.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6192,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1766.0,
						"zoom_level": 1.0
					},
					"stack_index": 14,
					"type": "text"
				},
				{
					"buffer": 14,
					"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/fast_rcnn/test.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 11299,
						"regions":
						{
						},
						"selection":
						[
							[
								10148,
								10148
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 5246.0,
						"zoom_level": 1.0
					},
					"stack_index": 15,
					"type": "text"
				},
				{
					"buffer": 15,
					"file": "tensorflow_serving/example/BUILD",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3629,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 580.0,
						"zoom_level": 1.0
					},
					"stack_index": 16,
					"type": "text"
				},
				{
					"buffer": 16,
					"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/setup.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5715,
						"regions":
						{
						},
						"selection":
						[
							[
								2231,
								2231
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 591.0,
						"zoom_level": 1.0
					},
					"stack_index": 17,
					"type": "text"
				},
				{
					"buffer": 17,
					"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/fast_rcnn/nms_wrapper.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 644,
						"regions":
						{
						},
						"selection":
						[
							[
								644,
								644
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 18,
					"type": "text"
				},
				{
					"buffer": 18,
					"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/fast_rcnn/config.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 9263,
						"regions":
						{
						},
						"selection":
						[
							[
								3302,
								3302
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1531.0,
						"zoom_level": 1.0
					},
					"stack_index": 19,
					"type": "text"
				},
				{
					"buffer": 19,
					"file": "tensorflow_serving/example/obj_detector_fetch.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6070,
						"regions":
						{
						},
						"selection":
						[
							[
								3616,
								3616
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1600.0,
						"zoom_level": 1.0
					},
					"stack_index": 20,
					"type": "text"
				},
				{
					"buffer": 20,
					"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/tools/_init_paths.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 627,
						"regions":
						{
						},
						"selection":
						[
							[
								627,
								627
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 21,
					"type": "text"
				},
				{
					"buffer": 21,
					"file": "tensorflow_serving/model_servers/server_core.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 13967,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 951.0,
						"zoom_level": 1.0
					},
					"stack_index": 22,
					"type": "text"
				},
				{
					"buffer": 22,
					"file": "tensorflow_serving/model_servers/main.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 11839,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 793.0,
						"zoom_level": 1.0
					},
					"stack_index": 23,
					"type": "text"
				},
				{
					"buffer": 23,
					"file": "tensorflow_serving/model_servers/server_core.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 11002,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1583.0,
						"zoom_level": 1.0
					},
					"stack_index": 24,
					"type": "text"
				},
				{
					"buffer": 24,
					"file": "tensorflow_serving/servables/tensorflow/session_bundle_factory.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3917,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 25,
					"type": "text"
				},
				{
					"buffer": 25,
					"file": "tensorflow_serving/example/obj_detector.proto",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1015,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 26,
					"type": "text"
				},
				{
					"buffer": 26,
					"file": "tensorflow_serving/example/obj_detector.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 17650,
						"regions":
						{
						},
						"selection":
						[
							[
								6288,
								6288
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 3513.0,
						"zoom_level": 1.0
					},
					"stack_index": 27,
					"type": "text"
				},
				{
					"buffer": 27,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2008,
						"regions":
						{
						},
						"selection":
						[
							[
								1564,
								1746
							]
						],
						"settings":
						{
							"auto_name": "error: cannot convert",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 28,
					"type": "text"
				},
				{
					"buffer": 28,
					"file": "tensorflow/tensorflow/core/util/command_line_flags.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3919,
						"regions":
						{
						},
						"selection":
						[
							[
								2291,
								2291
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1043.0,
						"zoom_level": 1.0
					},
					"stack_index": 29,
					"type": "text"
				},
				{
					"buffer": 29,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 0,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"default_dir": "C:\\dev\\vm\\data-share\\serving\\tensorflow_serving\\example",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 30,
					"type": "text"
				},
				{
					"buffer": 30,
					"file": "tensorflow_serving/servables/tensorflow/session_bundle_config.proto",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3642,
						"regions":
						{
						},
						"selection":
						[
							[
								3254,
								3273
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 233.0,
						"zoom_level": 1.0
					},
					"stack_index": 31,
					"type": "text"
				},
				{
					"buffer": 31,
					"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/tools/demo.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5115,
						"regions":
						{
						},
						"selection":
						[
							[
								665,
								665
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 32,
					"type": "text"
				},
				{
					"buffer": 32,
					"file": "tensorflow_serving/example/obj_detector_pb2.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 13859,
						"regions":
						{
						},
						"selection":
						[
							[
								72,
								72
							],
							[
								568,
								568
							],
							[
								666,
								666
							],
							[
								6074,
								6074
							],
							[
								6352,
								6352
							],
							[
								6641,
								6641
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 8.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 33,
					"type": "text"
				},
				{
					"buffer": 33,
					"file": "tensorflow_serving/example/obj_detector_utils.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1611,
						"regions":
						{
						},
						"selection":
						[
							[
								1176,
								1176
							]
						],
						"settings":
						{
							"auto_name": "",
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 34,
					"type": "text"
				},
				{
					"buffer": 34,
					"file": "tensorflow_serving/example/obj_detector_utils.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 11082,
						"regions":
						{
						},
						"selection":
						[
							[
								1220,
								1220
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 40.0,
						"zoom_level": 1.0
					},
					"stack_index": 35,
					"type": "text"
				},
				{
					"buffer": 35,
					"file": "tensorflow_serving/example/obj_detector_client.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6911,
						"regions":
						{
						},
						"selection":
						[
							[
								2983,
								2983
							]
						],
						"settings":
						{
							"spell_check": false,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1478.0,
						"zoom_level": 1.0
					},
					"stack_index": 36,
					"type": "text"
				},
				{
					"buffer": 36,
					"file": "tensorflow_serving/example/client_util.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1087,
						"regions":
						{
						},
						"selection":
						[
							[
								126,
								128
							]
						],
						"settings":
						{
							"auto_name": "client_util",
							"syntax": "Packages/Python/Python.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 37,
					"type": "text"
				},
				{
					"buffer": 37,
					"file": "tensorflow/tensorflow/tensorflow.bzl",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 30603,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 203.0,
						"zoom_level": 1.0
					},
					"stack_index": 38,
					"type": "text"
				},
				{
					"buffer": 38,
					"file": "tensorflow_serving/servables/tensorflow/session_bundle_source_adapter.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2938,
						"regions":
						{
						},
						"selection":
						[
							[
								1432,
								1438
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 6.0,
						"zoom_level": 1.0
					},
					"stack_index": 39,
					"type": "text"
				},
				{
					"buffer": 39,
					"file": "tf_models/syntaxnet/tensorflow/tensorflow/contrib/session_bundle/session_bundle.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2320,
						"regions":
						{
						},
						"selection":
						[
							[
								1239,
								1196
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 40,
					"type": "text"
				},
				{
					"buffer": 40,
					"file": "tensorflow/tensorflow/contrib/session_bundle/session_bundle.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3336,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 41,
					"type": "text"
				},
				{
					"buffer": 41,
					"file": "tensorflow_serving/servables/tensorflow/simple_servers.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2367,
						"regions":
						{
						},
						"selection":
						[
							[
								2215,
								2215
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 42,
					"type": "text"
				},
				{
					"buffer": 42,
					"file": "tensorflow/tensorflow/contrib/session_bundle/signature.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 10854,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 4046.0,
						"zoom_level": 1.0
					},
					"stack_index": 43,
					"type": "text"
				},
				{
					"buffer": 43,
					"file": "tensorflow_serving/servables/caffe/caffe_serving_session.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 13629,
						"regions":
						{
						},
						"selection":
						[
							[
								6375,
								6375
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 3523.0,
						"zoom_level": 1.0
					},
					"stack_index": 44,
					"type": "text"
				},
				{
					"buffer": 44,
					"file": "tensorflow_serving/servables/tensorflow/predict_impl.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5043,
						"regions":
						{
						},
						"selection":
						[
							[
								1321,
								1321
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 120.0,
						"zoom_level": 1.0
					},
					"stack_index": 45,
					"type": "text"
				},
				{
					"buffer": 45,
					"file": "tensorflow_serving/servables/tensorflow/predict_impl.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1399,
						"regions":
						{
						},
						"selection":
						[
							[
								937,
								880
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 46,
					"type": "text"
				},
				{
					"buffer": 46,
					"file": "tensorflow_serving/example/mnist_client.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5981,
						"regions":
						{
						},
						"selection":
						[
							[
								5320,
								5337
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2306.0,
						"zoom_level": 1.0
					},
					"stack_index": 47,
					"type": "text"
				},
				{
					"buffer": 47,
					"file": "tensorflow_serving/servables/tensorflow/BUILD",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 7457,
						"regions":
						{
						},
						"selection":
						[
							[
								2435,
								2435
							]
						],
						"settings":
						{
							"syntax": "Packages/C#/Build.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 961.0,
						"zoom_level": 1.0
					},
					"stack_index": 48,
					"type": "text"
				},
				{
					"buffer": 48,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 837308,
						"regions":
						{
							"match":
							{
								"flags": 112,
								"regions":
								[
									[
										2097,
										2107
									],
									[
										2333,
										2343
									],
									[
										2577,
										2587
									],
									[
										3585,
										3595
									],
									[
										3773,
										3783
									],
									[
										3959,
										3969
									],
									[
										4126,
										4136
									],
									[
										4443,
										4453
									],
									[
										4813,
										4823
									],
									[
										5169,
										5179
									],
									[
										5179,
										5189
									],
									[
										5502,
										5512
									],
									[
										5774,
										5784
									],
									[
										6112,
										6122
									],
									[
										6122,
										6132
									],
									[
										6182,
										6192
									],
									[
										6207,
										6217
									],
									[
										6309,
										6319
									],
									[
										6616,
										6626
									],
									[
										6826,
										6836
									],
									[
										7173,
										7183
									],
									[
										7414,
										7424
									],
									[
										7595,
										7605
									],
									[
										7963,
										7973
									],
									[
										8184,
										8194
									],
									[
										8773,
										8783
									],
									[
										8793,
										8803
									],
									[
										8976,
										8986
									],
									[
										8996,
										9006
									],
									[
										9353,
										9363
									],
									[
										9373,
										9383
									],
									[
										9650,
										9660
									],
									[
										9838,
										9848
									],
									[
										10161,
										10171
									],
									[
										10545,
										10555
									],
									[
										10837,
										10847
									],
									[
										11191,
										11201
									],
									[
										11201,
										11211
									],
									[
										11261,
										11271
									],
									[
										11286,
										11296
									],
									[
										11388,
										11398
									],
									[
										11707,
										11717
									],
									[
										11917,
										11927
									],
									[
										12264,
										12274
									],
									[
										12502,
										12512
									],
									[
										12712,
										12722
									],
									[
										12891,
										12901
									],
									[
										13613,
										13630
									],
									[
										13943,
										13960
									],
									[
										14124,
										14141
									],
									[
										14295,
										14312
									],
									[
										14422,
										14439
									],
									[
										14491,
										14508
									],
									[
										14554,
										14571
									],
									[
										14689,
										14706
									],
									[
										14754,
										14771
									],
									[
										14868,
										14885
									],
									[
										15103,
										15120
									],
									[
										15159,
										15176
									],
									[
										15234,
										15251
									],
									[
										15483,
										15500
									],
									[
										15533,
										15550
									],
									[
										15582,
										15599
									],
									[
										15631,
										15648
									],
									[
										15785,
										15802
									],
									[
										16080,
										16097
									],
									[
										16177,
										16194
									],
									[
										16373,
										16390
									],
									[
										16441,
										16458
									],
									[
										16624,
										16641
									],
									[
										16699,
										16716
									],
									[
										16888,
										16905
									],
									[
										16969,
										16986
									],
									[
										17139,
										17156
									],
									[
										17304,
										17321
									],
									[
										17464,
										17481
									],
									[
										17665,
										17682
									],
									[
										17913,
										17930
									],
									[
										18127,
										18144
									],
									[
										18338,
										18355
									],
									[
										18546,
										18563
									],
									[
										18865,
										18882
									],
									[
										19157,
										19174
									],
									[
										19566,
										19583
									],
									[
										19917,
										19934
									],
									[
										19991,
										20008
									],
									[
										20072,
										20089
									],
									[
										20147,
										20164
									],
									[
										20222,
										20239
									],
									[
										20492,
										20509
									],
									[
										20733,
										20750
									],
									[
										21021,
										21038
									],
									[
										21242,
										21259
									],
									[
										21639,
										21656
									],
									[
										22389,
										22406
									],
									[
										22719,
										22736
									],
									[
										22900,
										22917
									],
									[
										23071,
										23088
									],
									[
										23198,
										23215
									],
									[
										23267,
										23284
									],
									[
										23330,
										23347
									],
									[
										23465,
										23482
									],
									[
										23530,
										23547
									],
									[
										23644,
										23661
									],
									[
										23879,
										23896
									],
									[
										23935,
										23952
									],
									[
										24010,
										24027
									],
									[
										24259,
										24276
									],
									[
										24309,
										24326
									],
									[
										24358,
										24375
									],
									[
										24407,
										24424
									],
									[
										24561,
										24578
									],
									[
										24856,
										24873
									],
									[
										24953,
										24970
									],
									[
										25149,
										25166
									],
									[
										25217,
										25234
									],
									[
										25400,
										25417
									],
									[
										25475,
										25492
									],
									[
										25664,
										25681
									],
									[
										25745,
										25762
									],
									[
										25915,
										25932
									],
									[
										26080,
										26097
									],
									[
										26240,
										26257
									],
									[
										26441,
										26458
									],
									[
										26689,
										26706
									],
									[
										26903,
										26920
									],
									[
										27114,
										27131
									],
									[
										27322,
										27339
									],
									[
										27641,
										27658
									],
									[
										27933,
										27950
									],
									[
										28342,
										28359
									],
									[
										28693,
										28710
									],
									[
										28767,
										28784
									],
									[
										28848,
										28865
									],
									[
										28923,
										28940
									],
									[
										28998,
										29015
									],
									[
										29268,
										29285
									],
									[
										29509,
										29526
									],
									[
										29797,
										29814
									],
									[
										30018,
										30035
									],
									[
										30415,
										30432
									],
									[
										31173,
										31191
									],
									[
										32142,
										32147
									],
									[
										32344,
										32349
									],
									[
										32413,
										32418
									],
									[
										32564,
										32569
									],
									[
										32637,
										32642
									],
									[
										32666,
										32671
									],
									[
										32831,
										32836
									],
									[
										32884,
										32889
									],
									[
										32925,
										32930
									],
									[
										32980,
										32985
									],
									[
										33010,
										33015
									],
									[
										33028,
										33033
									],
									[
										33255,
										33260
									],
									[
										33422,
										33427
									],
									[
										33735,
										33740
									],
									[
										34052,
										34057
									],
									[
										34126,
										34131
									],
									[
										34170,
										34175
									],
									[
										34232,
										34237
									],
									[
										34305,
										34310
									],
									[
										34379,
										34384
									],
									[
										34426,
										34431
									],
									[
										34512,
										34517
									],
									[
										34522,
										34527
									],
									[
										34616,
										34621
									],
									[
										34631,
										34636
									],
									[
										34695,
										34700
									],
									[
										34736,
										34741
									],
									[
										34764,
										34769
									],
									[
										34962,
										34967
									],
									[
										35061,
										35066
									],
									[
										35130,
										35135
									],
									[
										35168,
										35173
									],
									[
										35318,
										35323
									],
									[
										35346,
										35351
									],
									[
										35388,
										35393
									],
									[
										35417,
										35422
									],
									[
										35602,
										35607
									],
									[
										35650,
										35655
									],
									[
										35705,
										35710
									],
									[
										35729,
										35734
									],
									[
										35739,
										35744
									],
									[
										35793,
										35798
									],
									[
										35809,
										35814
									],
									[
										35926,
										35931
									],
									[
										35939,
										35944
									],
									[
										35988,
										35993
									],
									[
										36001,
										36006
									],
									[
										36096,
										36101
									],
									[
										36106,
										36111
									],
									[
										36161,
										36166
									],
									[
										36171,
										36176
									],
									[
										36228,
										36233
									],
									[
										36281,
										36286
									],
									[
										36351,
										36356
									],
									[
										36383,
										36388
									],
									[
										36493,
										36498
									],
									[
										36550,
										36555
									],
									[
										36602,
										36607
									],
									[
										36632,
										36637
									],
									[
										36675,
										36680
									],
									[
										36709,
										36714
									],
									[
										36741,
										36746
									],
									[
										36845,
										36850
									],
									[
										36853,
										36858
									],
									[
										36944,
										36949
									],
									[
										37003,
										37008
									],
									[
										37111,
										37116
									],
									[
										37141,
										37146
									],
									[
										37220,
										37225
									],
									[
										37246,
										37251
									],
									[
										37418,
										37423
									],
									[
										37624,
										37629
									],
									[
										37907,
										37912
									],
									[
										38282,
										38287
									],
									[
										38370,
										38375
									],
									[
										38588,
										38593
									],
									[
										38630,
										38635
									],
									[
										38965,
										38970
									],
									[
										39340,
										39345
									],
									[
										39690,
										39695
									],
									[
										40022,
										40027
									],
									[
										40032,
										40037
									],
									[
										40412,
										40417
									],
									[
										40695,
										40700
									],
									[
										40860,
										40865
									],
									[
										40881,
										40886
									],
									[
										40902,
										40907
									],
									[
										40942,
										40947
									],
									[
										40963,
										40968
									],
									[
										40984,
										40989
									],
									[
										41025,
										41030
									],
									[
										41046,
										41051
									],
									[
										41067,
										41072
									],
									[
										41111,
										41116
									],
									[
										41132,
										41137
									],
									[
										41153,
										41158
									],
									[
										41286,
										41291
									],
									[
										41442,
										41447
									],
									[
										41505,
										41510
									],
									[
										41544,
										41549
									],
									[
										41662,
										41667
									],
									[
										41801,
										41806
									],
									[
										41838,
										41843
									],
									[
										41917,
										41922
									],
									[
										42009,
										42014
									],
									[
										42091,
										42096
									],
									[
										42247,
										42252
									],
									[
										42334,
										42339
									],
									[
										42422,
										42427
									],
									[
										42495,
										42500
									],
									[
										42688,
										42693
									],
									[
										42768,
										42773
									],
									[
										42861,
										42866
									],
									[
										42939,
										42944
									],
									[
										43125,
										43130
									],
									[
										43200,
										43205
									],
									[
										43291,
										43296
									],
									[
										43366,
										43371
									],
									[
										43695,
										43700
									],
									[
										43730,
										43735
									],
									[
										44123,
										44128
									],
									[
										44381,
										44386
									],
									[
										44470,
										44475
									],
									[
										44556,
										44561
									],
									[
										44702,
										44707
									],
									[
										44860,
										44865
									],
									[
										44891,
										44896
									],
									[
										44980,
										44985
									],
									[
										45021,
										45026
									],
									[
										45315,
										45320
									],
									[
										45571,
										45576
									],
									[
										45663,
										45668
									],
									[
										45707,
										45712
									],
									[
										45969,
										45974
									],
									[
										46152,
										46157
									],
									[
										46384,
										46389
									],
									[
										46586,
										46591
									],
									[
										46811,
										46816
									],
									[
										47100,
										47105
									],
									[
										47169,
										47174
									],
									[
										47415,
										47420
									],
									[
										47538,
										47543
									],
									[
										47758,
										47763
									],
									[
										47925,
										47930
									],
									[
										47976,
										47981
									],
									[
										48068,
										48073
									],
									[
										48135,
										48140
									],
									[
										48329,
										48334
									],
									[
										48561,
										48566
									],
									[
										48862,
										48867
									],
									[
										49014,
										49019
									],
									[
										49055,
										49060
									],
									[
										49147,
										49152
									],
									[
										49211,
										49216
									],
									[
										49442,
										49447
									],
									[
										49719,
										49724
									],
									[
										50013,
										50018
									],
									[
										50187,
										50192
									],
									[
										50256,
										50261
									],
									[
										50399,
										50404
									],
									[
										50441,
										50446
									],
									[
										50541,
										50546
									],
									[
										50606,
										50611
									],
									[
										50810,
										50815
									],
									[
										51101,
										51106
									],
									[
										51417,
										51422
									],
									[
										51591,
										51596
									],
									[
										51668,
										51673
									],
									[
										51762,
										51767
									],
									[
										51971,
										51976
									],
									[
										52192,
										52197
									],
									[
										52512,
										52517
									],
									[
										52789,
										52794
									],
									[
										53039,
										53044
									],
									[
										53180,
										53185
									],
									[
										53522,
										53527
									],
									[
										53559,
										53564
									],
									[
										53714,
										53719
									],
									[
										53767,
										53772
									],
									[
										54117,
										54122
									],
									[
										54340,
										54345
									],
									[
										54481,
										54486
									],
									[
										54502,
										54507
									],
									[
										54685,
										54690
									],
									[
										54882,
										54887
									],
									[
										54903,
										54908
									],
									[
										55237,
										55242
									],
									[
										55449,
										55454
									],
									[
										55471,
										55476
									],
									[
										55526,
										55531
									],
									[
										55682,
										55687
									],
									[
										55882,
										55887
									],
									[
										55904,
										55909
									],
									[
										55959,
										55964
									],
									[
										56175,
										56180
									],
									[
										56248,
										56253
									],
									[
										56343,
										56348
									],
									[
										56417,
										56422
									],
									[
										56475,
										56480
									],
									[
										56873,
										56878
									],
									[
										56983,
										56988
									],
									[
										57044,
										57049
									],
									[
										57211,
										57216
									],
									[
										57456,
										57461
									],
									[
										57466,
										57471
									],
									[
										57830,
										57835
									],
									[
										58196,
										58201
									],
									[
										58306,
										58311
									],
									[
										58492,
										58497
									],
									[
										58502,
										58507
									],
									[
										58614,
										58619
									],
									[
										58624,
										58629
									],
									[
										58740,
										58745
									],
									[
										58750,
										58755
									],
									[
										58925,
										58930
									],
									[
										58935,
										58940
									],
									[
										59292,
										59297
									],
									[
										59485,
										59490
									],
									[
										59510,
										59515
									],
									[
										59723,
										59728
									],
									[
										59822,
										59827
									],
									[
										59857,
										59862
									],
									[
										59892,
										59897
									],
									[
										59927,
										59932
									],
									[
										59970,
										59975
									],
									[
										60295,
										60300
									],
									[
										60486,
										60491
									],
									[
										60557,
										60562
									],
									[
										60613,
										60618
									],
									[
										60768,
										60773
									],
									[
										60919,
										60924
									],
									[
										60933,
										60938
									],
									[
										61244,
										61249
									],
									[
										61258,
										61263
									],
									[
										61369,
										61374
									],
									[
										61380,
										61385
									],
									[
										61403,
										61408
									],
									[
										61477,
										61482
									],
									[
										61632,
										61637
									],
									[
										61811,
										61816
									],
									[
										62030,
										62035
									],
									[
										62186,
										62191
									],
									[
										62422,
										62427
									],
									[
										62440,
										62445
									],
									[
										62470,
										62475
									],
									[
										62537,
										62542
									],
									[
										62692,
										62697
									],
									[
										62889,
										62894
									],
									[
										63114,
										63119
									],
									[
										63264,
										63269
									],
									[
										63432,
										63437
									],
									[
										63442,
										63447
									],
									[
										63473,
										63478
									],
									[
										63546,
										63551
									],
									[
										63630,
										63635
									],
									[
										63644,
										63649
									],
									[
										63675,
										63680
									],
									[
										63753,
										63758
									],
									[
										63842,
										63847
									],
									[
										63856,
										63861
									],
									[
										63887,
										63892
									],
									[
										63965,
										63970
									],
									[
										64071,
										64076
									],
									[
										64094,
										64099
									],
									[
										64214,
										64219
									],
									[
										64298,
										64303
									],
									[
										64312,
										64317
									],
									[
										64448,
										64453
									],
									[
										64528,
										64533
									],
									[
										64598,
										64603
									],
									[
										64817,
										64822
									],
									[
										64925,
										64930
									],
									[
										65164,
										65169
									],
									[
										65496,
										65501
									],
									[
										65661,
										65666
									],
									[
										65874,
										65879
									],
									[
										66201,
										66206
									],
									[
										66514,
										66519
									],
									[
										66748,
										66753
									],
									[
										66935,
										66940
									],
									[
										67263,
										67268
									],
									[
										67570,
										67575
									],
									[
										67590,
										67595
									],
									[
										67615,
										67620
									],
									[
										67631,
										67636
									],
									[
										67651,
										67656
									],
									[
										67673,
										67678
									],
									[
										67888,
										67893
									],
									[
										68018,
										68023
									],
									[
										68248,
										68253
									],
									[
										68414,
										68419
									],
									[
										68652,
										68657
									],
									[
										68848,
										68853
									],
									[
										69117,
										69122
									],
									[
										69406,
										69411
									],
									[
										69446,
										69451
									],
									[
										69534,
										69539
									],
									[
										69811,
										69816
									],
									[
										70094,
										70099
									],
									[
										70297,
										70302
									],
									[
										70544,
										70549
									],
									[
										70787,
										70792
									],
									[
										70947,
										70952
									],
									[
										71038,
										71043
									],
									[
										71303,
										71308
									],
									[
										71473,
										71478
									],
									[
										71803,
										71808
									],
									[
										71823,
										71828
									],
									[
										71848,
										71853
									],
									[
										71864,
										71869
									],
									[
										71884,
										71889
									],
									[
										71906,
										71911
									],
									[
										72121,
										72126
									],
									[
										72251,
										72256
									],
									[
										72481,
										72486
									],
									[
										72642,
										72647
									],
									[
										72880,
										72885
									],
									[
										73050,
										73055
									],
									[
										73319,
										73324
									],
									[
										73600,
										73605
									],
									[
										73640,
										73645
									],
									[
										73860,
										73865
									],
									[
										74143,
										74148
									],
									[
										74313,
										74318
									],
									[
										74526,
										74531
									],
									[
										74764,
										74769
									],
									[
										74916,
										74921
									],
									[
										75078,
										75083
									],
									[
										75169,
										75174
									],
									[
										75436,
										75441
									],
									[
										75606,
										75611
									],
									[
										75903,
										75908
									],
									[
										76190,
										76195
									],
									[
										76230,
										76235
									],
									[
										76310,
										76315
									],
									[
										76460,
										76465
									],
									[
										76618,
										76623
									],
									[
										76709,
										76714
									],
									[
										77038,
										77043
									],
									[
										77078,
										77083
									],
									[
										77150,
										77155
									],
									[
										77302,
										77307
									],
									[
										77462,
										77467
									],
									[
										77553,
										77558
									],
									[
										77955,
										77960
									],
									[
										78113,
										78118
									],
									[
										78125,
										78130
									],
									[
										78526,
										78531
									],
									[
										78546,
										78551
									],
									[
										78571,
										78576
									],
									[
										78587,
										78592
									],
									[
										78607,
										78612
									],
									[
										78629,
										78634
									],
									[
										78855,
										78860
									],
									[
										78863,
										78868
									],
									[
										79120,
										79125
									],
									[
										79251,
										79256
									],
									[
										79381,
										79386
									],
									[
										79703,
										79708
									],
									[
										79715,
										79720
									],
									[
										80002,
										80007
									],
									[
										80166,
										80171
									],
									[
										80342,
										80347
									],
									[
										80380,
										80385
									],
									[
										80537,
										80542
									],
									[
										80809,
										80814
									],
									[
										80937,
										80942
									],
									[
										81162,
										81167
									],
									[
										81326,
										81331
									],
									[
										81407,
										81412
									],
									[
										81582,
										81587
									],
									[
										81665,
										81670
									],
									[
										81989,
										81994
									],
									[
										82001,
										82006
									],
									[
										82314,
										82319
									],
									[
										82326,
										82331
									],
									[
										82697,
										82702
									],
									[
										83047,
										83052
									],
									[
										83292,
										83297
									],
									[
										83332,
										83337
									],
									[
										83404,
										83409
									],
									[
										83487,
										83492
									],
									[
										83668,
										83673
									],
									[
										83755,
										83760
									],
									[
										84024,
										84029
									],
									[
										84219,
										84224
									],
									[
										84415,
										84420
									],
									[
										84553,
										84558
									],
									[
										84658,
										84663
									],
									[
										84694,
										84699
									],
									[
										85072,
										85077
									],
									[
										85186,
										85191
									],
									[
										85472,
										85477
									],
									[
										85641,
										85646
									],
									[
										85896,
										85901
									],
									[
										86138,
										86143
									],
									[
										86370,
										86375
									],
									[
										86736,
										86741
									],
									[
										86785,
										86790
									],
									[
										86840,
										86845
									],
									[
										86978,
										86983
									],
									[
										87236,
										87241
									],
									[
										87387,
										87392
									],
									[
										87402,
										87407
									],
									[
										87422,
										87427
									],
									[
										87444,
										87449
									],
									[
										87605,
										87610
									],
									[
										87613,
										87618
									],
									[
										87894,
										87899
									],
									[
										87916,
										87921
									],
									[
										88187,
										88192
									],
									[
										88370,
										88375
									],
									[
										88576,
										88581
									],
									[
										88813,
										88818
									],
									[
										88996,
										89001
									],
									[
										89202,
										89207
									],
									[
										89538,
										89543
									],
									[
										89705,
										89710
									],
									[
										89832,
										89837
									],
									[
										90214,
										90219
									],
									[
										90341,
										90346
									],
									[
										90665,
										90670
									],
									[
										90859,
										90864
									],
									[
										90956,
										90961
									],
									[
										91140,
										91145
									],
									[
										91383,
										91388
									],
									[
										91518,
										91523
									],
									[
										91615,
										91620
									],
									[
										91882,
										91887
									],
									[
										92196,
										92201
									],
									[
										92428,
										92433
									],
									[
										92661,
										92666
									],
									[
										92919,
										92924
									],
									[
										93075,
										93080
									],
									[
										93368,
										93373
									],
									[
										93488,
										93493
									],
									[
										93608,
										93613
									],
									[
										93728,
										93733
									],
									[
										93848,
										93853
									],
									[
										93968,
										93973
									],
									[
										94088,
										94093
									],
									[
										94208,
										94213
									],
									[
										94328,
										94333
									],
									[
										94551,
										94556
									],
									[
										94807,
										94812
									],
									[
										94828,
										94833
									],
									[
										95063,
										95068
									],
									[
										95084,
										95089
									],
									[
										95323,
										95328
									],
									[
										95555,
										95560
									],
									[
										95742,
										95747
									],
									[
										96027,
										96032
									],
									[
										96268,
										96273
									],
									[
										96388,
										96393
									],
									[
										96508,
										96513
									],
									[
										96701,
										96706
									],
									[
										96803,
										96808
									],
									[
										96840,
										96845
									],
									[
										97007,
										97012
									],
									[
										97036,
										97041
									],
									[
										97147,
										97152
									],
									[
										97205,
										97210
									],
									[
										97309,
										97314
									],
									[
										97366,
										97371
									],
									[
										97678,
										97683
									],
									[
										97763,
										97768
									],
									[
										97807,
										97812
									],
									[
										97844,
										97849
									],
									[
										97904,
										97909
									],
									[
										98007,
										98012
									],
									[
										98068,
										98073
									],
									[
										98319,
										98324
									],
									[
										98380,
										98385
									],
									[
										98424,
										98429
									],
									[
										98497,
										98502
									],
									[
										98518,
										98523
									],
									[
										98684,
										98689
									],
									[
										98813,
										98818
									],
									[
										99021,
										99026
									],
									[
										170979,
										170984
									],
									[
										173137,
										173142
									],
									[
										174253,
										174258
									],
									[
										176418,
										176423
									],
									[
										177585,
										177590
									],
									[
										178799,
										178804
									],
									[
										179966,
										179971
									],
									[
										181180,
										181185
									],
									[
										182890,
										182895
									],
									[
										184910,
										184915
									],
									[
										186076,
										186081
									],
									[
										187290,
										187295
									],
									[
										188456,
										188461
									],
									[
										189670,
										189675
									],
									[
										191379,
										191384
									],
									[
										193940,
										193945
									],
									[
										195107,
										195112
									],
									[
										196321,
										196326
									],
									[
										197488,
										197493
									],
									[
										198702,
										198707
									],
									[
										200412,
										200417
									],
									[
										202432,
										202437
									],
									[
										203598,
										203603
									],
									[
										204812,
										204817
									],
									[
										205978,
										205983
									],
									[
										207192,
										207197
									],
									[
										208901,
										208906
									],
									[
										210921,
										210926
									],
									[
										212087,
										212092
									],
									[
										213301,
										213306
									],
									[
										214467,
										214472
									],
									[
										215681,
										215686
									],
									[
										217390,
										217395
									],
									[
										219410,
										219415
									],
									[
										220576,
										220581
									],
									[
										221790,
										221795
									],
									[
										222956,
										222961
									],
									[
										224170,
										224175
									],
									[
										225879,
										225884
									],
									[
										227899,
										227904
									],
									[
										229065,
										229070
									],
									[
										230279,
										230284
									],
									[
										231445,
										231450
									],
									[
										232659,
										232664
									],
									[
										234368,
										234373
									],
									[
										236931,
										236936
									],
									[
										238099,
										238104
									],
									[
										239313,
										239318
									],
									[
										240481,
										240486
									],
									[
										241695,
										241700
									],
									[
										243406,
										243411
									],
									[
										245426,
										245431
									],
									[
										246592,
										246597
									],
									[
										247806,
										247811
									],
									[
										248972,
										248977
									],
									[
										250186,
										250191
									],
									[
										251895,
										251900
									],
									[
										255186,
										255191
									],
									[
										260523,
										260528
									],
									[
										269332,
										269337
									],
									[
										270798,
										270803
									],
									[
										272234,
										272239
									],
									[
										273714,
										273719
									],
									[
										275158,
										275163
									],
									[
										276640,
										276645
									],
									[
										278086,
										278091
									],
									[
										279568,
										279573
									],
									[
										294936,
										294941
									],
									[
										296655,
										296660
									],
									[
										298380,
										298385
									],
									[
										300105,
										300110
									],
									[
										302380,
										302385
									],
									[
										302638,
										302643
									],
									[
										303025,
										303030
									],
									[
										303303,
										303308
									],
									[
										303862,
										303867
									],
									[
										304091,
										304096
									],
									[
										304369,
										304374
									],
									[
										304690,
										304695
									],
									[
										304948,
										304953
									],
									[
										305311,
										305316
									],
									[
										305383,
										305388
									],
									[
										305674,
										305679
									],
									[
										305980,
										305985
									],
									[
										306287,
										306292
									],
									[
										306594,
										306599
									],
									[
										306912,
										306917
									],
									[
										307231,
										307236
									],
									[
										307550,
										307555
									],
									[
										307868,
										307873
									],
									[
										308187,
										308192
									],
									[
										308522,
										308527
									],
									[
										308740,
										308745
									],
									[
										308975,
										308980
									],
									[
										309025,
										309030
									],
									[
										309055,
										309060
									],
									[
										309104,
										309109
									],
									[
										309264,
										309269
									],
									[
										309335,
										309340
									],
									[
										309395,
										309400
									],
									[
										309456,
										309461
									],
									[
										309531,
										309536
									],
									[
										309564,
										309569
									],
									[
										309717,
										309722
									],
									[
										309725,
										309730
									],
									[
										309952,
										309957
									],
									[
										310020,
										310025
									],
									[
										310112,
										310117
									],
									[
										310298,
										310303
									],
									[
										310698,
										310703
									],
									[
										310809,
										310814
									],
									[
										311347,
										311352
									],
									[
										311593,
										311598
									],
									[
										311935,
										311940
									],
									[
										312369,
										312374
									],
									[
										312608,
										312613
									],
									[
										312891,
										312896
									],
									[
										313124,
										313129
									],
									[
										313412,
										313417
									],
									[
										313523,
										313528
									],
									[
										313752,
										313757
									],
									[
										313985,
										313990
									],
									[
										314096,
										314101
									],
									[
										314325,
										314330
									],
									[
										314789,
										314794
									],
									[
										315224,
										315229
									],
									[
										315463,
										315468
									],
									[
										315669,
										315674
									],
									[
										315872,
										315877
									],
									[
										316139,
										316144
									],
									[
										316470,
										316475
									],
									[
										316725,
										316730
									],
									[
										316979,
										316984
									],
									[
										317358,
										317363
									],
									[
										317375,
										317380
									],
									[
										317708,
										317713
									],
									[
										318027,
										318032
									],
									[
										318155,
										318160
									],
									[
										318284,
										318289
									],
									[
										318378,
										318383
									],
									[
										318411,
										318416
									],
									[
										318445,
										318450
									],
									[
										318569,
										318574
									],
									[
										318614,
										318619
									],
									[
										318681,
										318686
									],
									[
										318724,
										318729
									],
									[
										318763,
										318768
									],
									[
										318814,
										318819
									],
									[
										318861,
										318866
									],
									[
										318909,
										318914
									],
									[
										318953,
										318958
									],
									[
										318998,
										319003
									],
									[
										319039,
										319044
									],
									[
										319089,
										319094
									],
									[
										319135,
										319140
									],
									[
										319184,
										319189
									],
									[
										319229,
										319234
									],
									[
										319278,
										319283
									],
									[
										319323,
										319328
									],
									[
										319372,
										319377
									],
									[
										319417,
										319422
									],
									[
										319469,
										319474
									],
									[
										319517,
										319522
									],
									[
										319566,
										319571
									],
									[
										319611,
										319616
									],
									[
										319660,
										319665
									],
									[
										319752,
										319757
									],
									[
										319922,
										319927
									],
									[
										319969,
										319974
									],
									[
										320222,
										320227
									],
									[
										320345,
										320350
									],
									[
										320383,
										320388
									],
									[
										320419,
										320424
									],
									[
										320475,
										320480
									],
									[
										320595,
										320600
									],
									[
										320651,
										320656
									],
									[
										320674,
										320679
									],
									[
										320709,
										320714
									],
									[
										320768,
										320773
									],
									[
										320886,
										320891
									],
									[
										320974,
										320979
									],
									[
										321023,
										321028
									],
									[
										321092,
										321097
									],
									[
										321181,
										321186
									],
									[
										321200,
										321205
									],
									[
										321326,
										321331
									],
									[
										321722,
										321727
									],
									[
										322089,
										322094
									],
									[
										322247,
										322252
									],
									[
										322524,
										322529
									],
									[
										322593,
										322598
									],
									[
										322660,
										322665
									],
									[
										322727,
										322732
									],
									[
										322794,
										322799
									],
									[
										322861,
										322866
									],
									[
										322928,
										322933
									],
									[
										322995,
										323000
									],
									[
										323062,
										323067
									],
									[
										323129,
										323134
									],
									[
										323196,
										323201
									],
									[
										323263,
										323268
									],
									[
										323330,
										323335
									],
									[
										323397,
										323402
									],
									[
										323464,
										323469
									],
									[
										323531,
										323536
									],
									[
										323598,
										323603
									],
									[
										323665,
										323670
									],
									[
										323732,
										323737
									],
									[
										323799,
										323804
									],
									[
										323866,
										323871
									],
									[
										323933,
										323938
									],
									[
										324000,
										324005
									],
									[
										324067,
										324072
									],
									[
										324134,
										324139
									],
									[
										324201,
										324206
									],
									[
										324268,
										324273
									],
									[
										324335,
										324340
									],
									[
										324402,
										324407
									],
									[
										324469,
										324474
									],
									[
										324536,
										324541
									],
									[
										324603,
										324608
									],
									[
										324670,
										324675
									],
									[
										324737,
										324742
									],
									[
										324804,
										324809
									],
									[
										324871,
										324876
									],
									[
										324944,
										324949
									],
									[
										324986,
										324991
									],
									[
										325052,
										325057
									],
									[
										325089,
										325094
									],
									[
										325162,
										325167
									],
									[
										325233,
										325238
									],
									[
										325304,
										325309
									],
									[
										325375,
										325380
									],
									[
										325428,
										325433
									],
									[
										325479,
										325484
									],
									[
										325505,
										325510
									],
									[
										325632,
										325637
									],
									[
										325657,
										325662
									],
									[
										325693,
										325698
									],
									[
										325766,
										325771
									],
									[
										325837,
										325842
									],
									[
										325908,
										325913
									],
									[
										325984,
										325989
									],
									[
										326029,
										326034
									],
									[
										326055,
										326060
									],
									[
										326162,
										326167
									],
									[
										326199,
										326204
									],
									[
										326272,
										326277
									],
									[
										326343,
										326348
									],
									[
										326414,
										326419
									],
									[
										326485,
										326490
									],
									[
										326556,
										326561
									],
									[
										326627,
										326632
									],
									[
										326698,
										326703
									],
									[
										326769,
										326774
									],
									[
										326840,
										326845
									],
									[
										326911,
										326916
									],
									[
										326982,
										326987
									],
									[
										327053,
										327058
									],
									[
										327124,
										327129
									],
									[
										327195,
										327200
									],
									[
										327266,
										327271
									],
									[
										327337,
										327342
									],
									[
										327408,
										327413
									],
									[
										327479,
										327484
									],
									[
										327550,
										327555
									],
									[
										327621,
										327626
									],
									[
										327692,
										327697
									],
									[
										327769,
										327774
									],
									[
										327814,
										327819
									],
									[
										327840,
										327845
									],
									[
										327902,
										327907
									],
									[
										328004,
										328009
									],
									[
										328078,
										328083
									],
									[
										328133,
										328138
									],
									[
										328218,
										328223
									],
									[
										328251,
										328256
									],
									[
										328330,
										328335
									],
									[
										328388,
										328393
									],
									[
										328454,
										328459
									],
									[
										328503,
										328508
									],
									[
										328554,
										328559
									],
									[
										328602,
										328607
									],
									[
										328652,
										328657
									],
									[
										328690,
										328695
									],
									[
										328827,
										328832
									],
									[
										328939,
										328944
									],
									[
										329045,
										329050
									],
									[
										329082,
										329087
									],
									[
										329165,
										329170
									],
									[
										329216,
										329221
									],
									[
										329267,
										329272
									],
									[
										329320,
										329325
									],
									[
										329375,
										329380
									],
									[
										329413,
										329418
									],
									[
										329572,
										329577
									],
									[
										329687,
										329692
									],
									[
										329797,
										329802
									],
									[
										329849,
										329854
									],
									[
										329934,
										329939
									],
									[
										329987,
										329992
									],
									[
										330040,
										330045
									],
									[
										330095,
										330100
									],
									[
										330235,
										330240
									],
									[
										330372,
										330377
									],
									[
										330386,
										330391
									],
									[
										330490,
										330495
									],
									[
										330505,
										330510
									],
									[
										330554,
										330559
									],
									[
										330623,
										330628
									],
									[
										330691,
										330696
									],
									[
										330776,
										330781
									],
									[
										330831,
										330836
									],
									[
										330883,
										330888
									],
									[
										330938,
										330943
									],
									[
										331008,
										331013
									],
									[
										331112,
										331117
									],
									[
										331173,
										331178
									],
									[
										331260,
										331265
									],
									[
										331286,
										331291
									],
									[
										331546,
										331551
									],
									[
										331726,
										331731
									],
									[
										331899,
										331904
									],
									[
										332134,
										332139
									],
									[
										332194,
										332199
									],
									[
										332352,
										332357
									],
									[
										332620,
										332625
									],
									[
										332819,
										332824
									],
									[
										332993,
										332998
									],
									[
										333242,
										333247
									],
									[
										333331,
										333336
									],
									[
										333378,
										333383
									],
									[
										333451,
										333456
									],
									[
										333520,
										333525
									],
									[
										333689,
										333694
									],
									[
										333971,
										333976
									],
									[
										334246,
										334251
									],
									[
										334385,
										334390
									],
									[
										334451,
										334456
									],
									[
										334598,
										334603
									],
									[
										334645,
										334650
									],
									[
										334718,
										334723
									],
									[
										334788,
										334793
									],
									[
										334901,
										334906
									],
									[
										335068,
										335073
									],
									[
										335252,
										335257
									],
									[
										335528,
										335533
									],
									[
										335617,
										335622
									],
									[
										335664,
										335669
									],
									[
										335737,
										335742
									],
									[
										335806,
										335811
									],
									[
										335910,
										335915
									],
									[
										336022,
										336027
									],
									[
										336064,
										336069
									],
									[
										336156,
										336161
									],
									[
										336225,
										336230
									],
									[
										336379,
										336384
									],
									[
										336567,
										336572
									],
									[
										336634,
										336639
									],
									[
										336842,
										336847
									],
									[
										336908,
										336913
									],
									[
										337055,
										337060
									],
									[
										337102,
										337107
									],
									[
										337175,
										337180
									],
									[
										337245,
										337250
									],
									[
										337358,
										337363
									],
									[
										337500,
										337505
									],
									[
										337561,
										337566
									],
									[
										337739,
										337744
									],
									[
										337978,
										337983
									],
									[
										338067,
										338072
									],
									[
										338114,
										338119
									],
									[
										338192,
										338197
									],
									[
										338261,
										338266
									],
									[
										338448,
										338453
									],
									[
										338732,
										338737
									],
									[
										338926,
										338931
									],
									[
										339112,
										339117
									],
									[
										339182,
										339187
									],
									[
										339248,
										339253
									],
									[
										339405,
										339410
									],
									[
										339452,
										339457
									],
									[
										339530,
										339535
									],
									[
										339600,
										339605
									],
									[
										339718,
										339723
									],
									[
										339895,
										339900
									],
									[
										340090,
										340095
									],
									[
										340333,
										340338
									],
									[
										340380,
										340385
									],
									[
										340454,
										340459
									],
									[
										340523,
										340528
									],
									[
										340777,
										340782
									],
									[
										340933,
										340938
									],
									[
										340973,
										340978
									],
									[
										341030,
										341035
									],
									[
										341108,
										341113
									],
									[
										341234,
										341239
									],
									[
										341300,
										341305
									],
									[
										341446,
										341451
									],
									[
										341493,
										341498
									],
									[
										341567,
										341572
									],
									[
										341637,
										341642
									],
									[
										341751,
										341756
									],
									[
										341920,
										341925
									],
									[
										342109,
										342114
									],
									[
										342347,
										342352
									],
									[
										342379,
										342384
									],
									[
										342449,
										342454
									],
									[
										342518,
										342523
									],
									[
										342711,
										342716
									],
									[
										342958,
										342963
									],
									[
										343209,
										343214
									],
									[
										343357,
										343362
									],
									[
										343423,
										343428
									],
									[
										343551,
										343556
									],
									[
										343583,
										343588
									],
									[
										343653,
										343658
									],
									[
										343723,
										343728
									],
									[
										343833,
										343838
									],
									[
										343994,
										343999
									],
									[
										344137,
										344142
									],
									[
										344215,
										344220
									],
									[
										344440,
										344445
									],
									[
										344487,
										344492
									],
									[
										344564,
										344569
									],
									[
										344633,
										344638
									],
									[
										344779,
										344784
									],
									[
										344949,
										344954
									],
									[
										345112,
										345117
									],
									[
										345233,
										345238
									],
									[
										345349,
										345354
									],
									[
										345465,
										345470
									],
									[
										345594,
										345599
									],
									[
										345634,
										345639
									],
									[
										345684,
										345689
									],
									[
										345825,
										345830
									],
									[
										345891,
										345896
									],
									[
										346046,
										346051
									],
									[
										346093,
										346098
									],
									[
										346170,
										346175
									],
									[
										346240,
										346245
									],
									[
										346357,
										346362
									],
									[
										346532,
										346537
									],
									[
										346694,
										346699
									],
									[
										346712,
										346717
									],
									[
										346932,
										346937
									],
									[
										347100,
										347105
									],
									[
										347137,
										347142
									],
									[
										347159,
										347164
									],
									[
										347373,
										347378
									],
									[
										347544,
										347549
									],
									[
										347567,
										347572
									],
									[
										347825,
										347830
									],
									[
										347972,
										347977
									],
									[
										347989,
										347994
									],
									[
										348213,
										348218
									],
									[
										348373,
										348378
									],
									[
										348397,
										348402
									],
									[
										348472,
										348477
									],
									[
										348541,
										348546
									],
									[
										348612,
										348617
									],
									[
										348731,
										348736
									],
									[
										348833,
										348838
									],
									[
										348878,
										348883
									],
									[
										348987,
										348992
									],
									[
										349250,
										349255
									],
									[
										349402,
										349407
									],
									[
										349434,
										349439
									],
									[
										349489,
										349494
									],
									[
										349534,
										349539
									],
									[
										349548,
										349553
									],
									[
										349751,
										349756
									],
									[
										349802,
										349807
									],
									[
										349862,
										349867
									],
									[
										350089,
										350094
									],
									[
										350124,
										350129
									],
									[
										350232,
										350237
									],
									[
										350500,
										350505
									],
									[
										350705,
										350710
									],
									[
										350749,
										350754
									],
									[
										350944,
										350949
									],
									[
										351039,
										351044
									],
									[
										351105,
										351110
									],
									[
										351154,
										351159
									],
									[
										351216,
										351221
									],
									[
										351264,
										351269
									],
									[
										351331,
										351336
									],
									[
										351502,
										351507
									],
									[
										351652,
										351657
									],
									[
										351708,
										351713
									],
									[
										351742,
										351747
									],
									[
										351927,
										351932
									],
									[
										351974,
										351979
									],
									[
										351988,
										351993
									],
									[
										352042,
										352047
									],
									[
										352083,
										352088
									],
									[
										352123,
										352128
									],
									[
										352170,
										352175
									],
									[
										352229,
										352234
									],
									[
										352321,
										352326
									],
									[
										352519,
										352524
									],
									[
										352567,
										352572
									],
									[
										352769,
										352774
									],
									[
										352827,
										352832
									],
									[
										352855,
										352860
									],
									[
										352924,
										352929
									],
									[
										352965,
										352970
									],
									[
										352979,
										352984
									],
									[
										353098,
										353103
									],
									[
										353158,
										353163
									],
									[
										353223,
										353228
									],
									[
										353248,
										353253
									],
									[
										353491,
										353496
									],
									[
										353521,
										353526
									],
									[
										353568,
										353573
									],
									[
										353632,
										353637
									],
									[
										353685,
										353690
									],
									[
										353806,
										353811
									],
									[
										353853,
										353858
									],
									[
										353911,
										353916
									],
									[
										354005,
										354010
									],
									[
										354318,
										354323
									],
									[
										354442,
										354447
									],
									[
										354487,
										354492
									],
									[
										354520,
										354525
									],
									[
										354559,
										354564
									],
									[
										354611,
										354616
									],
									[
										354697,
										354702
									],
									[
										354770,
										354775
									],
									[
										354819,
										354824
									],
									[
										354879,
										354884
									],
									[
										354978,
										354983
									],
									[
										355212,
										355217
									],
									[
										355284,
										355289
									],
									[
										355365,
										355370
									],
									[
										355428,
										355433
									],
									[
										355555,
										355560
									],
									[
										355619,
										355624
									],
									[
										355685,
										355690
									],
									[
										355740,
										355745
									],
									[
										355791,
										355796
									],
									[
										355855,
										355860
									],
									[
										355909,
										355914
									],
									[
										356076,
										356081
									],
									[
										356249,
										356254
									],
									[
										356363,
										356368
									],
									[
										356483,
										356488
									],
									[
										356525,
										356530
									],
									[
										356572,
										356577
									],
									[
										356607,
										356612
									],
									[
										356691,
										356696
									],
									[
										356715,
										356720
									],
									[
										356761,
										356766
									],
									[
										356787,
										356792
									],
									[
										356844,
										356849
									],
									[
										356890,
										356895
									],
									[
										356916,
										356921
									],
									[
										356975,
										356980
									],
									[
										356989,
										356994
									],
									[
										357020,
										357025
									],
									[
										357195,
										357200
									],
									[
										357323,
										357328
									],
									[
										357384,
										357389
									],
									[
										357414,
										357419
									],
									[
										357601,
										357606
									],
									[
										357835,
										357840
									],
									[
										357865,
										357870
									],
									[
										357977,
										357982
									],
									[
										358023,
										358028
									],
									[
										358208,
										358213
									],
									[
										358238,
										358243
									],
									[
										358453,
										358458
									],
									[
										358511,
										358516
									],
									[
										358541,
										358546
									],
									[
										358690,
										358695
									],
									[
										358815,
										358820
									],
									[
										358936,
										358941
									],
									[
										358966,
										358971
									],
									[
										359182,
										359187
									],
									[
										359285,
										359290
									],
									[
										359354,
										359359
									],
									[
										359384,
										359389
									],
									[
										359697,
										359702
									],
									[
										359761,
										359766
									],
									[
										360090,
										360095
									],
									[
										360144,
										360149
									],
									[
										360164,
										360169
									],
									[
										360262,
										360267
									],
									[
										360324,
										360329
									],
									[
										360656,
										360661
									],
									[
										360710,
										360715
									],
									[
										360730,
										360735
									],
									[
										360840,
										360845
									],
									[
										360884,
										360889
									],
									[
										360983,
										360988
									],
									[
										361301,
										361306
									],
									[
										361331,
										361336
									],
									[
										361462,
										361467
									],
									[
										361690,
										361695
									],
									[
										361759,
										361764
									],
									[
										361789,
										361794
									],
									[
										361954,
										361959
									],
									[
										362015,
										362020
									],
									[
										362045,
										362050
									],
									[
										362128,
										362133
									],
									[
										362280,
										362285
									],
									[
										362312,
										362317
									],
									[
										362367,
										362372
									],
									[
										362418,
										362423
									],
									[
										362508,
										362513
									],
									[
										362561,
										362566
									],
									[
										362644,
										362649
									],
									[
										362830,
										362835
									],
									[
										363023,
										363028
									],
									[
										363182,
										363187
									],
									[
										363229,
										363234
									],
									[
										363286,
										363291
									],
									[
										363495,
										363500
									],
									[
										363714,
										363719
									],
									[
										363918,
										363923
									],
									[
										364056,
										364061
									],
									[
										364103,
										364108
									],
									[
										364166,
										364171
									],
									[
										364277,
										364282
									],
									[
										364490,
										364495
									],
									[
										364539,
										364544
									],
									[
										364642,
										364647
									],
									[
										364708,
										364713
									],
									[
										364758,
										364763
									],
									[
										364838,
										364843
									],
									[
										365015,
										365020
									],
									[
										365201,
										365206
									],
									[
										365248,
										365253
									],
									[
										365306,
										365311
									],
									[
										365524,
										365529
									],
									[
										365744,
										365749
									],
									[
										365841,
										365846
									],
									[
										365924,
										365929
									],
									[
										365973,
										365978
									],
									[
										366034,
										366039
									],
									[
										366143,
										366148
									],
									[
										366313,
										366318
									],
									[
										366463,
										366468
									],
									[
										366528,
										366533
									],
									[
										366666,
										366671
									],
									[
										366713,
										366718
									],
									[
										366822,
										366827
									],
									[
										366932,
										366937
									],
									[
										367073,
										367078
									],
									[
										367184,
										367189
									],
									[
										367296,
										367301
									],
									[
										367596,
										367601
									],
									[
										367742,
										367747
									],
									[
										367756,
										367761
									],
									[
										367810,
										367815
									],
									[
										367880,
										367885
									],
									[
										367979,
										367984
									],
									[
										368011,
										368016
									],
									[
										368055,
										368060
									],
									[
										368270,
										368275
									],
									[
										368514,
										368519
									],
									[
										368528,
										368533
									],
									[
										368579,
										368584
									],
									[
										368646,
										368651
									],
									[
										368919,
										368924
									],
									[
										368951,
										368956
									],
									[
										368998,
										369003
									],
									[
										369217,
										369222
									],
									[
										369399,
										369404
									],
									[
										369464,
										369469
									],
									[
										369511,
										369516
									],
									[
										369620,
										369625
									],
									[
										369730,
										369735
									],
									[
										369871,
										369876
									],
									[
										369976,
										369981
									],
									[
										370082,
										370087
									],
									[
										370339,
										370344
									],
									[
										370371,
										370376
									],
									[
										370678,
										370683
									],
									[
										370896,
										370901
									],
									[
										370928,
										370933
									],
									[
										371234,
										371239
									],
									[
										371301,
										371306
									],
									[
										371443,
										371448
									],
									[
										371475,
										371480
									],
									[
										371794,
										371799
									],
									[
										371861,
										371866
									],
									[
										372010,
										372015
									],
									[
										372076,
										372081
									],
									[
										372123,
										372128
									],
									[
										372232,
										372237
									],
									[
										372342,
										372347
									],
									[
										372483,
										372488
									],
									[
										372594,
										372599
									],
									[
										372706,
										372711
									],
									[
										372924,
										372929
									],
									[
										372956,
										372961
									],
									[
										373307,
										373312
									],
									[
										373372,
										373377
									],
									[
										373616,
										373621
									],
									[
										373947,
										373952
									],
									[
										374014,
										374019
									],
									[
										374154,
										374159
									],
									[
										374242,
										374247
									],
									[
										374624,
										374629
									],
									[
										374716,
										374721
									],
									[
										374749,
										374754
									],
									[
										374866,
										374871
									],
									[
										374899,
										374904
									],
									[
										374953,
										374958
									],
									[
										374979,
										374984
									],
									[
										375193,
										375198
									],
									[
										375250,
										375255
									],
									[
										375268,
										375273
									],
									[
										375411,
										375416
									],
									[
										375522,
										375527
									],
									[
										375765,
										375770
									],
									[
										375812,
										375817
									],
									[
										375873,
										375878
									],
									[
										376052,
										376057
									],
									[
										376274,
										376279
									],
									[
										376318,
										376323
									],
									[
										376336,
										376341
									],
									[
										376479,
										376484
									],
									[
										376587,
										376592
									],
									[
										376820,
										376825
									],
									[
										376867,
										376872
									],
									[
										376927,
										376932
									],
									[
										377049,
										377054
									],
									[
										377272,
										377277
									],
									[
										377316,
										377321
									],
									[
										377334,
										377339
									],
									[
										377477,
										377482
									],
									[
										377520,
										377525
									],
									[
										377797,
										377802
									],
									[
										377834,
										377839
									],
									[
										377854,
										377859
									],
									[
										377902,
										377907
									],
									[
										377949,
										377954
									],
									[
										377983,
										377988
									],
									[
										378033,
										378038
									],
									[
										378096,
										378101
									],
									[
										378127,
										378132
									],
									[
										378179,
										378184
									],
									[
										378241,
										378246
									],
									[
										378272,
										378277
									],
									[
										378324,
										378329
									],
									[
										378386,
										378391
									],
									[
										378420,
										378425
									],
									[
										378471,
										378476
									],
									[
										378535,
										378540
									],
									[
										378566,
										378571
									],
									[
										378619,
										378624
									],
									[
										378682,
										378687
									],
									[
										378713,
										378718
									],
									[
										378766,
										378771
									],
									[
										378884,
										378889
									],
									[
										378900,
										378905
									],
									[
										378930,
										378935
									],
									[
										378977,
										378982
									],
									[
										379019,
										379024
									],
									[
										379064,
										379069
									],
									[
										379100,
										379105
									],
									[
										379263,
										379268
									],
									[
										379306,
										379311
									],
									[
										379582,
										379587
									],
									[
										379619,
										379624
									],
									[
										379639,
										379644
									],
									[
										379687,
										379692
									],
									[
										379734,
										379739
									],
									[
										379768,
										379773
									],
									[
										379818,
										379823
									],
									[
										379881,
										379886
									],
									[
										379912,
										379917
									],
									[
										379964,
										379969
									],
									[
										380026,
										380031
									],
									[
										380057,
										380062
									],
									[
										380109,
										380114
									],
									[
										380171,
										380176
									],
									[
										380205,
										380210
									],
									[
										380256,
										380261
									],
									[
										380320,
										380325
									],
									[
										380351,
										380356
									],
									[
										380404,
										380409
									],
									[
										380467,
										380472
									],
									[
										380498,
										380503
									],
									[
										380551,
										380556
									],
									[
										380670,
										380675
									],
									[
										380686,
										380691
									],
									[
										380716,
										380721
									],
									[
										380763,
										380768
									],
									[
										380805,
										380810
									],
									[
										380850,
										380855
									],
									[
										380886,
										380891
									],
									[
										381122,
										381127
									],
									[
										381191,
										381196
									],
									[
										381252,
										381257
									],
									[
										381277,
										381282
									],
									[
										381590,
										381595
									],
									[
										381607,
										381612
									],
									[
										381716,
										381721
									],
									[
										381733,
										381738
									],
									[
										381857,
										381862
									],
									[
										381874,
										381879
									],
									[
										381949,
										381954
									],
									[
										381980,
										381985
									],
									[
										382057,
										382062
									],
									[
										382081,
										382086
									],
									[
										382126,
										382131
									],
									[
										382252,
										382257
									],
									[
										382356,
										382361
									],
									[
										382501,
										382506
									],
									[
										382576,
										382581
									],
									[
										382767,
										382772
									],
									[
										382962,
										382967
									],
									[
										383113,
										383118
									],
									[
										383195,
										383200
									],
									[
										383278,
										383283
									],
									[
										383415,
										383420
									],
									[
										383489,
										383494
									],
									[
										383820,
										383825
									],
									[
										384069,
										384074
									],
									[
										384409,
										384414
									],
									[
										384639,
										384644
									],
									[
										384969,
										384974
									],
									[
										385201,
										385206
									],
									[
										385436,
										385441
									],
									[
										385616,
										385621
									],
									[
										385760,
										385765
									],
									[
										385939,
										385944
									],
									[
										385994,
										385999
									],
									[
										386057,
										386062
									],
									[
										386103,
										386108
									],
									[
										386151,
										386156
									],
									[
										386196,
										386201
									],
									[
										386246,
										386251
									],
									[
										386281,
										386286
									],
									[
										386399,
										386404
									],
									[
										386590,
										386595
									],
									[
										386638,
										386643
									],
									[
										386686,
										386691
									],
									[
										386736,
										386741
									],
									[
										386791,
										386796
									],
									[
										386826,
										386831
									],
									[
										386949,
										386954
									],
									[
										387142,
										387147
									],
									[
										387192,
										387197
									],
									[
										387242,
										387247
									],
									[
										387294,
										387299
									],
									[
										387411,
										387416
									],
									[
										387643,
										387648
									],
									[
										387974,
										387979
									],
									[
										388119,
										388124
									],
									[
										388194,
										388199
									],
									[
										388384,
										388389
									],
									[
										388553,
										388558
									],
									[
										388589,
										388594
									],
									[
										388617,
										388622
									],
									[
										388856,
										388861
									],
									[
										389195,
										389200
									],
									[
										389322,
										389327
									],
									[
										389560,
										389565
									],
									[
										389858,
										389863
									],
									[
										390045,
										390050
									],
									[
										390358,
										390363
									],
									[
										390564,
										390569
									],
									[
										390643,
										390648
									],
									[
										390960,
										390965
									],
									[
										391105,
										391110
									],
									[
										391180,
										391185
									],
									[
										391380,
										391385
									],
									[
										391397,
										391402
									],
									[
										391521,
										391526
									],
									[
										391538,
										391543
									],
									[
										391561,
										391566
									],
									[
										391589,
										391594
									],
									[
										391702,
										391707
									],
									[
										392035,
										392040
									],
									[
										392223,
										392228
									],
									[
										392489,
										392494
									],
									[
										392741,
										392746
									],
									[
										392921,
										392926
									],
									[
										393199,
										393204
									],
									[
										393483,
										393488
									],
									[
										393561,
										393566
									],
									[
										393586,
										393591
									],
									[
										393892,
										393897
									],
									[
										393909,
										393914
									],
									[
										394058,
										394063
									],
									[
										394075,
										394080
									],
									[
										394186,
										394191
									],
									[
										394475,
										394480
									],
									[
										394627,
										394632
									],
									[
										394711,
										394716
									],
									[
										394736,
										394741
									],
									[
										394993,
										394998
									],
									[
										395138,
										395143
									],
									[
										395213,
										395218
									],
									[
										395348,
										395353
									],
									[
										395645,
										395650
									],
									[
										395869,
										395874
									],
									[
										395886,
										395891
									],
									[
										396025,
										396030
									],
									[
										396042,
										396047
									],
									[
										396192,
										396197
									],
									[
										396209,
										396214
									],
									[
										396284,
										396289
									],
									[
										396315,
										396320
									],
									[
										396392,
										396397
									],
									[
										396416,
										396421
									],
									[
										396461,
										396466
									],
									[
										396616,
										396621
									],
									[
										396721,
										396726
									],
									[
										396920,
										396925
									],
									[
										397056,
										397061
									],
									[
										397101,
										397106
									],
									[
										397183,
										397188
									],
									[
										397267,
										397272
									],
									[
										397397,
										397402
									],
									[
										397453,
										397458
									],
									[
										397485,
										397490
									],
									[
										397762,
										397767
									],
									[
										398099,
										398104
									],
									[
										398358,
										398363
									],
									[
										398652,
										398657
									],
									[
										398901,
										398906
									],
									[
										399167,
										399172
									],
									[
										399381,
										399386
									],
									[
										399430,
										399435
									],
									[
										399500,
										399505
									],
									[
										399661,
										399666
									],
									[
										399805,
										399810
									],
									[
										400091,
										400096
									],
									[
										400328,
										400333
									],
									[
										400620,
										400625
									],
									[
										400873,
										400878
									],
									[
										400948,
										400953
									],
									[
										400973,
										400978
									],
									[
										401232,
										401237
									],
									[
										401377,
										401382
									],
									[
										401452,
										401457
									],
									[
										401587,
										401592
									],
									[
										401896,
										401901
									],
									[
										401913,
										401918
									],
									[
										402122,
										402127
									],
									[
										402139,
										402144
									],
									[
										402250,
										402255
									],
									[
										402267,
										402272
									],
									[
										402393,
										402398
									],
									[
										402410,
										402415
									],
									[
										402485,
										402490
									],
									[
										402516,
										402521
									],
									[
										402593,
										402598
									],
									[
										402617,
										402622
									],
									[
										402662,
										402667
									],
									[
										402788,
										402793
									],
									[
										402893,
										402898
									],
									[
										403094,
										403099
									],
									[
										403307,
										403312
									],
									[
										403391,
										403396
									],
									[
										403477,
										403482
									],
									[
										403609,
										403614
									],
									[
										403667,
										403672
									],
									[
										403716,
										403721
									],
									[
										404010,
										404015
									],
									[
										404347,
										404352
									],
									[
										404694,
										404699
									],
									[
										404924,
										404929
									],
									[
										405261,
										405266
									],
									[
										405527,
										405532
									],
									[
										405741,
										405746
									],
									[
										405790,
										405795
									],
									[
										405862,
										405867
									],
									[
										406043,
										406048
									],
									[
										406187,
										406192
									],
									[
										406479,
										406484
									],
									[
										406716,
										406721
									],
									[
										407014,
										407019
									],
									[
										407267,
										407272
									],
									[
										407340,
										407345
									],
									[
										407365,
										407370
									],
									[
										407618,
										407623
									],
									[
										407763,
										407768
									],
									[
										407838,
										407843
									],
									[
										408015,
										408020
									],
									[
										408032,
										408037
									],
									[
										408164,
										408169
									],
									[
										408181,
										408186
									],
									[
										408322,
										408327
									],
									[
										408550,
										408555
									],
									[
										408730,
										408735
									],
									[
										409044,
										409049
									],
									[
										409246,
										409251
									],
									[
										409558,
										409563
									],
									[
										409759,
										409764
									],
									[
										410083,
										410088
									],
									[
										410283,
										410288
									],
									[
										410502,
										410507
									],
									[
										410885,
										410890
									],
									[
										411140,
										411145
									],
									[
										411179,
										411184
									],
									[
										411456,
										411461
									],
									[
										411601,
										411606
									],
									[
										411676,
										411681
									],
									[
										411808,
										411813
									],
									[
										411857,
										411862
									],
									[
										411913,
										411918
									],
									[
										412099,
										412104
									],
									[
										412268,
										412273
									],
									[
										412616,
										412621
									],
									[
										412761,
										412766
									],
									[
										412836,
										412841
									],
									[
										412928,
										412933
									],
									[
										413127,
										413132
									],
									[
										413410,
										413415
									],
									[
										413673,
										413678
									],
									[
										413780,
										413785
									],
									[
										413890,
										413895
									],
									[
										414000,
										414005
									],
									[
										414153,
										414158
									],
									[
										414348,
										414353
									],
									[
										414428,
										414433
									],
									[
										414655,
										414660
									],
									[
										414839,
										414844
									],
									[
										415132,
										415137
									],
									[
										415277,
										415282
									],
									[
										415352,
										415357
									],
									[
										415534,
										415539
									],
									[
										415665,
										415670
									],
									[
										415787,
										415792
									],
									[
										416024,
										416029
									],
									[
										416199,
										416204
									],
									[
										416498,
										416503
									],
									[
										416643,
										416648
									],
									[
										416718,
										416723
									],
									[
										416900,
										416905
									],
									[
										416988,
										416993
									],
									[
										417109,
										417114
									],
									[
										417346,
										417351
									],
									[
										417521,
										417526
									],
									[
										417820,
										417825
									],
									[
										417965,
										417970
									],
									[
										418040,
										418045
									],
									[
										418222,
										418227
									],
									[
										418353,
										418358
									],
									[
										418475,
										418480
									],
									[
										418818,
										418823
									],
									[
										418994,
										418999
									],
									[
										419299,
										419304
									],
									[
										419444,
										419449
									],
									[
										419519,
										419524
									],
									[
										419701,
										419706
									],
									[
										419789,
										419794
									],
									[
										419910,
										419915
									],
									[
										420253,
										420258
									],
									[
										420429,
										420434
									],
									[
										420658,
										420663
									],
									[
										420952,
										420957
									],
									[
										421097,
										421102
									],
									[
										421172,
										421177
									],
									[
										421311,
										421316
									],
									[
										421484,
										421489
									],
									[
										421591,
										421596
									],
									[
										421747,
										421752
									],
									[
										421811,
										421816
									],
									[
										421912,
										421917
									],
									[
										422221,
										422226
									],
									[
										422366,
										422371
									],
									[
										422441,
										422446
									],
									[
										422580,
										422585
									],
									[
										422753,
										422758
									],
									[
										422861,
										422866
									],
									[
										423146,
										423151
									],
									[
										423210,
										423215
									],
									[
										423312,
										423317
									],
									[
										423663,
										423668
									],
									[
										423840,
										423845
									],
									[
										424043,
										424048
									],
									[
										424263,
										424268
									],
									[
										424489,
										424494
									],
									[
										424734,
										424739
									],
									[
										424794,
										424799
									],
									[
										425050,
										425055
									],
									[
										425110,
										425115
									],
									[
										425456,
										425461
									],
									[
										425599,
										425604
									],
									[
										425735,
										425740
									],
									[
										425890,
										425895
									],
									[
										426056,
										426061
									],
									[
										426279,
										426284
									],
									[
										426500,
										426505
									],
									[
										426514,
										426519
									],
									[
										426722,
										426727
									],
									[
										426784,
										426789
									],
									[
										427022,
										427027
									],
									[
										427129,
										427134
									],
									[
										427238,
										427243
									],
									[
										427327,
										427332
									],
									[
										427601,
										427606
									],
									[
										427723,
										427728
									],
									[
										427799,
										427804
									],
									[
										427953,
										427958
									],
									[
										427986,
										427991
									],
									[
										428033,
										428038
									],
									[
										428119,
										428124
									],
									[
										428163,
										428168
									],
									[
										428194,
										428199
									],
									[
										428308,
										428313
									],
									[
										428382,
										428387
									],
									[
										428428,
										428433
									],
									[
										428689,
										428694
									],
									[
										428958,
										428963
									],
									[
										429227,
										429232
									],
									[
										429368,
										429373
									],
									[
										429390,
										429395
									],
									[
										429707,
										429712
									],
									[
										429989,
										429994
									],
									[
										430014,
										430019
									],
									[
										430251,
										430256
									],
									[
										430554,
										430559
									],
									[
										430579,
										430584
									],
									[
										430829,
										430834
									],
									[
										430854,
										430859
									],
									[
										431092,
										431097
									],
									[
										431117,
										431122
									],
									[
										431377,
										431382
									],
									[
										431402,
										431407
									],
									[
										431677,
										431682
									],
									[
										431744,
										431749
									],
									[
										431893,
										431898
									],
									[
										432043,
										432048
									],
									[
										432255,
										432260
									],
									[
										432400,
										432405
									],
									[
										432546,
										432551
									],
									[
										432843,
										432848
									],
									[
										432911,
										432916
									],
									[
										433062,
										433067
									],
									[
										433214,
										433219
									],
									[
										433429,
										433434
									],
									[
										433582,
										433587
									],
									[
										433736,
										433741
									],
									[
										434056,
										434061
									],
									[
										434335,
										434340
									],
									[
										434569,
										434574
									],
									[
										434833,
										434838
									],
									[
										435117,
										435122
									],
									[
										435467,
										435472
									],
									[
										435599,
										435604
									],
									[
										435920,
										435925
									],
									[
										435947,
										435952
									],
									[
										436126,
										436131
									],
									[
										436177,
										436182
									],
									[
										436531,
										436536
									],
									[
										436542,
										436547
									],
									[
										436753,
										436758
									],
									[
										436864,
										436869
									],
									[
										437104,
										437109
									],
									[
										437455,
										437460
									],
									[
										437811,
										437816
									],
									[
										438059,
										438064
									],
									[
										438247,
										438252
									],
									[
										438481,
										438486
									],
									[
										438655,
										438660
									],
									[
										438919,
										438924
									],
									[
										439277,
										439282
									],
									[
										439498,
										439503
									],
									[
										439541,
										439546
									],
									[
										439612,
										439617
									],
									[
										439759,
										439764
									],
									[
										439975,
										439980
									],
									[
										439992,
										439997
									],
									[
										440182,
										440187
									],
									[
										440201,
										440206
									],
									[
										440311,
										440316
									],
									[
										440348,
										440353
									],
									[
										440515,
										440520
									],
									[
										440544,
										440549
									],
									[
										440655,
										440660
									],
									[
										440713,
										440718
									],
									[
										440817,
										440822
									],
									[
										440874,
										440879
									],
									[
										441186,
										441191
									],
									[
										441221,
										441226
									],
									[
										441248,
										441253
									],
									[
										441285,
										441290
									],
									[
										441535,
										441540
									],
									[
										441555,
										441560
									],
									[
										441578,
										441583
									],
									[
										441600,
										441605
									],
									[
										441748,
										441753
									],
									[
										441837,
										441842
									],
									[
										441977,
										441982
									],
									[
										442038,
										442043
									],
									[
										442075,
										442080
									],
									[
										442105,
										442110
									],
									[
										442137,
										442142
									],
									[
										442197,
										442202
									],
									[
										442245,
										442250
									],
									[
										442309,
										442314
									],
									[
										442381,
										442386
									],
									[
										442419,
										442424
									],
									[
										442488,
										442493
									],
									[
										442543,
										442548
									],
									[
										442577,
										442582
									],
									[
										442669,
										442674
									],
									[
										442806,
										442811
									],
									[
										443042,
										443047
									],
									[
										443061,
										443066
									],
									[
										443091,
										443096
									],
									[
										443192,
										443197
									],
									[
										443212,
										443217
									],
									[
										443229,
										443234
									],
									[
										443487,
										443492
									],
									[
										443501,
										443506
									],
									[
										443550,
										443555
									],
									[
										443564,
										443569
									],
									[
										443878,
										443883
									],
									[
										443895,
										443900
									],
									[
										443944,
										443949
									],
									[
										443961,
										443966
									],
									[
										444268,
										444273
									],
									[
										444283,
										444288
									],
									[
										444329,
										444334
									],
									[
										444344,
										444349
									],
									[
										444516,
										444521
									],
									[
										444549,
										444554
									],
									[
										444630,
										444635
									],
									[
										444723,
										444728
									],
									[
										444750,
										444755
									],
									[
										444803,
										444808
									],
									[
										444904,
										444909
									],
									[
										444963,
										444968
									],
									[
										444989,
										444994
									],
									[
										445025,
										445030
									],
									[
										445079,
										445084
									],
									[
										445113,
										445118
									],
									[
										445149,
										445154
									],
									[
										445230,
										445235
									],
									[
										445323,
										445328
									],
									[
										445343,
										445348
									],
									[
										445399,
										445404
									],
									[
										445430,
										445435
									],
									[
										445529,
										445534
									],
									[
										445620,
										445625
									],
									[
										445647,
										445652
									],
									[
										445694,
										445699
									],
									[
										445875,
										445880
									],
									[
										445924,
										445929
									],
									[
										446079,
										446084
									],
									[
										446382,
										446387
									],
									[
										446609,
										446614
									],
									[
										446775,
										446780
									],
									[
										446801,
										446806
									],
									[
										447040,
										447045
									],
									[
										447299,
										447304
									],
									[
										447325,
										447330
									],
									[
										447596,
										447601
									],
									[
										447621,
										447626
									],
									[
										447668,
										447673
									],
									[
										447685,
										447690
									],
									[
										447911,
										447916
									],
									[
										448129,
										448134
									],
									[
										448169,
										448174
									],
									[
										448194,
										448199
									],
									[
										448278,
										448283
									],
									[
										448310,
										448315
									],
									[
										448333,
										448338
									],
									[
										448364,
										448369
									],
									[
										448584,
										448589
									],
									[
										448593,
										448598
									],
									[
										448704,
										448709
									],
									[
										448756,
										448761
									],
									[
										448777,
										448782
									],
									[
										448835,
										448840
									],
									[
										448961,
										448966
									],
									[
										449156,
										449161
									],
									[
										449173,
										449178
									],
									[
										449343,
										449348
									],
									[
										449360,
										449365
									],
									[
										449941,
										449946
									],
									[
										450015,
										450020
									],
									[
										450059,
										450064
									],
									[
										450121,
										450126
									],
									[
										450194,
										450199
									],
									[
										450268,
										450273
									],
									[
										450315,
										450320
									],
									[
										450401,
										450406
									],
									[
										450411,
										450416
									],
									[
										450505,
										450510
									],
									[
										450520,
										450525
									],
									[
										450584,
										450589
									],
									[
										450625,
										450630
									],
									[
										450653,
										450658
									],
									[
										450851,
										450856
									],
									[
										450950,
										450955
									],
									[
										451019,
										451024
									],
									[
										451057,
										451062
									],
									[
										451098,
										451103
									],
									[
										451114,
										451119
									],
									[
										451231,
										451236
									],
									[
										451244,
										451249
									],
									[
										451293,
										451298
									],
									[
										451306,
										451311
									],
									[
										451401,
										451406
									],
									[
										451411,
										451416
									],
									[
										451466,
										451471
									],
									[
										451476,
										451481
									],
									[
										451533,
										451538
									],
									[
										451586,
										451591
									],
									[
										451726,
										451731
									],
									[
										451783,
										451788
									],
									[
										451835,
										451840
									],
									[
										451865,
										451870
									],
									[
										451898,
										451903
									],
									[
										451930,
										451935
									],
									[
										452034,
										452039
									],
									[
										452042,
										452047
									],
									[
										452133,
										452138
									],
									[
										452192,
										452197
									],
									[
										452300,
										452305
									],
									[
										452330,
										452335
									],
									[
										452409,
										452414
									],
									[
										452435,
										452440
									],
									[
										452593,
										452598
									],
									[
										452626,
										452631
									],
									[
										452645,
										452650
									],
									[
										452787,
										452792
									],
									[
										452808,
										452813
									],
									[
										452997,
										453002
									],
									[
										453055,
										453060
									],
									[
										453073,
										453078
									],
									[
										453267,
										453272
									],
									[
										453321,
										453326
									],
									[
										453364,
										453369
									],
									[
										453446,
										453451
									],
									[
										453475,
										453480
									],
									[
										453497,
										453502
									],
									[
										453717,
										453722
									],
									[
										453943,
										453948
									],
									[
										454226,
										454231
									],
									[
										454601,
										454606
									],
									[
										454689,
										454694
									],
									[
										454907,
										454912
									],
									[
										454949,
										454954
									],
									[
										455284,
										455289
									],
									[
										455695,
										455700
									],
									[
										455961,
										455966
									],
									[
										456274,
										456279
									],
									[
										456498,
										456503
									],
									[
										456825,
										456830
									],
									[
										457138,
										457143
									],
									[
										457392,
										457397
									],
									[
										457562,
										457567
									],
									[
										457906,
										457911
									],
									[
										458213,
										458218
									],
									[
										458233,
										458238
									],
									[
										458258,
										458263
									],
									[
										458274,
										458279
									],
									[
										458294,
										458299
									],
									[
										458316,
										458321
									],
									[
										458531,
										458536
									],
									[
										458661,
										458666
									],
									[
										458891,
										458896
									],
									[
										459057,
										459062
									],
									[
										459295,
										459300
									],
									[
										459491,
										459496
									],
									[
										459760,
										459765
									],
									[
										460049,
										460054
									],
									[
										460089,
										460094
									],
									[
										460177,
										460182
									],
									[
										460454,
										460459
									],
									[
										460737,
										460742
									],
									[
										460940,
										460945
									],
									[
										461187,
										461192
									],
									[
										461430,
										461435
									],
									[
										461590,
										461595
									],
									[
										461681,
										461686
									],
									[
										461946,
										461951
									],
									[
										462116,
										462121
									],
									[
										462446,
										462451
									],
									[
										462466,
										462471
									],
									[
										462491,
										462496
									],
									[
										462507,
										462512
									],
									[
										462527,
										462532
									],
									[
										462549,
										462554
									],
									[
										462764,
										462769
									],
									[
										462894,
										462899
									],
									[
										463124,
										463129
									],
									[
										463285,
										463290
									],
									[
										463523,
										463528
									],
									[
										463693,
										463698
									],
									[
										463962,
										463967
									],
									[
										464243,
										464248
									],
									[
										464283,
										464288
									],
									[
										464503,
										464508
									],
									[
										464786,
										464791
									],
									[
										464956,
										464961
									],
									[
										465169,
										465174
									],
									[
										465407,
										465412
									],
									[
										465559,
										465564
									],
									[
										465721,
										465726
									],
									[
										465812,
										465817
									],
									[
										466079,
										466084
									],
									[
										466249,
										466254
									],
									[
										466566,
										466571
									],
									[
										466853,
										466858
									],
									[
										466893,
										466898
									],
									[
										466973,
										466978
									],
									[
										467123,
										467128
									],
									[
										467281,
										467286
									],
									[
										467372,
										467377
									],
									[
										467701,
										467706
									],
									[
										467741,
										467746
									],
									[
										467813,
										467818
									],
									[
										467965,
										467970
									],
									[
										468125,
										468130
									],
									[
										468216,
										468221
									],
									[
										468638,
										468643
									],
									[
										468794,
										468799
									],
									[
										468806,
										468811
									],
									[
										469207,
										469212
									],
									[
										469227,
										469232
									],
									[
										469252,
										469257
									],
									[
										469268,
										469273
									],
									[
										469288,
										469293
									],
									[
										469310,
										469315
									],
									[
										469549,
										469554
									],
									[
										469557,
										469562
									],
									[
										469814,
										469819
									],
									[
										469943,
										469948
									],
									[
										470073,
										470078
									],
									[
										470384,
										470389
									],
									[
										470396,
										470401
									],
									[
										470707,
										470712
									],
									[
										470877,
										470882
									],
									[
										471061,
										471066
									],
									[
										471101,
										471106
									],
									[
										471266,
										471271
									],
									[
										471548,
										471553
									],
									[
										471680,
										471685
									],
									[
										471915,
										471920
									],
									[
										472087,
										472092
									],
									[
										472170,
										472175
									],
									[
										472353,
										472358
									],
									[
										472440,
										472445
									],
									[
										472796,
										472801
									],
									[
										473146,
										473151
									],
									[
										473391,
										473396
									],
									[
										473431,
										473436
									],
									[
										473503,
										473508
									],
									[
										473586,
										473591
									],
									[
										473767,
										473772
									],
									[
										473854,
										473859
									],
									[
										474143,
										474148
									],
									[
										474338,
										474343
									],
									[
										474534,
										474539
									],
									[
										474672,
										474677
									],
									[
										474777,
										474782
									],
									[
										474813,
										474818
									],
									[
										475211,
										475216
									],
									[
										475325,
										475330
									],
									[
										475611,
										475616
									],
									[
										475780,
										475785
									],
									[
										476055,
										476060
									],
									[
										476297,
										476302
									],
									[
										476529,
										476534
									],
									[
										476915,
										476920
									],
									[
										476964,
										476969
									],
									[
										477019,
										477024
									],
									[
										477157,
										477162
									],
									[
										477435,
										477440
									],
									[
										477602,
										477607
									],
									[
										477729,
										477734
									],
									[
										478111,
										478116
									],
									[
										478238,
										478243
									],
									[
										478582,
										478587
									],
									[
										478796,
										478801
									],
									[
										478893,
										478898
									],
									[
										479077,
										479082
									],
									[
										479320,
										479325
									],
									[
										479455,
										479460
									],
									[
										479552,
										479557
									],
									[
										479819,
										479824
									],
									[
										480133,
										480138
									],
									[
										480365,
										480370
									],
									[
										480598,
										480603
									],
									[
										480876,
										480881
									],
									[
										481032,
										481037
									],
									[
										481345,
										481350
									],
									[
										481465,
										481470
									],
									[
										481585,
										481590
									],
									[
										481705,
										481710
									],
									[
										481825,
										481830
									],
									[
										481945,
										481950
									],
									[
										482065,
										482070
									],
									[
										482185,
										482190
									],
									[
										482305,
										482310
									],
									[
										482548,
										482553
									],
									[
										482804,
										482809
									],
									[
										482825,
										482830
									],
									[
										483060,
										483065
									],
									[
										483081,
										483086
									],
									[
										483340,
										483345
									],
									[
										483572,
										483577
									],
									[
										483759,
										483764
									],
									[
										484044,
										484049
									],
									[
										484305,
										484310
									],
									[
										484425,
										484430
									],
									[
										484545,
										484550
									],
									[
										484758,
										484763
									],
									[
										484860,
										484865
									],
									[
										484897,
										484902
									],
									[
										485064,
										485069
									],
									[
										485093,
										485098
									],
									[
										485204,
										485209
									],
									[
										485262,
										485267
									],
									[
										485366,
										485371
									],
									[
										485423,
										485428
									],
									[
										485755,
										485760
									],
									[
										485840,
										485845
									],
									[
										485884,
										485889
									],
									[
										485921,
										485926
									],
									[
										485981,
										485986
									],
									[
										486264,
										486269
									],
									[
										486325,
										486330
									],
									[
										486369,
										486374
									],
									[
										486442,
										486447
									],
									[
										486463,
										486468
									],
									[
										486571,
										486576
									],
									[
										486803,
										486808
									],
									[
										558785,
										558790
									],
									[
										560943,
										560948
									],
									[
										562059,
										562064
									],
									[
										564224,
										564229
									],
									[
										565391,
										565396
									],
									[
										566605,
										566610
									],
									[
										567772,
										567777
									],
									[
										568986,
										568991
									],
									[
										570696,
										570701
									],
									[
										572716,
										572721
									],
									[
										573882,
										573887
									],
									[
										575096,
										575101
									],
									[
										576262,
										576267
									],
									[
										577476,
										577481
									],
									[
										579185,
										579190
									],
									[
										581746,
										581751
									],
									[
										582913,
										582918
									],
									[
										584127,
										584132
									],
									[
										585294,
										585299
									],
									[
										586508,
										586513
									],
									[
										588218,
										588223
									],
									[
										590238,
										590243
									],
									[
										591404,
										591409
									],
									[
										592618,
										592623
									],
									[
										593784,
										593789
									],
									[
										594998,
										595003
									],
									[
										596707,
										596712
									],
									[
										598727,
										598732
									],
									[
										599893,
										599898
									],
									[
										601107,
										601112
									],
									[
										602273,
										602278
									],
									[
										603487,
										603492
									],
									[
										605196,
										605201
									],
									[
										607216,
										607221
									],
									[
										608382,
										608387
									],
									[
										609596,
										609601
									],
									[
										610762,
										610767
									],
									[
										611976,
										611981
									],
									[
										613685,
										613690
									],
									[
										615705,
										615710
									],
									[
										616871,
										616876
									],
									[
										618085,
										618090
									],
									[
										619251,
										619256
									],
									[
										620465,
										620470
									],
									[
										622174,
										622179
									],
									[
										624737,
										624742
									],
									[
										625905,
										625910
									],
									[
										627119,
										627124
									],
									[
										628287,
										628292
									],
									[
										629501,
										629506
									],
									[
										631212,
										631217
									],
									[
										633232,
										633237
									],
									[
										634398,
										634403
									],
									[
										635612,
										635617
									],
									[
										636778,
										636783
									],
									[
										637992,
										637997
									],
									[
										639701,
										639706
									],
									[
										642992,
										642997
									],
									[
										648329,
										648334
									],
									[
										657138,
										657143
									],
									[
										658604,
										658609
									],
									[
										660040,
										660045
									],
									[
										661520,
										661525
									],
									[
										662964,
										662969
									],
									[
										664446,
										664451
									],
									[
										665892,
										665897
									],
									[
										667374,
										667379
									],
									[
										682742,
										682747
									],
									[
										684461,
										684466
									],
									[
										686186,
										686191
									],
									[
										687911,
										687916
									],
									[
										690206,
										690211
									],
									[
										690484,
										690489
									],
									[
										691063,
										691068
									],
									[
										691292,
										691297
									],
									[
										691570,
										691575
									],
									[
										691993,
										691998
									],
									[
										692065,
										692070
									],
									[
										692455,
										692460
									],
									[
										692893,
										692898
									],
									[
										693338,
										693343
									],
									[
										693790,
										693795
									],
									[
										694261,
										694266
									],
									[
										694750,
										694755
									],
									[
										695105,
										695110
									],
									[
										695394,
										695399
									],
									[
										695637,
										695642
									],
									[
										695687,
										695692
									],
									[
										695717,
										695722
									],
									[
										695873,
										695878
									],
									[
										696034,
										696039
									],
									[
										696105,
										696110
									],
									[
										696147,
										696152
									],
									[
										696170,
										696175
									],
									[
										696217,
										696222
									],
									[
										696243,
										696248
									],
									[
										696318,
										696323
									],
									[
										696351,
										696356
									],
									[
										696586,
										696591
									],
									[
										696868,
										696873
									],
									[
										697002,
										697007
									],
									[
										697122,
										697127
									],
									[
										697469,
										697474
									],
									[
										697683,
										697688
									],
									[
										697691,
										697696
									],
									[
										697915,
										697920
									],
									[
										697983,
										697988
									],
									[
										698075,
										698080
									],
									[
										698404,
										698409
									],
									[
										698515,
										698520
									],
									[
										698989,
										698994
									],
									[
										699249,
										699254
									],
									[
										699360,
										699365
									],
									[
										699589,
										699594
									],
									[
										699822,
										699827
									],
									[
										699933,
										699938
									],
									[
										700162,
										700167
									],
									[
										700646,
										700651
									],
									[
										701101,
										701106
									],
									[
										701340,
										701345
									],
									[
										701546,
										701551
									],
									[
										701749,
										701754
									],
									[
										702016,
										702021
									],
									[
										702347,
										702352
									],
									[
										702622,
										702627
									],
									[
										702876,
										702881
									],
									[
										703255,
										703260
									],
									[
										703272,
										703277
									],
									[
										703623,
										703628
									],
									[
										703942,
										703947
									],
									[
										704064,
										704069
									],
									[
										704109,
										704114
									],
									[
										704176,
										704181
									],
									[
										704219,
										704224
									],
									[
										704258,
										704263
									],
									[
										704309,
										704314
									],
									[
										704356,
										704361
									],
									[
										704404,
										704409
									],
									[
										704448,
										704453
									],
									[
										704493,
										704498
									],
									[
										704534,
										704539
									],
									[
										704584,
										704589
									],
									[
										704630,
										704635
									],
									[
										704679,
										704684
									],
									[
										704724,
										704729
									],
									[
										704773,
										704778
									],
									[
										704818,
										704823
									],
									[
										704867,
										704872
									],
									[
										704912,
										704917
									],
									[
										704964,
										704969
									],
									[
										705012,
										705017
									],
									[
										705061,
										705066
									],
									[
										705106,
										705111
									],
									[
										705155,
										705160
									],
									[
										705247,
										705252
									],
									[
										705417,
										705422
									],
									[
										705464,
										705469
									],
									[
										705717,
										705722
									],
									[
										705840,
										705845
									],
									[
										705878,
										705883
									],
									[
										705914,
										705919
									],
									[
										705970,
										705975
									],
									[
										706090,
										706095
									],
									[
										706146,
										706151
									],
									[
										706169,
										706174
									],
									[
										706204,
										706209
									],
									[
										706263,
										706268
									],
									[
										706381,
										706386
									],
									[
										706469,
										706474
									],
									[
										706518,
										706523
									],
									[
										706587,
										706592
									],
									[
										706676,
										706681
									],
									[
										706695,
										706700
									],
									[
										706834,
										706839
									],
									[
										706869,
										706874
									],
									[
										706912,
										706917
									],
									[
										707049,
										707054
									],
									[
										707445,
										707450
									],
									[
										707812,
										707817
									],
									[
										707970,
										707975
									],
									[
										708247,
										708252
									],
									[
										708316,
										708321
									],
									[
										708383,
										708388
									],
									[
										708450,
										708455
									],
									[
										708517,
										708522
									],
									[
										708584,
										708589
									],
									[
										708651,
										708656
									],
									[
										708718,
										708723
									],
									[
										708785,
										708790
									],
									[
										708852,
										708857
									],
									[
										708919,
										708924
									],
									[
										708986,
										708991
									],
									[
										709053,
										709058
									],
									[
										709120,
										709125
									],
									[
										709187,
										709192
									],
									[
										709254,
										709259
									],
									[
										709321,
										709326
									],
									[
										709388,
										709393
									],
									[
										709455,
										709460
									],
									[
										709522,
										709527
									],
									[
										709589,
										709594
									],
									[
										709656,
										709661
									],
									[
										709723,
										709728
									],
									[
										709790,
										709795
									],
									[
										709857,
										709862
									],
									[
										709920,
										709925
									],
									[
										709962,
										709967
									],
									[
										710020,
										710025
									],
									[
										710045,
										710050
									],
									[
										710081,
										710086
									],
									[
										710150,
										710155
									],
									[
										710217,
										710222
									],
									[
										710284,
										710289
									],
									[
										710347,
										710352
									],
									[
										710416,
										710421
									],
									[
										710483,
										710488
									],
									[
										710550,
										710555
									],
									[
										710637,
										710642
									],
									[
										710682,
										710687
									],
									[
										710748,
										710753
									],
									[
										710785,
										710790
									],
									[
										710858,
										710863
									],
									[
										710929,
										710934
									],
									[
										711000,
										711005
									],
									[
										711071,
										711076
									],
									[
										711124,
										711129
									],
									[
										711175,
										711180
									],
									[
										711201,
										711206
									],
									[
										711328,
										711333
									],
									[
										711353,
										711358
									],
									[
										711389,
										711394
									],
									[
										711462,
										711467
									],
									[
										711533,
										711538
									],
									[
										711604,
										711609
									],
									[
										711680,
										711685
									],
									[
										711725,
										711730
									],
									[
										711751,
										711756
									],
									[
										711858,
										711863
									],
									[
										711895,
										711900
									],
									[
										711968,
										711973
									],
									[
										712039,
										712044
									],
									[
										712110,
										712115
									],
									[
										712181,
										712186
									],
									[
										712256,
										712261
									],
									[
										712301,
										712306
									],
									[
										712327,
										712332
									],
									[
										712389,
										712394
									],
									[
										712491,
										712496
									],
									[
										712565,
										712570
									],
									[
										712620,
										712625
									],
									[
										712705,
										712710
									],
									[
										712738,
										712743
									],
									[
										712817,
										712822
									],
									[
										712875,
										712880
									],
									[
										712941,
										712946
									],
									[
										712990,
										712995
									],
									[
										713041,
										713046
									],
									[
										713089,
										713094
									],
									[
										713139,
										713144
									],
									[
										713177,
										713182
									],
									[
										713314,
										713319
									],
									[
										713426,
										713431
									],
									[
										713532,
										713537
									],
									[
										713569,
										713574
									],
									[
										713652,
										713657
									],
									[
										713703,
										713708
									],
									[
										713754,
										713759
									],
									[
										713807,
										713812
									],
									[
										713862,
										713867
									],
									[
										713900,
										713905
									],
									[
										714059,
										714064
									],
									[
										714174,
										714179
									],
									[
										714284,
										714289
									],
									[
										714336,
										714341
									],
									[
										714421,
										714426
									],
									[
										714474,
										714479
									],
									[
										714527,
										714532
									],
									[
										714582,
										714587
									],
									[
										714722,
										714727
									],
									[
										714859,
										714864
									],
									[
										714873,
										714878
									],
									[
										714977,
										714982
									],
									[
										714992,
										714997
									],
									[
										715041,
										715046
									],
									[
										715110,
										715115
									],
									[
										715178,
										715183
									],
									[
										715263,
										715268
									],
									[
										715318,
										715323
									],
									[
										715370,
										715375
									],
									[
										715425,
										715430
									],
									[
										715495,
										715500
									],
									[
										715599,
										715604
									],
									[
										715685,
										715690
									],
									[
										715822,
										715827
									],
									[
										715923,
										715928
									],
									[
										716012,
										716017
									],
									[
										716238,
										716243
									],
									[
										716298,
										716303
									],
									[
										716456,
										716461
									],
									[
										716724,
										716729
									],
									[
										716923,
										716928
									],
									[
										717097,
										717102
									],
									[
										717346,
										717351
									],
									[
										717435,
										717440
									],
									[
										717482,
										717487
									],
									[
										717555,
										717560
									],
									[
										717624,
										717629
									],
									[
										717793,
										717798
									],
									[
										718075,
										718080
									],
									[
										718350,
										718355
									],
									[
										718489,
										718494
									],
									[
										718555,
										718560
									],
									[
										718702,
										718707
									],
									[
										718749,
										718754
									],
									[
										718822,
										718827
									],
									[
										718892,
										718897
									],
									[
										719005,
										719010
									],
									[
										719172,
										719177
									],
									[
										719356,
										719361
									],
									[
										719632,
										719637
									],
									[
										719721,
										719726
									],
									[
										719768,
										719773
									],
									[
										719841,
										719846
									],
									[
										719910,
										719915
									],
									[
										720014,
										720019
									],
									[
										720126,
										720131
									],
									[
										720168,
										720173
									],
									[
										720260,
										720265
									],
									[
										720329,
										720334
									],
									[
										720483,
										720488
									],
									[
										720671,
										720676
									],
									[
										720738,
										720743
									],
									[
										720946,
										720951
									],
									[
										721012,
										721017
									],
									[
										721159,
										721164
									],
									[
										721206,
										721211
									],
									[
										721279,
										721284
									],
									[
										721349,
										721354
									],
									[
										721462,
										721467
									],
									[
										721604,
										721609
									],
									[
										721665,
										721670
									],
									[
										721843,
										721848
									],
									[
										722082,
										722087
									],
									[
										722171,
										722176
									],
									[
										722218,
										722223
									],
									[
										722296,
										722301
									],
									[
										722365,
										722370
									],
									[
										722552,
										722557
									],
									[
										722836,
										722841
									],
									[
										723030,
										723035
									],
									[
										723216,
										723221
									],
									[
										723286,
										723291
									],
									[
										723352,
										723357
									],
									[
										723509,
										723514
									],
									[
										723556,
										723561
									],
									[
										723634,
										723639
									],
									[
										723704,
										723709
									],
									[
										723822,
										723827
									],
									[
										723999,
										724004
									],
									[
										724194,
										724199
									],
									[
										724437,
										724442
									],
									[
										724484,
										724489
									],
									[
										724558,
										724563
									],
									[
										724627,
										724632
									],
									[
										724881,
										724886
									],
									[
										725037,
										725042
									],
									[
										725077,
										725082
									],
									[
										725134,
										725139
									],
									[
										725212,
										725217
									],
									[
										725338,
										725343
									],
									[
										725404,
										725409
									],
									[
										725550,
										725555
									],
									[
										725597,
										725602
									],
									[
										725671,
										725676
									],
									[
										725741,
										725746
									],
									[
										725855,
										725860
									],
									[
										726024,
										726029
									],
									[
										726169,
										726174
									],
									[
										726247,
										726252
									],
									[
										726472,
										726477
									],
									[
										726519,
										726524
									],
									[
										726596,
										726601
									],
									[
										726665,
										726670
									],
									[
										726811,
										726816
									],
									[
										726981,
										726986
									],
									[
										727144,
										727149
									],
									[
										727265,
										727270
									],
									[
										727381,
										727386
									],
									[
										727497,
										727502
									],
									[
										727626,
										727631
									],
									[
										727666,
										727671
									],
									[
										727716,
										727721
									],
									[
										727857,
										727862
									],
									[
										727923,
										727928
									],
									[
										728078,
										728083
									],
									[
										728125,
										728130
									],
									[
										728202,
										728207
									],
									[
										728272,
										728277
									],
									[
										728389,
										728394
									],
									[
										728564,
										728569
									],
									[
										728738,
										728743
									],
									[
										728799,
										728804
									],
									[
										728824,
										728829
									],
									[
										729137,
										729142
									],
									[
										729154,
										729159
									],
									[
										729263,
										729268
									],
									[
										729280,
										729285
									],
									[
										729404,
										729409
									],
									[
										729421,
										729426
									],
									[
										729496,
										729501
									],
									[
										729527,
										729532
									],
									[
										729604,
										729609
									],
									[
										729628,
										729633
									],
									[
										729673,
										729678
									],
									[
										729799,
										729804
									],
									[
										729903,
										729908
									],
									[
										730048,
										730053
									],
									[
										730123,
										730128
									],
									[
										730314,
										730319
									],
									[
										730509,
										730514
									],
									[
										730660,
										730665
									],
									[
										730742,
										730747
									],
									[
										730825,
										730830
									],
									[
										730962,
										730967
									],
									[
										731036,
										731041
									],
									[
										731367,
										731372
									],
									[
										731616,
										731621
									],
									[
										731956,
										731961
									],
									[
										732186,
										732191
									],
									[
										732516,
										732521
									],
									[
										732782,
										732787
									],
									[
										733045,
										733050
									],
									[
										733225,
										733230
									],
									[
										733369,
										733374
									],
									[
										733548,
										733553
									],
									[
										733603,
										733608
									],
									[
										733666,
										733671
									],
									[
										733712,
										733717
									],
									[
										733760,
										733765
									],
									[
										733805,
										733810
									],
									[
										733855,
										733860
									],
									[
										733890,
										733895
									],
									[
										734008,
										734013
									],
									[
										734199,
										734204
									],
									[
										734247,
										734252
									],
									[
										734295,
										734300
									],
									[
										734345,
										734350
									],
									[
										734400,
										734405
									],
									[
										734435,
										734440
									],
									[
										734558,
										734563
									],
									[
										734751,
										734756
									],
									[
										734801,
										734806
									],
									[
										734851,
										734856
									],
									[
										734903,
										734908
									],
									[
										735020,
										735025
									],
									[
										735286,
										735291
									],
									[
										735538,
										735543
									],
									[
										735718,
										735723
									],
									[
										735996,
										736001
									],
									[
										736280,
										736285
									],
									[
										736358,
										736363
									],
									[
										736383,
										736388
									],
									[
										736689,
										736694
									],
									[
										736706,
										736711
									],
									[
										736855,
										736860
									],
									[
										736872,
										736877
									],
									[
										736983,
										736988
									],
									[
										737272,
										737277
									],
									[
										737424,
										737429
									],
									[
										737508,
										737513
									],
									[
										737533,
										737538
									],
									[
										737790,
										737795
									],
									[
										737935,
										737940
									],
									[
										738010,
										738015
									],
									[
										738145,
										738150
									],
									[
										738442,
										738447
									],
									[
										738666,
										738671
									],
									[
										738683,
										738688
									],
									[
										738822,
										738827
									],
									[
										738839,
										738844
									],
									[
										738989,
										738994
									],
									[
										739006,
										739011
									],
									[
										739081,
										739086
									],
									[
										739112,
										739117
									],
									[
										739189,
										739194
									],
									[
										739213,
										739218
									],
									[
										739258,
										739263
									],
									[
										739413,
										739418
									],
									[
										739451,
										739456
									],
									[
										739483,
										739488
									],
									[
										739529,
										739534
									],
									[
										739567,
										739572
									],
									[
										739618,
										739623
									],
									[
										739691,
										739696
									],
									[
										739974,
										739979
									],
									[
										740168,
										740173
									],
									[
										740248,
										740253
									],
									[
										740348,
										740353
									],
									[
										740547,
										740552
									],
									[
										740683,
										740688
									],
									[
										740728,
										740733
									],
									[
										740810,
										740815
									],
									[
										740894,
										740899
									],
									[
										741024,
										741029
									],
									[
										741080,
										741085
									],
									[
										741112,
										741117
									],
									[
										741389,
										741394
									],
									[
										741726,
										741731
									],
									[
										741985,
										741990
									],
									[
										742279,
										742284
									],
									[
										742528,
										742533
									],
									[
										742794,
										742799
									],
									[
										743008,
										743013
									],
									[
										743057,
										743062
									],
									[
										743127,
										743132
									],
									[
										743288,
										743293
									],
									[
										743432,
										743437
									],
									[
										743718,
										743723
									],
									[
										743955,
										743960
									],
									[
										744247,
										744252
									],
									[
										744500,
										744505
									],
									[
										744575,
										744580
									],
									[
										744600,
										744605
									],
									[
										744859,
										744864
									],
									[
										745004,
										745009
									],
									[
										745079,
										745084
									],
									[
										745214,
										745219
									],
									[
										745523,
										745528
									],
									[
										745540,
										745545
									],
									[
										745749,
										745754
									],
									[
										745766,
										745771
									],
									[
										745877,
										745882
									],
									[
										745894,
										745899
									],
									[
										746020,
										746025
									],
									[
										746037,
										746042
									],
									[
										746112,
										746117
									],
									[
										746143,
										746148
									],
									[
										746220,
										746225
									],
									[
										746244,
										746249
									],
									[
										746289,
										746294
									],
									[
										746415,
										746420
									],
									[
										746453,
										746458
									],
									[
										746485,
										746490
									],
									[
										746531,
										746536
									],
									[
										746569,
										746574
									],
									[
										746620,
										746625
									],
									[
										746695,
										746700
									],
									[
										746979,
										746984
									],
									[
										747173,
										747178
									],
									[
										747253,
										747258
									],
									[
										747353,
										747358
									],
									[
										747554,
										747559
									],
									[
										747767,
										747772
									],
									[
										747851,
										747856
									],
									[
										747937,
										747942
									],
									[
										748069,
										748074
									],
									[
										748127,
										748132
									],
									[
										748176,
										748181
									],
									[
										748470,
										748475
									],
									[
										748807,
										748812
									],
									[
										749154,
										749159
									],
									[
										749384,
										749389
									],
									[
										749721,
										749726
									],
									[
										749987,
										749992
									],
									[
										750201,
										750206
									],
									[
										750250,
										750255
									],
									[
										750322,
										750327
									],
									[
										750503,
										750508
									],
									[
										750647,
										750652
									],
									[
										750939,
										750944
									],
									[
										751176,
										751181
									],
									[
										751474,
										751479
									],
									[
										751727,
										751732
									],
									[
										751800,
										751805
									],
									[
										751825,
										751830
									],
									[
										752078,
										752083
									],
									[
										752223,
										752228
									],
									[
										752298,
										752303
									],
									[
										752475,
										752480
									],
									[
										752492,
										752497
									],
									[
										752624,
										752629
									],
									[
										752641,
										752646
									],
									[
										752782,
										752787
									],
									[
										753010,
										753015
									],
									[
										753190,
										753195
									],
									[
										753504,
										753509
									],
									[
										753706,
										753711
									],
									[
										754018,
										754023
									],
									[
										754219,
										754224
									],
									[
										754543,
										754548
									],
									[
										754743,
										754748
									],
									[
										754962,
										754967
									],
									[
										755345,
										755350
									],
									[
										755600,
										755605
									],
									[
										755639,
										755644
									],
									[
										755916,
										755921
									],
									[
										756061,
										756066
									],
									[
										756136,
										756141
									],
									[
										756295,
										756300
									],
									[
										756341,
										756346
									],
									[
										756387,
										756392
									],
									[
										756419,
										756424
									],
									[
										756470,
										756475
									],
									[
										756531,
										756536
									],
									[
										756608,
										756613
									],
									[
										756783,
										756788
									],
									[
										756823,
										756828
									],
									[
										756874,
										756879
									],
									[
										756925,
										756930
									],
									[
										756983,
										756988
									],
									[
										757148,
										757153
									],
									[
										757317,
										757322
									],
									[
										757665,
										757670
									],
									[
										757810,
										757815
									],
									[
										757885,
										757890
									],
									[
										757977,
										757982
									],
									[
										758176,
										758181
									],
									[
										758459,
										758464
									],
									[
										758722,
										758727
									],
									[
										758829,
										758834
									],
									[
										758939,
										758944
									],
									[
										759049,
										759054
									],
									[
										759202,
										759207
									],
									[
										759397,
										759402
									],
									[
										759477,
										759482
									],
									[
										759704,
										759709
									],
									[
										759888,
										759893
									],
									[
										760181,
										760186
									],
									[
										760326,
										760331
									],
									[
										760401,
										760406
									],
									[
										760583,
										760588
									],
									[
										760714,
										760719
									],
									[
										760836,
										760841
									],
									[
										761073,
										761078
									],
									[
										761248,
										761253
									],
									[
										761547,
										761552
									],
									[
										761692,
										761697
									],
									[
										761767,
										761772
									],
									[
										761949,
										761954
									],
									[
										762037,
										762042
									],
									[
										762158,
										762163
									],
									[
										762395,
										762400
									],
									[
										762570,
										762575
									],
									[
										762869,
										762874
									],
									[
										763014,
										763019
									],
									[
										763089,
										763094
									],
									[
										763271,
										763276
									],
									[
										763402,
										763407
									],
									[
										763524,
										763529
									],
									[
										763867,
										763872
									],
									[
										764043,
										764048
									],
									[
										764348,
										764353
									],
									[
										764493,
										764498
									],
									[
										764568,
										764573
									],
									[
										764750,
										764755
									],
									[
										764838,
										764843
									],
									[
										764959,
										764964
									],
									[
										765302,
										765307
									],
									[
										765478,
										765483
									],
									[
										765707,
										765712
									],
									[
										766058,
										766063
									],
									[
										766235,
										766240
									],
									[
										766438,
										766443
									],
									[
										766658,
										766663
									],
									[
										766884,
										766889
									],
									[
										767129,
										767134
									],
									[
										767189,
										767194
									],
									[
										767445,
										767450
									],
									[
										767505,
										767510
									],
									[
										767851,
										767856
									],
									[
										767994,
										767999
									],
									[
										768130,
										768135
									],
									[
										768285,
										768290
									],
									[
										768451,
										768456
									],
									[
										768674,
										768679
									],
									[
										768895,
										768900
									],
									[
										768909,
										768914
									],
									[
										769117,
										769122
									],
									[
										769179,
										769184
									],
									[
										769417,
										769422
									],
									[
										769524,
										769529
									],
									[
										769633,
										769638
									],
									[
										769722,
										769727
									],
									[
										769996,
										770001
									],
									[
										770118,
										770123
									],
									[
										770194,
										770199
									],
									[
										770402,
										770407
									],
									[
										770446,
										770451
									],
									[
										770477,
										770482
									],
									[
										770591,
										770596
									],
									[
										770665,
										770670
									],
									[
										770711,
										770716
									],
									[
										770972,
										770977
									],
									[
										771241,
										771246
									],
									[
										771510,
										771515
									],
									[
										771651,
										771656
									],
									[
										771673,
										771678
									],
									[
										771990,
										771995
									],
									[
										772272,
										772277
									],
									[
										772297,
										772302
									],
									[
										772531,
										772536
									],
									[
										772556,
										772561
									],
									[
										772806,
										772811
									],
									[
										772831,
										772836
									],
									[
										773069,
										773074
									],
									[
										773094,
										773099
									],
									[
										773354,
										773359
									],
									[
										773379,
										773384
									],
									[
										773689,
										773694
									],
									[
										773988,
										773993
									],
									[
										774222,
										774227
									],
									[
										774486,
										774491
									],
									[
										774770,
										774775
									],
									[
										775187,
										775192
									],
									[
										775379,
										775384
									],
									[
										775430,
										775435
									],
									[
										775752,
										775757
									],
									[
										775763,
										775768
									],
									[
										775970,
										775975
									],
									[
										776081,
										776086
									],
									[
										776341,
										776346
									],
									[
										776692,
										776697
									],
									[
										777068,
										777073
									],
									[
										777336,
										777341
									],
									[
										777524,
										777529
									],
									[
										777728,
										777733
									],
									[
										777922,
										777927
									],
									[
										778120,
										778125
									],
									[
										778407,
										778412
									],
									[
										778570,
										778575
									],
									[
										778613,
										778618
									],
									[
										778683,
										778688
									],
									[
										778829,
										778834
									],
									[
										778974,
										778979
									],
									[
										779141,
										779146
									],
									[
										779393,
										779398
									],
									[
										779658,
										779663
									],
									[
										780076,
										780081
									],
									[
										780086,
										780091
									],
									[
										780185,
										780190
									],
									[
										780486,
										780491
									],
									[
										780510,
										780515
									],
									[
										780644,
										780649
									],
									[
										780680,
										780685
									],
									[
										780717,
										780722
									],
									[
										780733,
										780738
									],
									[
										780976,
										780981
									],
									[
										780986,
										780991
									],
									[
										781196,
										781201
									],
									[
										781231,
										781236
									],
									[
										781465,
										781470
									],
									[
										781475,
										781480
									],
									[
										781570,
										781575
									],
									[
										781580,
										781585
									],
									[
										781902,
										781907
									],
									[
										781915,
										781920
									],
									[
										781962,
										781967
									],
									[
										781975,
										781980
									],
									[
										782287,
										782292
									],
									[
										782297,
										782302
									],
									[
										782350,
										782355
									],
									[
										782360,
										782365
									],
									[
										782668,
										782673
									],
									[
										782687,
										782692
									],
									[
										782923,
										782928
									],
									[
										783118,
										783123
									],
									[
										783156,
										783161
									],
									[
										783187,
										783192
									],
									[
										783245,
										783250
									],
									[
										783273,
										783278
									],
									[
										783296,
										783301
									],
									[
										783319,
										783324
									],
									[
										783338,
										783343
									],
									[
										783383,
										783388
									],
									[
										783414,
										783419
									],
									[
										783437,
										783442
									],
									[
										783456,
										783461
									],
									[
										783520,
										783525
									],
									[
										783543,
										783548
									],
									[
										783594,
										783599
									],
									[
										783618,
										783623
									],
									[
										783635,
										783640
									],
									[
										783696,
										783701
									],
									[
										783722,
										783727
									],
									[
										783781,
										783786
									],
									[
										783806,
										783811
									],
									[
										783872,
										783877
									],
									[
										783935,
										783940
									],
									[
										783960,
										783965
									],
									[
										784029,
										784034
									],
									[
										784111,
										784116
									],
									[
										784330,
										784335
									],
									[
										784351,
										784356
									],
									[
										784368,
										784373
									],
									[
										784417,
										784422
									],
									[
										784451,
										784456
									],
									[
										784674,
										784679
									],
									[
										784776,
										784781
									],
									[
										784813,
										784818
									],
									[
										784980,
										784985
									],
									[
										785009,
										785014
									],
									[
										785120,
										785125
									],
									[
										785178,
										785183
									],
									[
										785282,
										785287
									],
									[
										785339,
										785344
									],
									[
										786017,
										786023
									],
									[
										786342,
										786348
									],
									[
										786722,
										786728
									],
									[
										786932,
										786938
									],
									[
										787065,
										787071
									],
									[
										787223,
										787229
									],
									[
										787464,
										787470
									],
									[
										787631,
										787637
									],
									[
										787943,
										787949
									],
									[
										788188,
										788194
									],
									[
										788356,
										788362
									],
									[
										788619,
										788625
									],
									[
										788900,
										788906
									],
									[
										789205,
										789211
									],
									[
										789539,
										789545
									],
									[
										789754,
										789760
									],
									[
										790039,
										790045
									],
									[
										790333,
										790339
									],
									[
										790587,
										790593
									],
									[
										790872,
										790878
									],
									[
										791166,
										791172
									],
									[
										791433,
										791439
									],
									[
										791728,
										791734
									],
									[
										792067,
										792073
									],
									[
										792303,
										792309
									],
									[
										792659,
										792665
									],
									[
										792968,
										792974
									],
									[
										793237,
										793243
									],
									[
										793516,
										793522
									],
									[
										793837,
										793843
									],
									[
										794058,
										794064
									],
									[
										794355,
										794361
									],
									[
										794609,
										794615
									],
									[
										794884,
										794890
									],
									[
										795206,
										795212
									],
									[
										795458,
										795464
									],
									[
										795700,
										795706
									],
									[
										795852,
										795858
									],
									[
										796120,
										796126
									],
									[
										796316,
										796322
									],
									[
										796538,
										796544
									],
									[
										796816,
										796822
									],
									[
										797080,
										797086
									],
									[
										797344,
										797350
									],
									[
										797581,
										797587
									],
									[
										797899,
										797905
									],
									[
										798232,
										798238
									],
									[
										798589,
										798595
									],
									[
										798911,
										798917
									],
									[
										799139,
										799145
									],
									[
										799401,
										799407
									],
									[
										799624,
										799630
									],
									[
										799890,
										799896
									],
									[
										800206,
										800212
									],
									[
										800460,
										800466
									],
									[
										800736,
										800742
									],
									[
										801003,
										801009
									],
									[
										801322,
										801328
									],
									[
										801639,
										801645
									],
									[
										801954,
										801960
									],
									[
										802239,
										802245
									],
									[
										802486,
										802492
									],
									[
										802747,
										802753
									],
									[
										803102,
										803108
									],
									[
										803359,
										803365
									],
									[
										803633,
										803639
									],
									[
										803934,
										803940
									],
									[
										804208,
										804214
									],
									[
										804459,
										804465
									],
									[
										804725,
										804731
									],
									[
										805055,
										805061
									],
									[
										805364,
										805370
									],
									[
										805687,
										805693
									],
									[
										805924,
										805930
									],
									[
										806200,
										806206
									],
									[
										806526,
										806532
									],
									[
										806810,
										806816
									],
									[
										807069,
										807075
									],
									[
										807353,
										807359
									],
									[
										807567,
										807573
									],
									[
										807637,
										807643
									],
									[
										807868,
										807874
									],
									[
										808163,
										808169
									],
									[
										808431,
										808437
									],
									[
										808715,
										808721
									],
									[
										808993,
										808999
									],
									[
										809065,
										809071
									],
									[
										809310,
										809316
									],
									[
										809547,
										809553
									],
									[
										809799,
										809805
									],
									[
										810042,
										810048
									],
									[
										810098,
										810104
									],
									[
										810366,
										810372
									],
									[
										810642,
										810648
									],
									[
										810874,
										810880
									],
									[
										811213,
										811219
									],
									[
										811526,
										811532
									],
									[
										811892,
										811898
									],
									[
										812204,
										812210
									],
									[
										812543,
										812549
									],
									[
										812857,
										812863
									],
									[
										813223,
										813229
									],
									[
										813517,
										813523
									],
									[
										813755,
										813761
									],
									[
										814027,
										814033
									],
									[
										814265,
										814271
									],
									[
										814560,
										814566
									],
									[
										814776,
										814782
									],
									[
										814838,
										814844
									],
									[
										815207,
										815213
									],
									[
										815487,
										815493
									],
									[
										816095,
										816101
									],
									[
										816368,
										816374
									],
									[
										816596,
										816602
									],
									[
										816764,
										816770
									],
									[
										817027,
										817033
									],
									[
										817308,
										817314
									],
									[
										817574,
										817580
									],
									[
										817908,
										817914
									],
									[
										818123,
										818129
									],
									[
										818408,
										818414
									],
									[
										818702,
										818708
									],
									[
										818956,
										818962
									],
									[
										819241,
										819247
									],
									[
										819535,
										819541
									],
									[
										819802,
										819808
									],
									[
										820097,
										820103
									],
									[
										820436,
										820442
									],
									[
										820672,
										820678
									],
									[
										820959,
										820965
									],
									[
										821280,
										821286
									],
									[
										821501,
										821507
									],
									[
										821770,
										821776
									],
									[
										822044,
										822050
									],
									[
										822345,
										822351
									],
									[
										822619,
										822625
									],
									[
										822904,
										822910
									],
									[
										823131,
										823137
									],
									[
										823457,
										823463
									],
									[
										823708,
										823714
									],
									[
										823781,
										823787
									],
									[
										824012,
										824018
									],
									[
										824271,
										824277
									],
									[
										824555,
										824561
									],
									[
										824769,
										824775
									],
									[
										824839,
										824845
									],
									[
										825070,
										825076
									],
									[
										825321,
										825327
									],
									[
										825396,
										825402
									],
									[
										825642,
										825648
									],
									[
										825910,
										825916
									],
									[
										826194,
										826200
									],
									[
										826472,
										826478
									],
									[
										826544,
										826550
									],
									[
										826789,
										826795
									],
									[
										827026,
										827032
									],
									[
										827278,
										827284
									],
									[
										827547,
										827553
									],
									[
										827608,
										827614
									],
									[
										827797,
										827803
									],
									[
										827855,
										827861
									],
									[
										828125,
										828131
									],
									[
										828401,
										828407
									],
									[
										828633,
										828639
									],
									[
										828972,
										828978
									],
									[
										829285,
										829291
									],
									[
										829651,
										829657
									],
									[
										829963,
										829969
									],
									[
										830302,
										830308
									],
									[
										830616,
										830622
									],
									[
										830982,
										830988
									],
									[
										831289,
										831295
									],
									[
										831505,
										831511
									],
									[
										831567,
										831573
									],
									[
										831956,
										831962
									],
									[
										832252,
										832258
									],
									[
										833040,
										833066
									],
									[
										833261,
										833287
									],
									[
										833649,
										833675
									],
									[
										833972,
										833998
									],
									[
										834338,
										834364
									],
									[
										834625,
										834651
									],
									[
										835216,
										835242
									],
									[
										835474,
										835500
									],
									[
										835890,
										835916
									],
									[
										836111,
										836137
									],
									[
										836534,
										836560
									],
									[
										836912,
										836938
									],
									[
										837219,
										837245
									]
								],
								"scope": ""
							}
						},
						"selection":
						[
							[
								834606,
								834651
							]
						],
						"settings":
						{
							"detect_indentation": false,
							"line_numbers": false,
							"output_tag": 10,
							"result_base_dir": "",
							"result_file_regex": "^([^ 	].*):$",
							"result_line_regex": "^ +([0-9]+):",
							"scroll_past_end": true,
							"syntax": "Packages/Default/Find Results.hidden-tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 580.0,
						"zoom_level": 1.0
					},
					"stack_index": 49,
					"type": "text"
				},
				{
					"buffer": 49,
					"file": "tools/bazel.rc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 525,
						"regions":
						{
						},
						"selection":
						[
							[
								525,
								525
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 50,
					"type": "text"
				},
				{
					"buffer": 50,
					"file": "tensorflow/third_party/gpus/cuda_configure.bzl",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 19840,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 470.0,
						"zoom_level": 1.0
					},
					"stack_index": 51,
					"type": "text"
				},
				{
					"buffer": 51,
					"file": "tensorflow_serving/core/manager.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5853,
						"regions":
						{
						},
						"selection":
						[
							[
								4574,
								4591
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1666.0,
						"zoom_level": 1.0
					},
					"stack_index": 52,
					"type": "text"
				},
				{
					"buffer": 52,
					"file": "tensorflow_serving/core/servable_data.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3578,
						"regions":
						{
						},
						"selection":
						[
							[
								2306,
								2306
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 906.0,
						"zoom_level": 1.0
					},
					"stack_index": 53,
					"type": "text"
				},
				{
					"buffer": 53,
					"file": "tensorflow_serving/servables/caffe/caffe_session_bundle_config.proto",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2201,
						"regions":
						{
						},
						"selection":
						[
							[
								456,
								456
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 54,
					"type": "text"
				},
				{
					"buffer": 54,
					"file": "tensorflow_serving/servables/caffe/caffe_source_adapter.proto",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 252,
						"regions":
						{
						},
						"selection":
						[
							[
								238,
								214
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 55,
					"type": "text"
				},
				{
					"buffer": 55,
					"file": "tensorflow_serving/sources/storage_path/static_storage_path_source.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1573,
						"regions":
						{
						},
						"selection":
						[
							[
								1521,
								1521
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 56,
					"type": "text"
				},
				{
					"buffer": 56,
					"file": "tensorflow_serving/core/source_adapter.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 9272,
						"regions":
						{
						},
						"selection":
						[
							[
								2851,
								2851
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 841.0,
						"zoom_level": 1.0
					},
					"stack_index": 57,
					"type": "text"
				},
				{
					"buffer": 57,
					"file": "tensorflow/third_party/gpus/cuda/build_defs.bzl.tpl",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 431,
						"regions":
						{
						},
						"selection":
						[
							[
								431,
								431
							]
						],
						"settings":
						{
							"syntax": "Packages/HTML/HTML.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 58,
					"type": "text"
				},
				{
					"buffer": 58,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 80,
						"regions":
						{
						},
						"selection":
						[
							[
								80,
								0
							]
						],
						"settings":
						{
							"auto_name": "bazel-bin/tensorflow_serving/example/obj_detector_",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 59,
					"type": "text"
				},
				{
					"buffer": 59,
					"file": "/C/dev/public/libfrengine/build_py/bindings/py/setup.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1003,
						"regions":
						{
						},
						"selection":
						[
							[
								908,
								911
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				},
				{
					"buffer": 60,
					"file": "tensorflow/third_party/gpus/cuda/BUILD.tpl",
					"semi_transient": true,
					"settings":
					{
						"buffer_size": 3176,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/HTML/HTML.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 100.0,
						"zoom_level": 1.0
					},
					"stack_index": 60,
					"type": "text"
				},
				{
					"buffer": 61,
					"file": "tensorflow_serving/g3doc/custom_servable.md",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5701,
						"regions":
						{
						},
						"selection":
						[
							[
								3632,
								3632
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 123.0,
						"zoom_level": 1.0
					},
					"stack_index": 61,
					"type": "text"
				},
				{
					"buffer": 62,
					"file": "tensorflow_serving/core/target.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6522,
						"regions":
						{
						},
						"selection":
						[
							[
								4190,
								4190
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1383.0,
						"zoom_level": 1.0
					},
					"stack_index": 62,
					"type": "text"
				},
				{
					"buffer": 63,
					"file": "tensorflow/tensorflow/workspace.bzl",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 7406,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 63,
					"type": "text"
				},
				{
					"buffer": 64,
					"file": "tensorflow_serving/batching/batching_session.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 15812,
						"regions":
						{
						},
						"selection":
						[
							[
								14616,
								14640
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 6626.0,
						"zoom_level": 1.0
					},
					"stack_index": 64,
					"type": "text"
				},
				{
					"buffer": 65,
					"file": "tensorflow_serving/batching/basic_batch_scheduler.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 12376,
						"regions":
						{
						},
						"selection":
						[
							[
								6955,
								6955
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 3081.0,
						"zoom_level": 1.0
					},
					"stack_index": 65,
					"type": "text"
				},
				{
					"buffer": 66,
					"file": "tensorflow_serving/session_bundle/signature.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 11016,
						"regions":
						{
						},
						"selection":
						[
							[
								7824,
								7824
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 3231.0,
						"zoom_level": 1.0
					},
					"stack_index": 66,
					"type": "text"
				},
				{
					"buffer": 67,
					"file": "tensorflow/tensorflow/core/framework/tensor.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 29776,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 14826.0,
						"zoom_level": 1.0
					},
					"stack_index": 67,
					"type": "text"
				},
				{
					"buffer": 68,
					"file": "/C/dev/vm/data-share/caffe-ssd/src/caffe/layers/detection_output_layer.cpp",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 18226,
						"regions":
						{
						},
						"selection":
						[
							[
								2394,
								2394
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 68,
					"type": "text"
				},
				{
					"buffer": 69,
					"file": "tensorflow/tensorflow/core/framework/tensor_shape.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 11483,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 583.0,
						"zoom_level": 1.0
					},
					"stack_index": 69,
					"type": "text"
				},
				{
					"buffer": 70,
					"file": "tensorflow/tensorflow/core/framework/tensor_shape.proto",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1556,
						"regions":
						{
						},
						"selection":
						[
							[
								299,
								315
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 70,
					"type": "text"
				},
				{
					"buffer": 71,
					"file": "tensorflow_serving/session_bundle/session_bundle_test.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3390,
						"regions":
						{
						},
						"selection":
						[
							[
								1419,
								1421
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 33.0,
						"zoom_level": 1.0
					},
					"stack_index": 71,
					"type": "text"
				},
				{
					"buffer": 72,
					"file": "tensorflow/tensorflow/core/platform/env.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 16462,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 220.0,
						"zoom_level": 1.0
					},
					"stack_index": 72,
					"type": "text"
				},
				{
					"buffer": 73,
					"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/utils/blob.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1654,
						"regions":
						{
						},
						"selection":
						[
							[
								1654,
								1654
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 73,
					"type": "text"
				},
				{
					"buffer": 74,
					"file": "/C/dev/vm/data-share/rcnn_data/py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af/lib/roi_data_layer/minibatch.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 8225,
						"regions":
						{
						},
						"selection":
						[
							[
								1608,
								1608
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 240.0,
						"zoom_level": 1.0
					},
					"stack_index": 74,
					"type": "text"
				},
				{
					"buffer": 75,
					"file": "tensorflow/tensorflow/core/framework/types.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 8129,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 3.0,
						"zoom_level": 1.0
					},
					"stack_index": 75,
					"type": "text"
				},
				{
					"buffer": 76,
					"file": "tensorflow/tensorflow/core/common_runtime/threadpool_device.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2789,
						"regions":
						{
						},
						"selection":
						[
							[
								1289,
								1289
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 76,
					"type": "text"
				},
				{
					"buffer": 77,
					"file": "tensorflow/tensorflow/core/common_runtime/device.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1919,
						"regions":
						{
						},
						"selection":
						[
							[
								979,
								979
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 77,
					"type": "text"
				},
				{
					"buffer": 78,
					"file": "tensorflow/tensorflow/contrib/session_bundle/manifest.proto",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2426,
						"regions":
						{
						},
						"selection":
						[
							[
								1270,
								1270
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 78,
					"type": "text"
				},
				{
					"buffer": 79,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3747,
						"regions":
						{
						},
						"selection":
						[
							[
								1257,
								1258
							]
						],
						"settings":
						{
							"auto_name": "im_info shape (1, 3)",
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 79,
					"type": "text"
				},
				{
					"buffer": 80,
					"file": "tensorflow_serving/sources/storage_path/file_system_storage_path_source.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 10268,
						"regions":
						{
						},
						"selection":
						[
							[
								9552,
								9564
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 3946.0,
						"zoom_level": 1.0
					},
					"stack_index": 80,
					"type": "text"
				},
				{
					"buffer": 81,
					"file": "tensorflow_serving/servables/caffe/caffe_simple_servers_test.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2374,
						"regions":
						{
						},
						"selection":
						[
							[
								1631,
								1631
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 81,
					"type": "text"
				},
				{
					"buffer": 82,
					"file": "tensorflow/third_party/gpus/cuda/build_defs.bzl",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 488,
						"regions":
						{
						},
						"selection":
						[
							[
								34,
								34
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 82,
					"type": "text"
				},
				{
					"buffer": 83,
					"file": "tensorflow/tensorflow/core/platform/posix/test.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5472,
						"regions":
						{
						},
						"selection":
						[
							[
								4771,
								4771
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2546.0,
						"zoom_level": 1.0
					},
					"stack_index": 83,
					"type": "text"
				},
				{
					"buffer": 84,
					"file": "tensorflow/tensorflow/contrib/session_bundle/test_util.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1292,
						"regions":
						{
						},
						"selection":
						[
							[
								1010,
								1027
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 84,
					"type": "text"
				},
				{
					"buffer": 85,
					"file": "tf_models/syntaxnet/syntaxnet/syntaxnet.bzl",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 4462,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 85,
					"type": "text"
				},
				{
					"buffer": 86,
					"file": "tensorflow/third_party/gpus/cuda/BUILD",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 0,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C#/Build.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 86,
					"type": "text"
				},
				{
					"buffer": 87,
					"file": "tensorflow/tensorflow/core/client/tensor_c_api_test.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 4655,
						"regions":
						{
						},
						"selection":
						[
							[
								2220,
								2231
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 87,
					"type": "text"
				},
				{
					"buffer": 88,
					"file": "tensorflow/tensorflow/core/framework/tensor_testutil.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 7230,
						"regions":
						{
						},
						"selection":
						[
							[
								1563,
								1563
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 143.0,
						"zoom_level": 1.0
					},
					"stack_index": 88,
					"type": "text"
				},
				{
					"buffer": 89,
					"file": "tensorflow/tensorflow/core/framework/tensor_shape.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 12978,
						"regions":
						{
						},
						"selection":
						[
							[
								1449,
								1449
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 143.0,
						"zoom_level": 1.0
					},
					"stack_index": 89,
					"type": "text"
				},
				{
					"buffer": 90,
					"file": "tensorflow_serving/servables/caffe/caffe_source_adapter.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2969,
						"regions":
						{
						},
						"selection":
						[
							[
								2908,
								2908
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 86.0,
						"zoom_level": 1.0
					},
					"stack_index": 90,
					"type": "text"
				},
				{
					"buffer": 91,
					"file": "tensorflow_serving/servables/caffe/caffe_session_bundle_factory_test.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 7856,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 2821.0,
						"zoom_level": 1.0
					},
					"stack_index": 91,
					"type": "text"
				},
				{
					"buffer": 92,
					"file": "README.md",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 9308,
						"regions":
						{
						},
						"selection":
						[
							[
								2784,
								2784
							]
						],
						"settings":
						{
							"syntax": "Packages/Markdown/Markdown.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 310.0,
						"zoom_level": 1.0
					},
					"stack_index": 92,
					"type": "text"
				},
				{
					"buffer": 93,
					"file": "tensorflow_serving/servables/caffe/caffe_session_bundle_factory.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 7768,
						"regions":
						{
						},
						"selection":
						[
							[
								2414,
								2414
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 386.0,
						"zoom_level": 1.0
					},
					"stack_index": 93,
					"type": "text"
				},
				{
					"buffer": 94,
					"file": "tensorflow_serving/servables/caffe/caffe_source_adapter_test.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3833,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 263.0,
						"zoom_level": 1.0
					},
					"stack_index": 94,
					"type": "text"
				},
				{
					"buffer": 95,
					"file": "tensorflow_serving/servables/caffe/test_data/py_layers/00000001/weights.caffemodel",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 0,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 95,
					"type": "text"
				},
				{
					"buffer": 96,
					"file": "tensorflow_serving/servables/caffe/test_data/py_layers/00000001/deploy.prototxt",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 496,
						"regions":
						{
						},
						"selection":
						[
							[
								496,
								496
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 96,
					"type": "text"
				},
				{
					"buffer": 97,
					"file": "tensorflow_serving/servables/caffe/test_data/py_layers/00000001/test_python_layer.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 402,
						"regions":
						{
						},
						"selection":
						[
							[
								402,
								402
							]
						],
						"settings":
						{
							"auto_name": "SimpleL",
							"history_list_is_closing": true,
							"syntax": "Packages/Python/Python.sublime-syntax",
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 97,
					"type": "text"
				},
				{
					"buffer": 98,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 3118,
						"regions":
						{
						},
						"selection":
						[
							[
								1359,
								1364
							]
						],
						"settings":
						{
							"auto_name": "{'google': <module 'google' (built-in)>, 'copy_reg",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 98,
					"type": "text"
				},
				{
					"buffer": 99,
					"file": "tensorflow_serving/servables/caffe/test_data/BUILD",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 895,
						"regions":
						{
						},
						"selection":
						[
							[
								895,
								895
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 99,
					"type": "text"
				},
				{
					"buffer": 100,
					"file": "tensorflow_serving/servables/caffe/test_data/mnist_sample.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5118,
						"regions":
						{
						},
						"selection":
						[
							[
								5118,
								5118
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 6,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 100,
					"type": "text"
				},
				{
					"buffer": 101,
					"file": "tensorflow_serving/servables/caffe/simple_thread_sink.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1290,
						"regions":
						{
						},
						"selection":
						[
							[
								1137,
								1137
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 101,
					"type": "text"
				},
				{
					"buffer": 102,
					"file": "tensorflow_serving/servables/caffe/predict_impl.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1250,
						"regions":
						{
						},
						"selection":
						[
							[
								1250,
								1250
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 102,
					"type": "text"
				},
				{
					"buffer": 103,
					"file": "tensorflow_serving/servables/caffe/predict_impl.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5557,
						"regions":
						{
						},
						"selection":
						[
							[
								2500,
								2500
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 401.0,
						"zoom_level": 1.0
					},
					"stack_index": 103,
					"type": "text"
				},
				{
					"buffer": 104,
					"file": "tensorflow_serving/servables/caffe/simple_thread_sink.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1013,
						"regions":
						{
						},
						"selection":
						[
							[
								369,
								369
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 104,
					"type": "text"
				},
				{
					"buffer": 105,
					"file": "tensorflow/tensorflow/core/framework/tensor.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 24328,
						"regions":
						{
						},
						"selection":
						[
							[
								19986,
								19986
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 10053.0,
						"zoom_level": 1.0
					},
					"stack_index": 105,
					"type": "text"
				},
				{
					"buffer": 106,
					"file": "tensorflow/tensorflow/core/framework/tensor.proto",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2468,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 106,
					"type": "text"
				},
				{
					"buffer": 107,
					"file": "tensorflow/tensorflow/core/framework/types.proto",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1834,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 107,
					"type": "text"
				},
				{
					"buffer": 108,
					"file": "tensorflow/tensorflow/core/protobuf/meta_graph.proto",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 8885,
						"regions":
						{
						},
						"selection":
						[
							[
								4499,
								4499
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 343.0,
						"zoom_level": 1.0
					},
					"stack_index": 108,
					"type": "text"
				},
				{
					"buffer": 109,
					"file": "tf_models/inception/inception/data/build_imagenet_data.py",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 26199,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 11763.0,
						"zoom_level": 1.0
					},
					"stack_index": 109,
					"type": "text"
				}
			]
		},
		{
			"selected": 0,
			"sheets":
			[
				{
					"buffer": 110,
					"file": "tensorflow_serving/servables/caffe/caffe_simple_servers.cc",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 4386,
						"regions":
						{
						},
						"selection":
						[
							[
								3127,
								3127
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 866.0,
						"zoom_level": 1.0
					},
					"stack_index": 110,
					"type": "text"
				},
				{
					"buffer": 111,
					"file": "tensorflow_serving/model_servers/BUILD",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5192,
						"regions":
						{
						},
						"selection":
						[
							[
								0,
								0
							]
						],
						"settings":
						{
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 4,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1566.0,
						"zoom_level": 1.0
					},
					"stack_index": 111,
					"type": "text"
				},
				{
					"buffer": 112,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 4670,
						"regions":
						{
						},
						"selection":
						[
							[
								3757,
								3802
							]
						],
						"settings":
						{
							"auto_name": "# Copyright 2016 Google Inc. All Rights Reserved.",
							"syntax": "Packages/Python/Python.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 1176.0,
						"zoom_level": 1.0
					},
					"stack_index": 112,
					"type": "text"
				},
				{
					"buffer": 113,
					"file": "tensorflow_serving/servables/caffe/caffe_source_adapter.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1948,
						"regions":
						{
						},
						"selection":
						[
							[
								867,
								867
							]
						],
						"settings":
						{
							"syntax": "Packages/C++/C++.sublime-syntax",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 113,
					"type": "text"
				},
				{
					"buffer": 114,
					"file": "tensorflow_serving/servables/caffe/caffe_simple_servers.h",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 2360,
						"regions":
						{
						},
						"selection":
						[
							[
								1492,
								1492
							]
						],
						"settings":
						{
							"history_list_is_closing": true,
							"syntax": "Packages/C++/C++.sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 114,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 29.0
	},
	"input":
	{
		"height": 55.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			],
			[
				1,
				0,
				2,
				1
			]
		],
		"cols":
		[
			0.0,
			0.529727669419,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": false,
	"output.find_results":
	{
		"height": 0.0
	},
	"pinned_build_system": "",
	"project": "dev.sublime-project",
	"replace":
	{
		"height": 54.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"predict",
				"tensorflow_serving\\servables\\caffe\\predict_impl.cc"
			],
			[
				"server",
				"tensorflow_serving\\model_servers\\server_core.cc"
			],
			[
				"caffe_sign",
				"tensorflow_serving\\servables\\caffe\\caffe_signature.cc"
			],
			[
				"signa",
				"tensorflow\\tensorflow\\contrib\\session_bundle\\signature.cc"
			],
			[
				"mnist_client.py",
				"tensorflow_serving\\example\\mnist_client.py"
			],
			[
				"main.cc",
				"tensorflow_serving\\model_servers\\main.cc"
			],
			[
				"mnist",
				"tensorflow_serving\\example\\mnist_inference.cc"
			],
			[
				"caffe_session_bundle_factory_test",
				"tensorflow_serving\\servables\\caffe\\caffe_session_bundle_factory_test.cc"
			],
			[
				"mnist_in",
				"tensorflow_serving\\example\\mnist_inference.cc"
			],
			[
				"readme",
				"README.md"
			],
			[
				"server_core.cc",
				"tensorflow_serving\\model_servers\\server_core.cc"
			],
			[
				"mnist_client",
				"tensorflow_serving\\example\\mnist_client.py"
			],
			[
				"server_core",
				"tensorflow_serving\\model_servers\\server_core.cc"
			],
			[
				"caffe_source_",
				"serving\\tensorflow_serving\\servables\\caffe\\caffe_source_adapter.h"
			],
			[
				"server_",
				"serving\\tensorflow_serving\\model_servers\\server_core.h"
			],
			[
				"main.",
				"serving\\tensorflow_serving\\model_servers\\main.cc"
			],
			[
				"session_bundle_",
				"serving\\tensorflow_serving\\servables\\tensorflow\\session_bundle_factory.h"
			],
			[
				"workspace.bzl",
				"serving\\tensorflow_serving\\workspace.bzl"
			],
			[
				"obj_detec",
				"serving\\tensorflow_serving\\example\\obj_detector.cc"
			],
			[
				"flags.cc",
				"serving\\tensorflow\\tensorflow\\core\\util\\command_line_flags.cc"
			],
			[
				"caffe_ser",
				"serving\\tensorflow_serving\\servables\\caffe\\caffe_serving_session.cc"
			],
			[
				"tensor.h",
				"serving\\tensorflow\\tensorflow\\core\\framework\\tensor.h"
			],
			[
				"caffe_session_bundle_factory_test_py",
				"serving\\tensorflow_serving\\servables\\caffe\\caffe_session_bundle_factory_test_py.cc"
			],
			[
				"rcnn_client",
				"serving\\tensorflow_serving\\example\\rcnn_client.py"
			],
			[
				"caffe_session_",
				"serving\\tensorflow_serving\\servables\\caffe\\caffe_session_bundle_factory.cc"
			],
			[
				"session_bundle_config.",
				"serving\\tensorflow_serving\\servables\\tensorflow\\session_bundle_config.proto"
			],
			[
				"tensorshape.pro",
				"serving\\tensorflow\\tensorflow\\core\\framework\\tensor_shape.proto"
			],
			[
				"caffe_py",
				"serving\\tensorflow_serving\\servables\\caffe\\caffe_py_util.cc"
			],
			[
				"tensor",
				"serving\\tensorflow\\tensorflow\\core\\framework\\tensor.h"
			],
			[
				"types.h",
				"serving\\tensorflow\\tensorflow\\core\\framework\\tensor_types.h"
			],
			[
				"env.h",
				"serving\\tensorflow\\tensorflow\\core\\platform\\env.h"
			],
			[
				"demo",
				"py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af\\tools\\demo.py"
			],
			[
				"setup",
				"serving\\tensorflow_serving\\example\\rcnn_setup.py"
			],
			[
				"setup.",
				"py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af\\lib\\setup.py"
			],
			[
				"demo.py",
				"py-faster-rcnn-d14cb16b78816cc5ab0f10283381cf2ff3c6a1af\\tools\\demo.py"
			],
			[
				"tensor_types",
				"serving\\tensorflow\\tensorflow\\core\\framework\\tensor_types.h"
			],
			[
				"",
				"serving\\tensorflow\\tensorflow\\core\\framework\\tensor.h"
			],
			[
				"test_py.cc",
				"tensorflow_serving\\servables\\caffe\\caffe_session_bundle_factory_test_py.cc"
			],
			[
				"config.proto",
				"tensorflow_serving\\servables\\caffe\\caffe_session_bundle_config.proto"
			],
			[
				"caffe_serving",
				"tensorflow_serving\\servables\\caffe\\caffe_serving_session.cc"
			],
			[
				"test_py",
				"tensorflow_serving\\servables\\caffe\\test_data\\py_layers\\00000001\\test_python_layer.py"
			],
			[
				"tensorpro",
				"tensorflow\\tensorflow\\core\\framework\\tensor.proto"
			],
			[
				"caffe_serving_",
				"tensorflow_serving\\servables\\caffe\\caffe_serving_session.h"
			],
			[
				"manif",
				"tensorflow\\tensorflow\\contrib\\session_bundle\\manifest.proto"
			],
			[
				"manifest.proto",
				"tensorflow\\tensorflow\\contrib\\session_bundle\\manifest.proto"
			],
			[
				"tensor.p",
				"tensorflow\\tensorflow\\core\\framework\\tensor.proto"
			],
			[
				"types.proto",
				"tensorflow\\tensorflow\\core\\framework\\types.proto"
			],
			[
				"tensor.prot",
				"tensorflow\\tensorflow\\core\\framework\\tensor.proto"
			],
			[
				"signat",
				"tensorflow_serving\\session_bundle\\signature.h"
			],
			[
				"resource_val",
				"tensorflow_serving\\resources\\resource_values.h"
			],
			[
				"caffe_session",
				"tensorflow_serving\\servables\\caffe\\caffe_session_bundle.h"
			],
			[
				"signature.h",
				"tensorflow_serving\\session_bundle\\signature.h"
			],
			[
				"build_con",
				"tensorflow\\tensorflow\\core\\platform\\default\\build_config.bzl"
			],
			[
				"basicbatchscheduler",
				"tensorflow_serving\\batching\\basic_batch_scheduler.h"
			],
			[
				"proto",
				"tensorflow\\google\\protobuf\\protobuf.bzl"
			],
			[
				"session.h",
				"tensorflow\\tensorflow\\core\\public\\session.h"
			],
			[
				"servable_han",
				"tensorflow_serving\\core\\servable_handle.h"
			],
			[
				"caffe_source_adapter_test",
				"tensorflow_serving\\servables\\caffe\\caffe_source_adapter_test.cc"
			],
			[
				"tensorflow/core/platform/test.h",
				"tensorflow\\tensorflow\\core\\platform\\test.h"
			],
			[
				"factory_test",
				"tensorflow_serving\\servables\\caffe\\caffe_session_bundle_factory_test.cc"
			],
			[
				"tensorflow/core/platform/env.",
				"tensorflow\\tensorflow\\core\\platform\\env.cc"
			],
			[
				"blob.hpp",
				"C:\\dev\\vm\\data-share\\caffe\\build_ubuntu\\install\\include\\caffe\\blob.hpp"
			],
			[
				"blob.",
				"C:\\dev\\vm\\data-share\\caffe\\build_ubuntu\\install\\include\\caffe\\blob.hpp"
			],
			[
				"net.hpp",
				"C:\\dev\\vm\\data-share\\caffe\\build_ubuntu\\install\\include\\caffe\\net.hpp"
			],
			[
				"blob.h",
				"C:\\dev\\vm\\data-share\\caffe\\build_ubuntu\\install\\include\\caffe\\blob.hpp"
			],
			[
				"net.h",
				"C:\\dev\\vm\\data-share\\caffe\\build_ubuntu\\install\\include\\caffe\\net.hpp"
			],
			[
				"init_ma",
				"tensorflow\\tensorflow\\core\\platform\\init_main.h"
			],
			[
				"errors.h",
				"tensorflow\\tensorflow\\core\\lib\\core\\errors.h"
			],
			[
				"arrayslice.",
				"tensorflow\\tensorflow\\core\\lib\\gtl\\array_slice.h"
			],
			[
				"arrayslice",
				"tensorflow\\tensorflow\\core\\lib\\gtl\\array_slice.h"
			],
			[
				"factory_test.cc",
				"tensorflow_serving\\servables\\caffe\\caffe_session_bundle_factory_test.cc"
			],
			[
				"sign",
				"tensorflow_serving\\session_bundle\\signature.h"
			],
			[
				"caffe.cpp",
				"C:\\dev\\vm\\data-share\\caffe\\tools\\caffe.cpp"
			],
			[
				"common.hpp",
				"C:\\dev\\vm\\data-share\\caffe\\build_ubuntu\\install\\include\\caffe\\common.hpp"
			]
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 500.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"",
				"C:\\dev\\vm\\data-share\\cv-worker\\dev.sublime-project"
			]
		],
		"width": 380.0
	},
	"select_symbol":
	{
		"height": 57.0,
		"last_filter": "",
		"selected_items":
		[
			[
				"Tensor",
				"Tensor"
			],
			[
				"CreateBatchingSession",
				"CreateBatchingSession"
			],
			[
				"CaffeSessionBundle",
				"CaffeSessionBundle"
			],
			[
				"ArraySlice",
				"ArraySlice"
			],
			[
				"AsTensor",
				"AsTensor"
			],
			[
				"DataType",
				"dataType"
			],
			[
				"InvalidArgument",
				"InvalidArgumentError"
			],
			[
				"Session",
				"Session"
			],
			[
				"SharedBatchScheduler",
				"SharedBatchScheduler"
			],
			[
				"strC",
				"StrCat"
			],
			[
				"strcat",
				"StrCat"
			],
			[
				"strCat",
				"StrCat"
			]
		],
		"width": 600.0
	},
	"selected_group": 0,
	"settings":
	{
	},
	"show_minimap": true,
	"show_open_files": true,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 447.0,
	"status_bar_visible": true,
	"template_settings":
	{
		"max_columns": 2
	}
}
